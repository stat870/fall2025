# Model selection 

## Announcements 

- Project deadlines:
  - **December 5th:** return your peer review to your classmate(s).
  - **December 12th:** give your presentation by this date (it can be earlier than this also). You will need to schedule a 30min meeting with me. 
  - **December 19th:** submit your final project and tutorial on canvas, including both your classmate's and my feedback.

## Model selection 

- Review: building a statistical model 
- Models for data generated by designed experiments
- Convergence issues in models for data generated by designed experiments 
- Model selection in other contexts 

### Model selection in other contexts  

```{r echo=FALSE, fig.cap="Figure 2 in Hooten and Hobbs (2015)", out.width = '80%', fig.align='center'}
knitr::include_graphics("../figures/model_selection_hooten2015.png")
```


#### The coefficient of determination R^2^

-   Usually interpreted as the proportion of the variation in $y$ that is explained with the variation in $x$. 
-   Used as a metric for predictive ability and model fit. 
-   Can increase when adding more predictors. 

The R^2^ of a given model (and observed data) is calculated as $$R^2 = \frac{MSS}{TSS}= 1 - \frac{RSS}{TSS},$$ where $RSS$ is the residual sum of squares and $TSS$ is the total sum o squares.

-   Write out formula on whiteboard.

##### Adjusted R^2^

The adjusted R^2^ also penalizes the addition of extra parameters 

$$R^2_{adj} = R^2 - (1 - R^2) \frac{p-1}{n-p},$$ 

where $R^2$ is the one defined above, $p$ is the number of parameters and $n$ is the total number of observations. 

##### Out-of-Sample R^2^

- The issue with R^2^ 
- Bootstrapped R^2^ 
- Reference: [Out-of-Sample R^2^: Estimation and Inference](https://www.tandfonline.com/doi/abs/10.1080/00031305.2023.2216252) 


#### Akaike Information Criterion (AIC)

- Used as a metric for predictive ability and model fit. 
- Lower value is better. 
- Values are always compared to other models (i.e., there are no general rules about reasonable AIC values). 

The AIC of a given model $M$ and observed data $\mathbf{y}$ is calculated as  
$$AIC_M = 2p - 2\log(\hat{L}),$$  
$p$ is the number of parameters estimated in the model and $\hat{L}$ is the maximized value of the likelihood function for the model (i.e., $\hat{L}=p(\mathbf{y}|\hat{\boldsymbol\beta}, M)$).

#### Bayesian Information Criterion (BIC)

The BIC of a given model (and observed data) is a variant of AIC and is calculated as  

$$BIC = p\log(n) - 2\log(\hat{L}),$$  

where $p$ is the number of parameters estimated in the model, $n$ is the number of observations, and $\hat{L}$ is the maximized value of the likelihood function for the model (i.e., $\hat{L}=p(\mathbf{y}|\hat{\boldsymbol\beta}, M)$).


#### Regularization 

##### Model-based model selection  

- These methods are sometimes a useful alternative for scenarios where the predictors present collinearity.  
- regularization, shrinkage, penalization.  

##### Refresh: Ordinary Least Squares Regression

$$\hat{\beta}_{OLS}=(\mathbf{X}^\top\mathbf{X})^{-1}\mathbf{X}^\top\mathbf{y}$$

##### Ridge Regression  

- Solve $\underset{\beta}{\text{argmin}} = (\mathbf{y}-\mathbf{X}\beta)^\top(\mathbf{y}-\mathbf{X}\beta)+\lambda(\beta^\top \beta - c)$.   
- $\hat{\beta}_{\text{Ridge}}=(\mathbf{X}^\top\mathbf{X}+\lambda\mathbf{I})^{-1}\mathbf{X}^\top\mathbf{y}$.
- $\hat{\beta}_{\text{Ridge}}$ for $\lambda = 0$.  
- As the ridge parameter $\lambda$ increases, the estimates can become close to zero.  
- Useful for cases with collinearity and unstable $\hat{\beta}_{OLS}$.  
- Pick the smallest value of $\lambda$ that produces stable estimates of $\beta$.  
- There are also automatic selections of $\lambda$.  


##### Lasso Regression  

- Lasso = Least absolute shrinkage and selection operator  
- Solve $\underset{\beta_0, \beta }{\text{argmin}} = \left\{ \sum_{i=1}^{N}(y_i - \beta_0 - x_i^T \beta)^2 \right\} \text{subject to} \sum_{j=1}^{p}|\beta_j| \leq t$. 
- $\hat{\beta}$ does not have a closed form solution.  
- As penalization increases, the $\beta$ estimates can become zero. 
- Not great for cases with collinearity. Elastic net is a better alternative.  
- Problems with high-dimensional data (e.g., genetics data). Elastic net is a better alternative.  

##### Elastic Net Regression  

- Combines the L1 (Lasso) and L2 (Ridge) regularization penalties.  
- $\hat{\beta} \equiv \underset{\beta}{\text{argmin}}(||y-\mathbf{X}\beta||^2 +\lambda_2||\beta||^2 +\lambda_1||\beta||_1)$, where $||\beta||_1 = \sum_{j=1}^{p}|\beta_j|$. 
- Ridge and Lasso are special cases of Elastic Net.  

##### Some thoghts about regularization  

- Interpretation.  
- Objective of the study.   


### A few thoughts

- Model-based model selection 
- Most of the times we are using the data twice! 
- Other techniques, also depending on modeling objective.
- A useful question: do the results change? 
- Multi-model inference [[example](https://sites.warnercnr.colostate.edu/wp-content/uploads/sites/73/2017/05/Burnham-and-Anderson-2004-SMR.pdf)] 
- Iterating over a single model [[link](https://wildlife.onlinelibrary.wiley.com/doi/10.1002/jwmg.891)]

## Applied example 

### Example I: A designed experiment 

[Get R code](../scripts/12012025_model_selection.Rmd) 


### Example II: Not a designed experiment 


## References 

- Out of sample R^2^ [Hawinkel et al. (2024)](https://www.tandfonline.com/doi/abs/10.1080/00031305.2023.2216252)
- Model selection [Hooten and Hobbs (2015)](https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1890/14-0661.1) 



