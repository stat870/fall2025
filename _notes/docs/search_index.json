[["inference-estimability-degrees-of-freedom-hypothesis-tests-contrasts.html", "Day 8 Inference – Estimability, degrees of freedom, hypothesis tests, contrasts 8.1 Announcements 8.2 Review: Hypothesis tests, t-tests, F tests 8.3 Applied example 8.4 Estimability 8.5 Why are LMM best for analyzing unbalanced experiments/incomplete blocks 8.6 Wednesday", " Day 8 Inference – Estimability, degrees of freedom, hypothesis tests, contrasts 8.1 Announcements Highlight questions from Assignment 3: Why are degrees of freedom not always integer numbers? What is the best size of blocks when designing an experiment? Figure 8.1: Overview of today’s class. 8.2 Review: Hypothesis tests, t-tests, F tests 8.2.1 Hypothesis tests Classic statistics have been deeply associated to falsifiability in the advancement of Science. Hugely influenced by Karl Popper, the advancement of Science can be understood as a string of hypotheses that are constantly evaluated. In statistics, hypothesis tests evaluate whether a given statistic (i.e., a function of the data) is too extreme for the null hypothesis to be true. Normally, the null hypothesis is business as usual, and we want to evaluate if we just made a scientific discovery, or if there’s not enough evidence to say that there’s something different going on. Figure 8.2: Types of errors in hypothesis tests. 8.2.2 On p-values The p value is the probability of observing the [t/F] statistic under the null hypothesis. Often, we assume \\(\\alpha = 0.05\\), and reject \\(H_0\\) if \\(p&lt;\\alpha\\). ASA’s statement on p-values Scientists rise up against statistical significance Discussion Counterargument: In defense of p-values Gelman “Let us have the serenity to embrace the variation that we cannot reduce, the courage to reduce the variation we cannot embrace, and the wisdom to distinguish one from the other.” [see talk] 8.2.3 t-tests Usually used to test whether one value/parameter/linear combination of parameters is different to a certain number. Steps: Define the target quantity \\(\\theta\\). For example, \\(\\theta = \\beta_1-\\beta_2\\). After observing the data, estimate \\(\\hat{\\boldsymbol\\beta}\\) and \\(se(\\hat{\\boldsymbol\\beta})\\). Calculate the test \\(t\\) statistic as \\(t^\\star = \\frac{\\hat\\theta}{se(\\hat\\theta)}\\). Compare \\(t^{\\star}\\) to \\(t_{1-\\frac{\\alpha}{2}, dfe}\\). Why \\(t\\) distribution? \\(N\\) for known se, \\(t\\) for \\(\\hat{se}\\). Note that \\(t^\\star\\) depends on: sample size (\\(dfe\\)) experiment design (other df) the variability \\(\\sigma^2\\) associated to that system. Note that \\(t_{crit}\\) depends on \\(dfe\\). 8.2.4 F-tests are multivariate t-tests Test the relevance of multiple factors at once. Consistent with the “bins” concept of ANOVA. Usually used to test whether a group of factor is relevant to explain variability in the data. Why \\(F\\) distribution? Ratio between two independent \\(\\chi^2\\) distributions divided by their df. Steps: Calculate Mean Squared Errors for the different sources of variability \\(= \\frac{SSH}{df}\\). Calculate the F statistic as the ratio between mean squares, \\(=\\frac{MSH}{MSE}\\). Compute \\(P(F&gt;F^\\star)\\), using the degrees of freedom of both mean squares, df of the numerator and df of the denominator. Note, again, the relationship between the p value and sample size and \\(\\sigma^2\\). Note, again, that the \\(P(F&gt;F^\\star)\\) depends on the df of the denominator. 8.3 Applied example A baker wanted to determine the effect that the amount of fat in a recipe of cookie dough would have on the texture of the cookie. She also wanted to determine if the temperature (°F) at which the cookies were baked would have an influence on the texture of the surface. The texture of the cookie is measured by determining the amount of force (g) required to penetrate the cookie surface. Experimentation: The process she used was to make a batch of cookie dough for each of the four recipes every day, and baked them one by one in the oven in different batches. She carried this process out each of four days when she baked cookies at three different temperatures. library(tidyverse) library(lme4) library(emmeans) df &lt;- read.csv(&quot;../data/cookies.csv&quot;) df$Temperature &lt;- as.factor(df$Temperature) df$fat_perc &lt;- as.factor(df$fat_perc) df$Day &lt;- as.factor(df$Day) df %&gt;% ggplot(aes(Temperature, force))+ theme_classic()+ scale_fill_manual(values = c(&quot;#DCD6F7&quot;, &quot;#B4869F&quot;, &quot;#985F6F&quot;, &quot;#4E4C67&quot;))+ labs(x = &quot;Temperature (°F)&quot;, fill = &quot;Fat (%)&quot;, y = &quot;Force (g)&quot;)+ theme(aspect.ratio = .5)+ geom_boxplot(aes(group = paste(Temperature, fat_perc), fill = factor(fat_perc))) The corresponding model is \\[y_{ijk} = \\mu + T_i + F_j + TF_{ij} + d_k + \\varepsilon_{ijk} , \\\\ d_k \\sim N(0, \\sigma^2_d), \\\\ \\varepsilon_{ijk} \\sim N(0, \\sigma^2_\\varepsilon). \\] m &lt;- lmer(force ~ 1 + Temperature*fat_perc + (1|Day), data = df) 8.3.1 t-test Objective: compare treatment means between 375 degrees temperature and 4% fat and 400 degrees temperature and 4% fat. Remember \\(t^\\star = \\frac{\\hat\\theta}{se(\\hat\\theta)}\\) \\(\\hat\\theta = \\mu_{22} - \\mu_{32}\\) \\(se(\\hat\\theta) = \\sqrt{\\frac{2 \\sigma^2_\\varepsilon}{r}}\\) R code to get t statistics by hand d &lt;- data.frame(Temperature = as.factor(c(375, 400)), fat_perc = factor(4)) hat.mu &lt;- predict(m, newdata = d, re.form = NA) hat.theta &lt;- hat.mu[2]-hat.mu[1] hat.sigma2 &lt;- sigma(m)^2 hat.sigma2_b &lt;- as.data.frame(VarCorr(m))[1,4] reps &lt;- n_distinct(df$Day) se.hat.theta &lt;- sqrt((2* hat.sigma2)/reps) emmeans(m, ~Temperature:fat_perc, contr = list(c(0, 0, 0, 0, -1, 1, rep(0, 6))))$contrasts ## contrast estimate SE df t.ratio p.value ## c(0, 0, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0) 2.92 0.633 33 4.621 0.0001 ## ## Degrees-of-freedom method: kenward-roger hat.theta ## 2 ## 2.925 se.hat.theta ## [1] 0.6330342 t.star &lt;- hat.theta/se.hat.theta t.star ## 2 ## 4.620603 dt(t.star, df = 33) ## 2 ## 8.202921e-05 8.3.2 F-test Objective: find out if temperature, in general, affects the texture of the cookie. \\(H_0:\\) all temperatures have the same effect on texture. \\(H_a:\\) at least one temperature has a different effect on texture. \\(F^\\star = \\frac{MS_{Temp}}{MS_{resid}}\\) \\(F_{critical} = F_{1-\\alpha, df_{numerator}, df_{denominator}}\\) R code to get t statistics by hand m.full &lt;- lmer(force ~ 1 + Temperature*fat_perc + (1|Day), data = df) m.TF &lt;- lmer(force ~ 1 + Temperature + fat_perc + (1|Day), data = df) m.T &lt;- lmer(force ~ 1 + Temperature + (1|Day), data = df) m.F &lt;- lmer(force ~ 1 + fat_perc + (1|Day), data = df) m.n &lt;- lmer(force ~ 1 + (1|Day), data = df) # SS II residual sum(resid(m.full)^2) ## [1] 26.84223 # SS II TxF sum((predict(m.full, re.form = NA) - predict(m.TF, re.form = NA))^2) ## [1] 1.505 # SS II F sum((predict(m.TF, re.form = NA) - predict(m.T, re.form = NA))^2) ## [1] 3.665 # SS II T sum((predict(m.TF, re.form = NA) - predict(m.F, re.form = NA))^2) ## [1] 179.5317 # P(F&gt;Fcrit) for temperature F.T &lt;- (179.5317/2)/(26.44833/33) 1 - pf(F.T, df1 = 2, df2 = 33) ## [1] 1.998401e-15 # P(F&gt;Fcrit) for fat% F.F &lt;- (3.665/3)/(26.44833/33) 1 - pf(F.F, df1 = 3, df2 = 33) ## [1] 0.2264496 # P(F&gt;Fcrit) for TxF F.TF &lt;- (1.505/6)/(26.44833/33) 1 - pf(F.TF, df1 = 6, df2 = 33) ## [1] 0.9256322 8.4 Estimability A quantity is estimable if it is possible to find a linear combination of the data that provides an unbiased and unique estimate of that quantity. For matrix models, linear estimable functions of \\(\\boldsymbol\\beta\\) take on the form of linear combinations of the parameter vector such as \\(a&#39;\\boldsymbol\\beta\\) where a is a \\(p\\times1\\) vector of constants. A linear function \\(a&#39;\\boldsymbol\\beta\\) is estimable if and only if there exists a vector \\(r\\) such that \\(a = \\mathbf{X}&#39;\\mathbf{X}r\\). Each function \\(x_i&#39;\\boldsymbol\\beta\\) is estimable where \\(x_i\\) is the \\(i\\)th row of \\(\\mathbf{X}\\). 8.4.1 Contrasts Linear combinations of parameters. Often connected to hypothesis tests where \\(H_0: \\mathbf{K}\\boldsymbol\\beta = 0\\). 8.4.2 Adjusting degrees of freedom for mixed models Contrasts for balanced &amp; complete designs have straightforward calculation of degrees of freedom. Sometimes, estimating the df is not as straightforward. Unbalanced designs, incomplete blocks, missing data cases. Satterthwaite df approximation Kenward-Rogers df approximation Variance-component-only LMMs with balanced data and their compound symmetry marginal model equivalents, the estimated covariance of \\(\\beta \\mathbf{K}\\) will tend to be underestimated 8.5 Why are LMM best for analyzing unbalanced experiments/incomplete blocks Incomplete block designs 8.6 Wednesday Come early and bring your computer "]]
