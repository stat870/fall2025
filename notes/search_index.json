[["index.html", "STAT 870 - Analysis of Messy Data Day 1 Welcome to STAT 870! 1.1 About this course: 1.2 Learning goals 1.3 Overview of the course 1.4 What is messy data anyways? 1.5 Linear Models review 1.6 On notation 1.7 Homework &amp; Announcements", " STAT 870 - Analysis of Messy Data Josefina Lacasa Fall 2025 Day 1 Welcome to STAT 870! August 25th, 2025 1.1 About this course: About me About you: Figure 1.1: Students of this class In rounds: What’s your major and what do you expect to learn in 870? 1.1.1 Logistics Website Syllabus Statistical programming requirements Rough mindmap of the course (on whiteboard) Semester project Grades: A (100-89.999999999(!!!)), B (89.99-79.99), C (79.99-69.99), D (69.99-59.99), F (&lt;59.99). Attendance policies &amp; kahoots 1.2 Learning goals By the end of this course, you should be able to: Identify the treatment design, experiment design, experimental unit and observational unit of simple and complex designed experiments. Distinguish the benefits/disadvantages of different experiment designs. Write the statistical model that corresponds to data generated by designed experiments. Write the Materials and Methods section in a paper (or thesis) that describes the designed experiment. 1.3 Overview of the course Figure 1.2: Mindmap 1.4 What is messy data anyways? Complex dependence structures Complex structure in covariates 1.5 Linear Models review Perhaps the most common model of all time (default in most software) is \\[y_{i} = \\mu_i + \\varepsilon_i, \\ \\varepsilon_i \\sim N(0, \\sigma^2),\\] where: \\(y_{i}\\) is the observed value for the \\(i\\)th observation, \\(\\mu_i\\) is the expected value for the \\(i\\)th observation, \\(\\varepsilon_i\\) is the residual (i.e., the difference between observed and expected). All residuals are iid normal. This example uses the model equation form. The model above can also be written usign the probability distribution form. The probability distribution form is much more flexible because it is compatible with other distributions beyond the normal. It goes like this: \\[y_{i} \\sim N(\\mu_i, \\sigma^2),\\] where the elements are the same as described above. Likewise, we can use the vectorized notation of the probability distribution form, and say that \\[\\mathbf{y} \\sim N(\\boldsymbol\\mu, \\sigma^2 \\mathbf{I}),\\] where: \\(\\mathbf{y} \\equiv [y_1, y_2, ..., y_n]&#39;\\) is the vector of observed values, \\(\\boldsymbol\\mu \\equiv [\\mu_1, \\mu_2, ..., \\mu_n]&#39;\\) is the vector of expected values, \\(\\sigma^2 \\mathbf{I}\\) is the variance-covariance matrix. Note that, more generally, we can call the variance-covariance matrix \\(\\boldsymbol{\\Sigma}\\) or \\(\\mathbf{V}\\). Note that \\[\\sigma^2 \\mathbf{I} = \\sigma^2 \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 1 \\end{bmatrix} = \\begin{bmatrix} \\sigma^2 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma^2 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\dots &amp; \\sigma^2 \\end{bmatrix}.\\] The assumptions behind this model are: Normal distribution of the data Constant variance Independence Linearity In this course, we will mostly deal with cases where: The assumption of independence does not hold (basic and complex designed experiments), The assumption of normality does not make sense (e.g., data are counts, proportions, or stricly positive), The assumption of constant variance does not hold/make sense (e.g., larger biomass is associated to larger variance of said biomass). We will approach most problems with a 3-step approach (Chapter 2 in Stroup et al.): What is the distribution of the data? What is the link function? What is the blueprint of the design/data? Figure 1.3: Common variable distributions. Page 60 in Stroup et al. (2024) 1.6 On notation scalars: \\(y\\), \\(\\sigma\\), \\(\\beta_0\\) vectors: \\(\\mathbf{y} \\equiv [y_1, y_2, ..., y_n]&#39;\\), \\(\\boldsymbol{\\beta} \\equiv [\\beta_1, \\beta_2, ..., \\beta_p]&#39;\\), \\(\\boldsymbol{u}\\) matrices: \\(\\mathbf{X}\\), \\(\\Sigma\\) probability distribution: \\(y \\sim N(0, \\sigma^2)\\), \\(\\mathbf{y} \\sim N(\\boldsymbol{0}, \\sigma^2\\mathbf{I})\\). 1.7 Homework &amp; Announcements Install R and RStudio. Submit Assignment 1 by next Monday. Read Chapter 2 in Stroup et al. (2024) Applied Linear Mixed Models Workshop - Sept 13 &amp; 14 "],["designed-experiments-review.html", "Day 2 Designed Experiments Review 2.1 Announcements 2.2 Designed experiments 2.3 Homework &amp; Announcements", " Day 2 Designed Experiments Review August 27th 2.1 Announcements Assignment 1 is due next Wednesday. Applied Linear Mixed Models Workshop 09/13-09/14. Workshop is full. Email me if you still want to join. 2.2 Designed experiments The golden rules of designed experiments: Replication Randomization Local control Figure 2.1: Principles for conducting valid and efficient experiments. From Box, Hunter and Hunter (2005). About George Box I - A Conversation with George Box [link] About George Box II - An Accidental Statistician: The Life and Memories of George E. P. Box [link] 2.2.1 Treatment structure Solely about the treatment. Most likely connected to the research question and thus defined by the subject matter expert. Figure 2.2: Schematic representation of a one-way treatment structure. A one-way treatment structure means that the expected value of the observations, \\(y\\), is affected by one treatment factor with \\(k\\) levels: \\[g(\\mu_{ij}) = \\eta_{ij} = \\eta_{0} + T_i,\\] where: \\(\\mu_{ij}\\) is the expected value of \\(y_{ij}\\), the observation of the \\(i\\)th treatment and \\(j\\)th repetition, \\(g(\\cdot)\\) is the link function, that ensures that the mean is within the support of the chosen distribution, \\(\\eta_{ij}\\) is the linear predictor of \\(g(\\mu_{ij})\\), \\(\\eta_{0}\\) is the overall mean, and \\(T_i\\) is the effect of the \\(i\\)th level of the treatment \\(T\\). Figure 2.3: Schematic representation of a two-way factorial treatment structure. A two-way treatment structure is similar to a one-way treatment structure, only now the expected value is affected by two treatment factors: \\[g(\\mu_{ijk}) = \\eta_{ijk} = \\eta_{0} + T_i + G_j + (TG)_{ij} ,\\] where: \\(\\mu_{ijk}\\) is the expected value of \\(y_{ij}\\), the observation of the \\(i\\)th treatment \\(T\\), \\(j\\)th treatment \\(G\\), and \\(k\\) repetition, \\(g(\\cdot)\\) is the link function, that ensures that the mean is within the support of the chosen distribution, \\(\\eta_{ijk}\\) is the linear predictor of \\(g(\\mu_{ijk})\\), \\(\\eta_{0}\\) is the overall mean, \\(T_i\\) is the effect of the \\(i\\)th level of the treatment \\(T\\), \\(G_j\\) is the effect of the \\(j\\)th level of the treatment \\(G\\), and \\((TG)_{ij}\\) is the interaction between the \\(i\\)th level of treatment \\(T\\), and \\(j\\)th level of treatment \\(G\\). Figure 2.4: Schematic representation of a tree-way factorial treatment structure. 2.2.2 Design structure The experiment design describes the data generating process (“blueprint” of the data). You’ll notice that the models above only consider the treatment sources of variability to explain variations in the observed values. How were the treatments above (logistically) applied? Useful questions: What is the experimental unit? What is the blueprint of the design? (which observations are similar to what) 2.3 Homework &amp; Announcements Submit Assignment 1 by next Wednesday. Applied Linear Mixed Models Workshop is full. Email me if you still want to join. "],["designed-experiments-review-1.html", "Day 3 Designed Experiments Review 3.1 Announcements 3.2 Semester project 3.3 Designed experiments 3.4 Applied example: fungicide effects on barley genotypes 3.5 Homework &amp; Announcements", " Day 3 Designed Experiments Review September 3rd 3.1 Announcements Assignment 1 is due today! Assignment 2 is posted and due next week. Applied Linear Mixed Models Workshop 09/13-09/14. Workshop is full. Email me if you still want to join. Read Chapter 2 of Stroup’s GLMM book (this chapter only, 1ed&gt;2ed). 3.2 Semester project Proposal due September 24th – see example 1 | see example 2 Must contain: Background, Objectives. Data for the project must be ready to go. Final product: Manuscript + Tutorial. Working in teams vs. solo. Start as soon as possible! 3.3 Designed experiments Let’s review the basics of designed experiments: Research question Planning the experiment Treatment structure - directly dependent on the research question Design structure - mostly depending on logistics. May depend on the research question. 3.3.1 Design structures While the treatment structure defines what effects we will try and estimate as precisely as possible, the design structure tells us the story of how those treatments were applied. Some of the most common designed experiments include completely randomized design (CRD), randomized complete block design (RCBD), split-plot design, relevant variations to the designs above: generalized randomized complete block design, strip-plot design, repeated measures designs. Figure 3.1: Schematic description of an experiment with a completely randomized design Figure 3.2: Schematic description of an experiment with a randomized complete block design Figure 3.3: Schematic description of a field experiment with a split-plot design 3.4 Applied example: fungicide effects on barley genotypes Let’s consider an experiment that aimed to study the effect of fungicide and barley genotypes on grain yield. The study includes two levels of fungicide, and 70 different genotypes, considering all their possible combinations. What is the treatment structure of the experiment? Do we know the design structure of the experiment? Now, the actual experiment was run like this: library(tidyverse) library(agridat) library(lme4) library(emmeans) df_fungicide &lt;- agridat::durban.splitplot df_fungicide %&gt;% ggplot(aes(bed, row))+ coord_fixed()+ geom_tile(aes(fill = fung))+ geom_tile(aes(), color = &quot;black&quot;, fill = NA) What is the treatment structure? What are the experimental units? How many independent observations per treatment factor? What is the design structure? Download R code 3.5 Homework &amp; Announcements Assignment 1 due today. Applied Linear Mixed Models Workshop is full. Email me if you still want to join. "],["linear-mixed-models.html", "Day 4 Linear mixed models 4.1 Announcements 4.2 Statistical models 4.3 The general linear model 4.4 Linear mixed models applied to designed experiments 4.5 Homework", " Day 4 Linear mixed models September 8th 4.1 Announcements Assignment 1 grades are posted. Remember that you have one chance to resubmit for full points. Assignment 2 is due on Wednesday. Last chance to join the mixed models workshop this weekend. Semester project: Proposal due September 24th – see example 1 | see example 2 Must contain: Background, Objectives. Data for the project must be ready to go. Final product: Manuscript + Tutorial. Working in teams vs. solo. Start as soon as possible! This week Today: intro to mixed models + blocks modeling discussion Wednesday: mixed models applied to the fungicide experiment + Kahoot 4.2 Statistical models Statistical models can be typically defined as the combination of a deterministic component (e.g., the linear predictor), and a random component (e.g., the distribution of the data). Learn more about this way of describing statistical models in Stroup et al. (2024) page 4 (and all of Chapter 1 in general). 4.3 The general linear model The general linear model is one of the most commonly used statistical models. We call it “general” because it assumes that the data arise from a normal distribution. [Note: Do not mix up “general” (normal distribution) with “generalized” (multiple possible distributions).] We call it “linear” because the parameters enter the model linearly. For example, the predictor \\(\\beta_0 +\\beta_1 x + \\beta_2 x^2\\) corresponds to a linear model, because the parameters enter linearly. The predictor \\(\\alpha \\exp(\\beta x)\\) corresponds to a nonlinear model, because the parameters enter as the power of the data. The general linear model can be written as \\[\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol\\varepsilon, \\\\ \\boldsymbol{\\varepsilon}\\sim N(\\boldsymbol{0}, \\mathbf{\\Sigma}),\\] where \\(\\mathbf{y}\\) is the vector of the data, \\(\\mathbf{X}\\) is the matrix containing the data, \\(\\boldsymbol{\\beta}\\) is a vector containing all parameters, \\(\\boldsymbol\\varepsilon\\) is a vector containing the residuals (i.e., the difference between observed and predicted). Note that \\(\\boldsymbol\\varepsilon\\) arises from a multivariate normal distribution, with mean zero and a variance-covariance matrix \\(\\mathbf{\\Sigma} = \\sigma^2 \\mathbf{I}\\). Recall some cool properties: The least squares estimate of \\(\\beta\\), \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\), is also the the MLE, is unbiased. \\(\\hat\\sigma^2 = \\frac{SSE}{df_e}\\), not the MLE, is the unbiased estimate of \\(\\sigma^2\\). \\(CI(\\hat\\mu_{ij}) = \\hat\\mu_{ij} \\pm t_{df, \\frac{\\alpha}{2}} \\cdot se(\\hat{\\mu}_{ij})\\), where \\(\\sigma^2_e\\) is the residual variance, and \\(r\\) is the number of repetitions. Under a CRD, \\(se(\\mu_{ij}) = \\sqrt{\\frac{\\sigma^2_e}{r}}\\), where \\(\\sigma^2_e\\) is the residual variance, and \\(r\\) is the number of repetitions. ANOVA tables help us distinguish the most important sources of variability in the data. See Gelman (2005) Essentially, an analysis of variance (ANOVA) divides the predictors into bins and analyzes whether each bin is relevant to explain variability in the data. Typically, ANOVA tables show the sources of variation, their degrees of freedom, an F statistic and, sometimes, a p-value. Sources of variation may come from treatment factors, or elements of the design. Degrees of freedom tells us how many independent variables there are. The F statistic serves as a multivariate t statistic, since it is used to test multiple predictors. 4.3.1 Applied example Swine nutritionists are studying the effect of dietary treatments on the % of fecal dry matter, and whether said effects change at different points in time. In this experiment, 450 pigs were allocated in pens of 5 pigs. Each pen is independently assigned with dietary treatment. There are 6 dietary treatments: negative C, positive C, and 4 different pre-commercial additives. So, there are 45 pens per treatment. One (randomly selected) pig per pen was observed at 3 points in time. What is the experimental unit? What is the observational unit? What is the treatment structure? What is the design structure? A reasonable model could be \\[y_{ijk} \\sim N(\\mu_{ijk}, \\sigma^2),\\] \\[\\mu_{ijk} = \\eta_{ijk},\\] \\[\\eta_{ijk} = \\eta_0 + D_i + T_j + (DT)_{ij},\\] where: \\(y_{ijk}\\) is the observed dry matter (%) for the \\(i\\)th dietary treatment, \\(j\\)th time, and \\(k\\)th repetition, \\(\\mu_{ijk}\\) is the expected value for the \\(i\\)th dietary treatment, \\(j\\)th time, and \\(k\\)th repetition, \\(\\sigma^2\\) is the residual variance, \\(\\eta_{ijk}\\), the linear predictor is equivalent to the expected value because the link function is the identity function, \\(\\eta_0\\) is the overall mean of the linear predictor, \\(F_i\\) is the effect of the \\(i\\)th dietary treatment, \\(G_j\\) is the effect of the \\(j\\)th time, \\((FG)_{ij}\\) is the interaction for the \\(i\\)th dietary treatment and \\(j\\)th time A reasonable ANOVA shell could be: Table 4.1: ANOVA table for a CRD with a two-way factorial treatment structure. Source df Diet d-1 Time t-1 D x T (d-1)(t-1) Error N-(d*t) Total N-1 Table 4.1: ANOVA table for the diet CRD. Source df Diet 5 Time 2 D x T 10 Error 252 Total 269 Let’s implement that model: # load the data pigs_crd &lt;- read.csv(&quot;../../data_confid/fecalm_swine_crd.csv&quot;) pigs_crd$Day &lt;- as.factor(pigs_crd$Day) # fit the model m &lt;- lm(F.Dry.matter ~ Day*Trt, data = pigs_crd) Model checks plot(predict(m), abs(resid(m))) car::qqPlot(m) ## [1] 109 148 # anova car::Anova(m, type = 2, test.statistic = &quot;F&quot;) ## Anova Table (Type II tests) ## ## Response: F.Dry.matter ## Sum Sq Df F value Pr(&gt;F) ## Day 850.7 2 8.6720 0.0002279 *** ## Trt 896.5 5 3.6556 0.0032772 ** ## Day:Trt 673.1 10 1.3723 0.1934086 ## Residuals 12360.9 252 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can even get the standard errors and, together with the degrees of freedom, get the 95% confidence intervals. library(emmeans) # get the s.e. by hand n.reps &lt;- 45 sigma2.hat &lt;- sigma(m)^2 # here you have the s.e. (se.mu &lt;- sqrt(sigma2.hat/n.reps)) ## [1] 1.044041 # get the CI # get the mean mu &lt;- as.data.frame(emmeans(m, ~Trt, at = list(Trt = &quot;A&quot;)))[,2] # get the t statistic for the CI t.star &lt;- qt(p = .975, df = m$df.residual) (lower.CI_TrtA &lt;- mu - t.star*se.mu) ## [1] 16.40749 (upper.CI_TrtA &lt;- mu + t.star*se.mu) ## [1] 20.5198 mg_means &lt;- emmeans(m, ~Trt) head(mg_means) ## Trt emmean SE df lower.CL upper.CL ## A 18.5 1.04 252 16.4 20.5 ## B 23.6 1.04 252 21.5 25.6 ## C 20.1 1.04 252 18.1 22.2 ## D 18.3 1.04 252 16.3 20.4 ## E 19.8 1.04 252 17.8 21.9 ## F 18.6 1.04 252 16.5 20.6 ## ## Results are averaged over the levels of: Day ## Confidence level used: 0.95 4.4 Linear mixed models applied to designed experiments When the independence assumption cannot be taken for granted, especially if we know the dependence patterns in the data, we must account for said dependence pattern. Take the previous example, but in a blocked design instead of a CRD. The blocks are basically rooms that hold 90 pens. This means that the observations are not really independent anymore. After all, it’s hard to find rooms that fit 90 pens! Consider an experiment with the same treatments, same number of EUs, but ran in 3 different rooms with 30 pens each. Each room has 5 pens of each dietary treatment. What is the experimental unit? What is the observational unit? What is the treatment structure? What is the design structure? Let’s re-do the statistical model and ANOVA shell: A reasonable model could be \\[y_{ijkl}|b_k \\sim N(\\mu_{ijk}, \\sigma^2),\\] \\[\\mu_{ijkl} = \\eta_{ijkl},\\] \\[\\eta_{ijkl} = \\eta_0 + D_i + T_j + (DT)_{ij} + b_k,\\] where: \\(y_{ijk}\\) is the observed dry matter (%) for the \\(i\\)th dietary treatment, \\(j\\)th time, and \\(k\\)th room and \\(l\\)th repetition, \\(\\mu_{ijk}\\) is the expected value for the \\(i\\)th dietary treatment, \\(j\\)th time, and \\(k\\)th room and \\(l\\)th repetition, \\(\\sigma^2\\) is the residual variance, \\(\\eta_{ijk}\\), the linear predictor is equivalent to the expected value because the link function is the identity function, \\(\\eta_0\\) is the overall mean of the linear predictor, \\(F_i\\) is the effect of the \\(i\\)th dietary treatment, \\(G_j\\) is the effect of the \\(j\\)th time, \\((FG)_{ij}\\) is the interaction for the \\(i\\)th dietary treatment and \\(j\\)th time, \\(b_k\\) is the effect of the \\(k\\)th room, \\(b_k \\sim N(0, \\sigma^2_b)\\). A reasonable ANOVA shell could be: Table 4.2: ANOVA table for a GRCBD with a two-way factorial treatment structure. Source df Room r-1 Diet d-1 Time t-1 D x T (d-1)(t-1) Error N-(d*t)-r Total N-1 Table 4.2: ANOVA table for the diet GRCBD. Source df Room 2 Diet 5 Time 2 D x T 10 Error 250 Total 269 Let’s implement that model: # load the data pigs_grcbd &lt;- read.csv(&quot;../../data_confid/fecalm_swine_grcbd.csv&quot;) pigs_grcbd$Day &lt;- as.factor(pigs_grcbd$Day) pigs_grcbd$Room &lt;- as.factor(pigs_grcbd$Room) # fit the model library(lme4) m_random &lt;- lmer(F.Dry.matter ~ Day*Trt + (1|Room), data = pigs_grcbd) Model checks plot(predict(m_random), abs(resid(m_random))) # anova car::Anova(m_random, type = 2, test.statistic = &quot;F&quot;) ## Analysis of Deviance Table (Type II Wald F tests with Kenward-Roger df) ## ## Response: F.Dry.matter ## F Df Df.res Pr(&gt;F) ## Day 10.7115 2 250 3.441e-05 *** ## Trt 4.7730 5 250 0.0003483 *** ## Day:Trt 1.1153 10 250 0.3509474 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Notice that the 252 degrees of freedom (aka independent observations) from the CRD are now spread across different levels, connected to different variance components, blocks and random error. 4.4.1 General notation for linear mixed models The standard notation for mixed models is \\[\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{Z}\\boldsymbol{u} + \\boldsymbol\\varepsilon, \\\\ \\boldsymbol{u}\\sim N(\\boldsymbol{0}, \\mathbf{G}), \\\\ \\boldsymbol{\\varepsilon}\\sim N(\\boldsymbol{0}, \\mathbf{R}),\\] where: \\(\\mathbf{y}\\) is the vector containing the data, \\(\\boldsymbol{\\beta}\\) is the vector containing the model fixed effects (usually treatments), \\(\\mathbf{X}\\) is the matrix containing information about the treatment allocation, \\(\\boldsymbol{u}\\) is the vector containing the model random effects (usually elements of the design). Note that \\(\\boldsymbol{u}\\) arises from a normal distribution with mean 0 and variance-covariance matrix \\(\\mathbf{G})\\). \\(\\mathbf{Z}\\) is the matrix containing information about the design, \\(\\boldsymbol\\varepsilon\\) is the model residual. Note that \\(\\boldsymbol{\\varepsilon}\\) arises from a normal distribution with mean 0 and variance-covariance matrix \\(\\mathbf{R})\\). Note that the conditional distribution of \\(\\mathbf{y}\\) is \\[\\mathbf{y} \\vert \\boldsymbol{u} \\sim N(\\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{Z}\\boldsymbol{u},\\mathbf{R}).\\] The marginal distribution of \\(\\mathbf{y}\\) is \\[\\mathbf{y} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\mathbf{Z}\\mathbf{G}\\mathbf{Z}&#39; + \\mathbf{R}).\\] So, essentially what happened before was that the treatment effects haven’t changed, their effect sizes and means haven’t changed, but the variance-covariance matrix did change. A couple things did change: Shrinkage Estimation of \\(\\sigma^2\\) Estimation of \\(se(\\hat\\mu)\\) Degrees of freedom change 4.4.2 Class discussion: Blocks - random or fixed? Treatment point estimates don’t change whether blocks are fixed or random. m_fixed &lt;- lm(F.Dry.matter ~ Day*Trt + Room, data = pigs_grcbd) mg_means_fixed &lt;- emmeans(m_fixed, ~Trt, contr = list(c(1, -1, 0, 0, 0, 0))) ## NOTE: Results may be misleading due to involvement in interactions mg_means_random &lt;- emmeans(m_random, ~Trt, contr = list(c(1, -1, 0, 0, 0, 0))) ## NOTE: Results may be misleading due to involvement in interactions as.data.frame(mg_means_fixed$emmeans) %&gt;% mutate(blocks = &quot;fixed&quot;) %&gt;% bind_rows(as.data.frame(mg_means_random$emmeans) %&gt;% mutate(blocks = &quot;random&quot;)) %&gt;% ggplot(aes(Trt, emmean))+ geom_errorbar(aes(ymin = emmean-SE, ymax = emmean+SE, color = blocks), position = position_dodge(width = .2), width = 0)+ geom_text(aes(x = 4.5, y = 22.75), label = &quot;s.e.(difference)&quot;)+ geom_errorbar(aes(x = 5.4, y = 22, ymin = 22 , ymax = 22 + SE), width = 0.1, data = as.data.frame(mg_means_random$contrasts))+ geom_errorbar(aes(x = 5.2, y = 22, ymin = 22 , ymax = 22 + SE), width = 0.1, data = as.data.frame(mg_means_fixed$contrasts))+ geom_point(aes(color = blocks), position = position_dodge(width = .2))+ theme_classic()+ labs(color = &quot;Blocks modeled as&quot;, y = &quot;Fecal DM (%)&quot;, x = &quot;Treatment&quot;)+ theme(legend.position = &quot;bottom&quot;) ## Warning in geom_text(aes(x = 4.5, y = 22.75), label = &quot;s.e.(difference)&quot;): All aesthetics have length 1, but the data has 12 rows. ## ℹ Please consider using `annotate()` or provide this layer with data containing a single row. More references: Dixon (2016). Gelman (2005) 4.5 Homework Read Chapter 2 in Stroup’s GLMM book (1st ed. superior to 2nd ed.), particularly the “What Would Fisher Do” ANOVA shells. "],["linear-mixed-models-1.html", "Day 5 Linear mixed models 5.1 Announcements 5.2 Linear mixed models 5.3 Next week", " Day 5 Linear mixed models September 10th 5.1 Announcements Assignment 2 is due today! 5.2 Linear mixed models Mixed models come in handy when the (in)famous independence assumption cannot be taken for granted, especially if we know the dependence patterns in the data. We typically know the dependence patterns in the data when running an experiment. For example, the fungicide-barley experiment. Let’s re-do the statistical model and ANOVA shell: A reasonable model could be \\[y_{ijk}|b_k, u_{i(k)} \\sim N(\\mu_{ijk}, \\sigma_e^2),\\] \\[\\mu_{ijk} = \\eta_{ijk},\\] \\[\\eta_{ijk} = \\eta_0 + F_i +G_j + (FG)_{ij}+b_k+ u_{i(k)},\\] where: \\(y_{ijk}\\) is the observed yield for the \\(i\\)th fungicide treatment, \\(j\\)th genotype treatment, and \\(k\\)th repetition, \\(\\mu_{ijk}\\) is the expected value for the \\(i\\)th fungicide treatment, \\(j\\)th genotype treatment, and \\(k\\)th repetition, \\(\\sigma^2\\) is the residual variance, \\(\\eta_{ijk}\\), the linear predictor is equivalent to the expected value because the link function is the identity function, \\(\\eta_0\\) is the overall mean of the linear predictor, \\(F_i\\) is the effect of the \\(i\\)th fungicide treatment, \\(G_j\\) is the effect of the \\(j\\)th genotype treatment, \\((FG)_{ij}\\) is the interaction for the \\(i\\)th fungicide treatment and \\(j\\)th genotype treatment, \\(b_k\\) is the effect of the \\(k\\)th block, \\(b_k \\sim N(0, \\sigma^2_b)\\) \\(u_{i(k)}\\) is the effect of the whole plot corresponding to the \\(i\\)th fungicide treatment in the \\(k\\)th block, \\(u_{i(k)} \\sim N(0, \\sigma^2_u)\\). A reasonable ANOVA shell could be: Table 5.1: This would be the ANOVA under a CRD. Source df - - Fungicide f-1 = 1 - - Genotype g-1 = 69 Fung x Gen (f-1)(g-1) = 69 Error N-fg = 420 Total N-1 Table 5.1: ANOVA table for the fungicide-barley split-plot design. See also Table 24.9 from Milliken and Johnson as a helpful reference. Source df Block b-1 Fungicide f-1 Fungicide(Block) (b-1)(f-1) Genotype g-1 Fung x Gen (f-1)(g-1) Gen(Block x Fung) f(b-1)(g-1) Total N-1 Table 5.1: ANOVA table for the fungicide-barley split-plot design. Source df Block 3 Fungicide 1 Fungicide(Block) 3 Genotype 69 Fung x Gen 69 Gen(Block x Fung) 414 Total 559 Notice that the 420 degrees of freedom (aka independent observations) from the CRD ANOVA are now spread across different levels, connected to different variance components: blocks, whole plots, and random error. This means that there is a different number of independent observations for Fungicide compared to Genotype. The variance components will affect the inference distinctly: Standard errors for comparisons at the whole plot: \\(se(\\mu_{i\\cdot}-\\mu_{i&#39;\\cdot}) = \\sqrt{\\frac{2(\\sigma^2_e + g\\sigma^2_u)}{gb}}\\) Standard errors for comparisons at the split plot: \\(se(\\mu_{\\cdot j}-\\mu_{\\cdot j&#39;}) = \\sqrt{\\frac{2 \\sigma^2_e }{fb}}\\) Let’s apply that model: library(lme4) # load the data url &lt;- &quot;https://raw.githubusercontent.com/stat870/fall2025/refs/heads/main/data/fung_barley_sp.csv&quot; fung_sp &lt;- read.csv(url) fung_sp %&gt;% ggplot(aes(bed, row))+ coord_fixed()+ theme_minimal()+ geom_tile(aes(fill = fung))+ geom_tile(aes(), color = &quot;black&quot;, fill = NA) # fit the model m &lt;- lmer(yield ~ fung*gen + (1|block/fung), data = fung_sp) # anova car::Anova(m, type = 2, test.statistic = &quot;F&quot;) ## Analysis of Deviance Table (Type II Wald F tests with Kenward-Roger df) ## ## Response: yield ## F Df Df.res Pr(&gt;F) ## fung 40.2718 1 3 0.007915 ** ## gen 7.2008 69 414 &lt; 2.2e-16 *** ## fung:gen 0.9331 69 414 0.629010 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 5.2.1 Why do we call these multi-level models? The data are clustered in groups that were generated under similar conditions. What are those groups in the barley example? Figure 5.1: Intuitive visualization of a split-plot design model as a hierarchical (multilevel) model. 5.2.2 Degrees of freedom in a mixed model Degrees of freedom for means are not that easy and straightforward to compute under mixed models anymore. There are approximate methods to calculate the degrees of freedom, like Satterthwaite or Kenward-Roger. mg_means &lt;- emmeans(m, ~fung:gen) head(mg_means) ## fung gen emmean SE df lower.CL upper.CL ## F1 G01 5.24 0.174 31.4 4.88 5.59 ## F2 G01 4.58 0.174 31.4 4.23 4.94 ## F1 G02 5.38 0.174 31.4 5.02 5.73 ## F2 G02 4.75 0.174 31.4 4.39 5.11 ## F1 G03 6.53 0.174 31.4 6.18 6.89 ## F2 G03 5.62 0.174 31.4 5.26 5.97 ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 However, computing the degrees of freedom for the comparisons is much more straightforward (esp. because this a balanced design). Take a look at the ANOVA table, and the experimental units. sigma2_e &lt;- sigma(m)^2 sigma2_wp &lt;- as.data.frame(VarCorr(m))[1,]$vcov sigma2_b &lt;- as.data.frame(VarCorr(m))[2,]$vcov levels_wp &lt;- dplyr::n_distinct(fung_sp$fung) levels_sp &lt;- dplyr::n_distinct(fung_sp$gen) reps &lt;- dplyr::n_distinct(fung_sp$block) Differences between levels of the factor at the whole plot – \\(se(\\mu_{i\\cdot}-\\mu_{i&#39;\\cdot}) = \\sqrt{\\frac{2(\\sigma^2_e + g\\sigma^2_u)}{gb}}\\). # get the s.e. for comparisons between fungicide treatments by hand sqrt( 2*(sigma2_e + levels_sp*sigma2_wp) / (levels_sp*reps)) ## [1] 0.086331 emmeans(m, ~fung, contr = list(c(1, -1)))$contr ## NOTE: Results may be misleading due to involvement in interactions ## contrast estimate SE df t.ratio p.value ## c(1, -1) 0.548 0.0863 3 6.346 0.0079 ## ## Results are averaged over the levels of: gen ## Degrees-of-freedom method: kenward-roger Differences between levels of the factor at the split plot – \\(se(\\mu_{\\cdot j}-\\mu_{\\cdot j&#39;}) = \\sqrt{\\frac{2 \\sigma^2_e }{fb}}\\). # get the s.e. for comparisons between genotype treatments by hand sqrt( 2*(sigma2_e ) / (levels_wp*reps)) ## [1] 0.1405927 emmeans(m, ~gen, contr = list(c(1, -1, rep(0, 68))))$contr ## NOTE: Results may be misleading due to involvement in interactions ## contrast estimate SE df t.ratio ## c(1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.152 0.141 414 -1.085 ## p.value ## 0.2787 ## ## Results are averaged over the levels of: fung ## Degrees-of-freedom method: kenward-roger 5.3 Next week Monday: shrinkage, use of BLUPs versus BLUEs, REML Wednesday: GLMMs "],["linear-mixed-models-3.html", "Day 6 Linear Mixed Models 6.1 Announcements 6.2 Mixed Models Review 6.3 Estimation of parameters in Mixed Models 6.4 R demo", " Day 6 Linear Mixed Models September 15th 6.1 Announcements Linear mixed models workshop Assignment 3 is posted. See Google docs here. Assignment 2 grades will be posted this evening. Project proposal due Sept 24 (next week) Schedule an appointment Talk to your classmates to find a partner What’s that wooden cup I drink from: Lionel Messi drinking mate after winning the World Cup in 2022. Yerba mate Wikipedia Yerba Mate Tea (Ilex paraguariensis): A Comprehensive Review on Chemistry, Health Implications, and Technological Considerations. Heck and De Mejia, 2007. 6.2 Mixed Models Review Distributions of the data Linear predictor Models for some of the parameters (e.g., \\(u \\sim N(0, \\sigma^2_u)\\)). Mixed models are hierarchical models Sharing information between groups 6.3 Estimation of parameters in Mixed Models 6.3.1 Analysis of variance (ANOVA) ANOVA can be considered a special case of the linear model. Each row of the ANOVA table corresponds to a source of variation and the variance of a corresponding to a set of regression coefficients. ANOVA table for split-plot table { width: 100%; border-collapse: collapse; margin: 20px 0; } th, td { border: 1px solid #ddd; padding: 8px; text-align: left; } th { background-color: #f4f4f4; font-weight: bold; } tr:nth-child(even) { background-color: #f9f9f9; } Source df Sum of Squares (Partial or type III) Mean Square Expected Mean Square Block \\[r-1\\] \\[SS_b\\] \\[\\frac{SS_b}{df_b}\\] \\[\\sigma^2_{\\varepsilon}+c\\sigma^2_w+ac\\sigma^2_b\\] A \\[a-1\\] \\[SS_A\\] \\[\\frac{SS_A}{df_A}\\] \\[\\sigma^2_{\\varepsilon}+c\\sigma^2_w+\\phi^2(\\alpha)\\] Error(whole plot) \\[(r-1)(a-1)\\] \\[SS_w\\] \\[\\frac{SS_w}{df_w}\\] \\[\\sigma^2_{\\varepsilon}+c\\sigma^2_w\\] C \\[c-1\\] \\[SS_C\\] \\[\\frac{SS_C}{df_C}\\] \\[\\sigma^2_{\\varepsilon}+\\phi^2(\\gamma)\\] A x C \\[(a-1)(c-1)\\] \\[SS_{AC}\\] \\[\\frac{SS_{AC}}{df_{AC}}\\] \\[\\sigma^2_{\\varepsilon}+\\phi^2(\\alpha \\gamma)\\] Error(split plot) \\[a(r-1)(c-1)\\] \\[SS_e\\] \\[\\frac{SS_e}{df_e}\\] \\[\\sigma^2_{\\varepsilon}\\] See Gelman (2005). 6.3.2 Estimation of fixed effects Fixed effects are normally estimated using least squares (\\(\\hat{\\boldsymbol{\\beta}}_{LSE}\\)) or maximum likelihood estimation (\\(\\hat{\\boldsymbol{\\beta}}_{MLE}\\)). Under the assumption of a normal distribution, \\(\\hat{\\boldsymbol{\\beta}}_{LSE}=\\hat{\\boldsymbol{\\beta}}_{MLE} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\). Least squares estimation (LSE) does not make distributional assumptions, it’s basically just optimizing a function of the squared differences between the estimations and the observations. \\[(\\mathbf{y}-\\mathbf{X}\\hat{\\boldsymbol{\\beta}})^2 = (\\mathbf{y}-\\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\hat{\\beta}}) = \\mathbf{y}^T\\mathbf{y} - 2\\mathbf{y}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} + \\hat{\\boldsymbol{\\beta}}^T \\mathbf{X}^T \\mathbf{X} \\hat{\\boldsymbol{\\beta}}\\] Set first derivative to zero \\[\\underset{\\boldsymbol{\\hat{\\beta}}}{\\mathrm{argmin}}\\, (\\mathbf{y}-\\mathbf{X}\\hat{\\boldsymbol{\\beta}})^2 = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\] Because LSE does not make distributional assumptions, there’s no variance parameter associated to our model, and we don’t get any uncertainty estimates. Maximum likelihood estimation (MLE) does make distributional assumptions. \\[\\hat{\\boldsymbol{\\beta}} = \\underset{\\boldsymbol{\\hat{\\beta}}} {\\mathrm{argmax}}\\, \\mathcal{L}_n(\\boldsymbol{\\hat{\\beta}};\\mathbf{y})\\] Computationally, we normally target the log-likelihood: \\[\\hat{\\boldsymbol{\\beta}} = \\underset{\\boldsymbol{\\hat{\\beta}}} {\\mathrm{argmin}}\\, - \\ln \\mathcal{L}_n(\\boldsymbol{\\hat{\\beta}};\\mathbf{y})\\] Fixed effects are often called BLUEs: Best: minimum variance Linear Unbiased Estimator 6.3.3 Estimation of variance components Variance components may be estimated with a range of different methods. Maximum Likelihood Estimation of variance components provides downward biased estimates for small data (most experimental data). \\(\\ell_{ML}(\\boldsymbol{\\sigma; \\boldsymbol{\\beta}, \\mathbf{y}}) = - (\\frac{n}{2}) \\log(2\\pi)-(\\frac{1}{2}) \\log ( \\vert \\mathbf{V}(\\boldsymbol\\sigma) \\vert ) - (\\frac{1}{2}) (\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta})^T[\\mathbf{V}(\\boldsymbol\\sigma)]^{-1}(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta})\\) Restricted Maximum Likelihood (REML) avoids MLE’s downward bias under small data and is thus is the default in most mixed effects models. In REML, the likelihood is maximized after accounting for the model’s fixed effects. In REML, \\(\\ell_{REML}(\\boldsymbol{\\sigma};\\mathbf{y}) = - (\\frac{n-p}{2}) \\log (2\\pi) - (\\frac{1}{2}) \\log ( \\vert \\mathbf{V}(\\boldsymbol\\sigma) \\vert ) - (\\frac{1}{2})log \\left( \\vert \\mathbf{X}^T[\\mathbf{V}(\\boldsymbol\\sigma)]^{-1}\\mathbf{X} \\vert \\right) - (\\frac{1}{2})\\mathbf{r}[\\mathbf{V}(\\boldsymbol\\sigma)]^{-1}\\mathbf{r}\\), where \\(p = rank(\\mathbf{X})\\), \\(\\mathbf{r} = \\mathbf{y}-\\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{ML}\\). Start with initial values for \\(\\boldsymbol{\\sigma}\\), \\(\\tilde{\\boldsymbol{\\sigma}}\\). Compute \\(\\mathbf{G}(\\tilde{\\boldsymbol{\\sigma}})\\) and \\(\\mathbf{R}(\\tilde{\\boldsymbol{\\sigma}})\\). Obtain \\(\\boldsymbol{\\beta}\\) and \\(\\mathbf{b}\\). Update \\(\\tilde{\\boldsymbol{\\sigma}}\\). Repeat until convergence. Variance components may be \\(&lt;0\\); in that case they are set to zero. Method of moments (MoM) is derived from ANOVA tables that include the expected mean squares. Use actual sums of squares and expected mean squares (EMS) from ANOVA to clear for the different variance components. e.g., \\(\\tilde{\\sigma}^2_{w} = \\frac{SSw - SS_e}{c}\\) (See table above). However, if \\(\\tilde{\\sigma}^2_{w} &lt; 0\\), \\(\\hat{\\sigma}^2_{w} = 0\\). Issues with variance estimates equal to zero Type I error inflation Frey et al. (2024) 6.3.4 Estimating random effects Random effects arise from a multivariate normal distribution \\[\\mathbf{u} \\sim MVN (\\boldsymbol{0}, \\mathbf{G}),\\] where \\(\\mathbf{G} = \\begin{bmatrix} \\sigma^2_b \\mathbf{I} &amp; 0 \\\\ 0&amp; \\sigma^2_w \\mathbf{I} \\end{bmatrix}\\). Random effects are shrunk. Shrinkage depends on magnitude of variance estimate. Random effects are often called BLUPs: Best: minimum variance, Linear Unbiased Predictor 6.3.5 Degrees of freedom in mixed models Denominator degrees of freedom may not be exactly derived from ANOVA tables. Increase the complexity of covariance structure, and the df will be more different to the ones on the ANOVA table. Degrees of freedom are approximated under mixed models for bias correction. Satterthwaite approximation Kenward-Roger approximation 6.4 R demo Download R code "],["generalized-linear-mixed-models.html", "Day 7 Generalized Linear Mixed Models 7.1 Announcements 7.2 Review about the (general) linear mixed model 7.3 Generalized linear mixed models 7.4 Reading", " Day 7 Generalized Linear Mixed Models September 17th 7.1 Announcements Assignment 2 grades are posted Assignment 3 due on Sunday This blog post on “bad science” 7.2 Review about the (general) linear mixed model One of the most common notations is the model equation form: \\[y_{ij} = \\mu + T_i + \\varepsilon_{ij} , \\\\ \\varepsilon_{ij}\\sim N(0, \\sigma^2),\\] which is very much restrictive. 7.3 Generalized linear mixed models Now, if we relax the assumption of the normal distributions, there are several other probability distributions that could describe how the data are generated. GLMMs are linear models for variables from any distribution from the exponential family. Exponential family: pdf can be written as \\[f(y\\vert \\theta) = \\exp \\begin{bmatrix} \\frac{y\\theta- b(\\theta)}{a(\\theta)} + c(y, \\phi) \\end{bmatrix} ,\\] where \\(y\\) is the random variable (e.g., the response), \\(\\theta\\) is a canonical parameter (related to the mean) and \\(\\phi\\) is a scale parameter (related to the dispersion). Figure 7.1: Common variable distributions. Page 60 in Stroup et al. (2024) Probability distributions 7.3.1 Three steps to modeling data Define the linear predictor \\(\\boldsymbol\\eta = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{u}\\), Define the probability distribution for the data, \\(\\mathbf{y}|\\mathbf{u}\\), Define the link function \\(g(\\cdot)\\), \\(\\boldsymbol\\eta = g(\\mathbf{y}|\\mathbf{u})\\). 7.3.2 Implications for model fitting Least Squares Estimator is no longer Maximum Likelihood Estimator Variance is no longer \\(\\hat\\sigma^2 = \\frac{SSE}{df_e}\\) The whole concept of degrees of freedom is more diffuse ANOVA shells are still useful to analyze designs are number of independent, true, replicates The \\(Var(\\mathbf{y}|\\mathbf{u})\\) may be specified, but not specify the full likelihood (check out the properties above) Quasi-likelihood for modeling overdispersion or repeated measures in GLMMs: \\(E(\\mathbf{y} \\vert \\mathbf{b}) = \\boldsymbol{\\mu}\\vert \\mathbf{b}\\) \\(Var(\\mathbf{y} \\vert \\mathbf{b}) = \\mathbf{V}_{\\mu}^{1/2}\\mathbf{A}\\mathbf{V}_{\\mu}^{1/2}\\) 7.4 Reading Stats majors: Chapter 5 in Stroup et al. (2024) Non-stats majors: LMMs and GLMMs chapter by Ben Bolker The value of generalized linear mixed models for data analysis in the plant sciences Generalized Linear Mixed Models in Dairy Cattle Breeding "],["inference-estimability-degrees-of-freedom-hypothesis-tests-contrasts.html", "Day 8 Inference – Estimability, degrees of freedom, hypothesis tests, contrasts 8.1 Announcements 8.2 Review: Hypothesis tests, t-tests, F tests 8.3 Applied example 8.4 Estimability 8.5 Applied example II 8.6 Wednesday", " Day 8 Inference – Estimability, degrees of freedom, hypothesis tests, contrasts 8.1 Announcements Highlight questions from Assignment 3: Why are degrees of freedom not always integer numbers? What is the difference between sums of squares? What is the best size of blocks when designing an experiment? Figure 8.1: Overview of today’s class. 8.2 Review: Hypothesis tests, t-tests, F tests 8.2.1 Hypothesis tests Classic statistics have been deeply associated to falsifiability in the advancement of Science. Hugely influenced by Karl Popper, the advancement of Science can be understood as a string of hypotheses that are constantly evaluated. In statistics, hypothesis tests evaluate whether a given statistic (i.e., a function of the data) is too extreme for the null hypothesis to be true. Normally, the null hypothesis is business as usual, and we want to evaluate if we just made a scientific discovery, or if there’s not enough evidence to say that there’s something different going on. Figure 8.2: Types of errors in hypothesis tests. 8.2.2 On p-values The p value is the probability of observing the [t/F] statistic under the null hypothesis. Often, we assume \\(\\alpha = 0.05\\), and reject \\(H_0\\) if \\(p&lt;\\alpha\\). ASA’s statement on p-values Scientists rise up against statistical significance Discussion Counterargument: In defense of p-values Gelman “Let us have the serenity to embrace the variation that we cannot reduce, the courage to reduce the variation we cannot embrace, and the wisdom to distinguish one from the other.” [see talk] 8.2.3 t-tests Usually used to test whether one value/parameter/linear combination of parameters is different to a certain number. Steps: Define the target quantity \\(\\theta\\). For example, \\(\\theta = \\beta_1-\\beta_2\\). After observing the data, estimate \\(\\hat{\\boldsymbol\\beta}\\) and \\(se(\\hat{\\boldsymbol\\beta})\\). Calculate the test \\(t\\) statistic as \\(t^\\star = \\frac{\\hat\\theta}{se(\\hat\\theta)}\\). Compare \\(t^{\\star}\\) to \\(t_{1-\\frac{\\alpha}{2}, dfe}\\). Why \\(t\\) distribution? \\(N\\) for known se, \\(t\\) for \\(\\hat{se}\\). Note that \\(t^\\star\\) depends on: sample size (\\(dfe\\)) experiment design (other df) the variability \\(\\sigma^2\\) associated to that system. Note that \\(t_{crit}\\) depends on \\(dfe\\). 8.2.4 F-tests are multivariate t-tests Test the relevance of multiple factors at once. Consistent with the “bins” concept of ANOVA. Usually used to test whether a group of factor is relevant to explain variability in the data. Why \\(F\\) distribution? Ratio between two independent \\(\\chi^2\\) distributions divided by their df. Steps: Calculate Mean Squared Errors for the different sources of variability \\(= \\frac{SSH}{df}\\). Calculate the F statistic as the ratio between mean squares, \\(=\\frac{MSH}{MSE}\\). Compute \\(P(F&gt;F^\\star)\\), using the degrees of freedom of both mean squares, df of the numerator and df of the denominator. Note, again, the relationship between the p value and sample size and \\(\\sigma^2\\). Note, again, that the \\(P(F&gt;F^\\star)\\) depends on the df of the denominator. 8.3 Applied example From Milliken and Johnson (2009). A baker wanted to determine the effect that the amount of fat in a recipe of cookie dough would have on the texture of the cookie. She also wanted to determine if the temperature (°F) at which the cookies were baked would have an influence on the texture of the surface. The texture of the cookie is measured by determining the amount of force (g) required to penetrate the cookie surface. Experimentation: The process she used was to make a batch of cookie dough for each of the four recipes every day, and baked them one by one in the oven in different batches. She carried this process out each of four days when she baked cookies at three different temperatures. library(tidyverse) library(lme4) library(emmeans) df &lt;- read.csv(&quot;../data/cookies.csv&quot;) df$Temperature &lt;- as.factor(df$Temperature) df$fat_perc &lt;- as.factor(df$fat_perc) df$Day &lt;- as.factor(df$Day) df %&gt;% ggplot(aes(Temperature, force))+ theme_classic()+ scale_fill_manual(values = c(&quot;#DCD6F7&quot;, &quot;#B4869F&quot;, &quot;#985F6F&quot;, &quot;#4E4C67&quot;))+ labs(x = &quot;Temperature (°F)&quot;, fill = &quot;Fat (%)&quot;, y = &quot;Force (g)&quot;)+ theme(aspect.ratio = .5)+ geom_boxplot(aes(group = paste(Temperature, fat_perc), fill = factor(fat_perc))) ANOVA table for cookie RCBD ANOVA table for cookie RCBD table { width: 100%; border-collapse: collapse; margin: 20px 0; } th, td { border: 1px solid #ddd; padding: 8px; text-align: left; } th { background-color: #f4f4f4; font-weight: bold; } tr:nth-child(even) { background-color: #f9f9f9; } Source df Block \\(r-1 = 3\\) Temp \\(t-1 = 2\\) Fat \\(f-1 = 3\\) \\(T \\times F\\) \\((t-1)(f-1) = 6\\) Error \\(N-tf-(r-1) = 33\\) The corresponding model is \\[y_{ijk} = \\mu + T_i + F_j + TF_{ij} + d_k + \\varepsilon_{ijk} , \\\\ d_k \\sim N(0, \\sigma^2_d), \\\\ \\varepsilon_{ijk} \\sim N(0, \\sigma^2_\\varepsilon), \\] and can be fitted to the data with the following R code: m &lt;- lmer(force ~ 1 + Temperature*fat_perc + (1|Day), data = df) The degrees of freedom for the denominator are straightforward. 8.3.1 t-test Objective: compare treatment means between 375 degrees temperature and 4% fat and 400 degrees temperature and 4% fat. Remember \\(t^\\star = \\frac{\\hat\\theta}{se(\\hat\\theta)}\\) \\(\\hat\\theta = \\mu_{22} - \\mu_{32}\\) \\(se(\\hat\\theta) = \\sqrt{\\frac{2 \\sigma^2_\\varepsilon}{r}}\\) R code to get t statistics by hand d &lt;- data.frame(Temperature = as.factor(c(375, 400)), fat_perc = factor(4)) hat.mu &lt;- predict(m, newdata = d, re.form = NA) hat.theta &lt;- hat.mu[2]-hat.mu[1] hat.sigma2 &lt;- sigma(m)^2 hat.sigma2_b &lt;- as.data.frame(VarCorr(m))[1,4] reps &lt;- n_distinct(df$Day) se.hat.theta &lt;- sqrt((2* hat.sigma2)/reps) emmeans(m, ~Temperature:fat_perc, contr = list(c(0, 0, 0, 0, -1, 1, rep(0, 6))))$contrasts ## contrast estimate SE df t.ratio p.value ## c(0, 0, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0) 2.92 0.633 33 4.621 0.0001 ## ## Degrees-of-freedom method: kenward-roger hat.theta ## 2 ## 2.925 se.hat.theta ## [1] 0.6330342 t.star &lt;- hat.theta/se.hat.theta t.star ## 2 ## 4.620603 dt(t.star, df = 33) ## 2 ## 8.202921e-05 8.3.2 F-test Objective: find out if temperature, in general, affects the texture of the cookie. \\(H_0:\\) all temperatures have the same effect on texture. \\(H_a:\\) at least one temperature has a different effect on texture. \\(F^\\star = \\frac{MS_{Temp}}{MS_{resid}}\\) \\(F_{critical} = F_{1-\\alpha, df_{numerator}, df_{denominator}}\\) R code to get t statistics by hand m.full &lt;- lmer(force ~ 1 + Temperature*fat_perc + (1|Day), data = df) m.TF &lt;- lmer(force ~ 1 + Temperature + fat_perc + (1|Day), data = df) m.T &lt;- lmer(force ~ 1 + Temperature + (1|Day), data = df) m.F &lt;- lmer(force ~ 1 + fat_perc + (1|Day), data = df) m.n &lt;- lmer(force ~ 1 + (1|Day), data = df) # SS II residual sum(resid(m.full)^2) ## [1] 26.84223 # SS II TxF sum((predict(m.full, re.form = NA) - predict(m.TF, re.form = NA))^2) ## [1] 1.505 # SS II F sum((predict(m.TF, re.form = NA) - predict(m.T, re.form = NA))^2) ## [1] 3.665 # SS II T sum((predict(m.TF, re.form = NA) - predict(m.F, re.form = NA))^2) ## [1] 179.5317 # P(F&gt;Fcrit) for temperature F.T &lt;- (179.5317/2)/(26.44833/33) 1 - pf(F.T, df1 = 2, df2 = 33) ## [1] 1.998401e-15 # P(F&gt;Fcrit) for fat% F.F &lt;- (3.665/3)/(26.44833/33) 1 - pf(F.F, df1 = 3, df2 = 33) ## [1] 0.2264496 # P(F&gt;Fcrit) for TxF F.TF &lt;- (1.505/6)/(26.44833/33) 1 - pf(F.TF, df1 = 6, df2 = 33) ## [1] 0.9256322 8.4 Estimability A quantity is estimable if it is possible to find a linear combination of the data that provides an unbiased and unique estimate of that quantity. For matrix models, linear estimable functions of \\(\\boldsymbol\\beta\\) take on the form of linear combinations of the parameter vector such as \\(a&#39;\\boldsymbol\\beta\\) where a is a \\(p\\times1\\) vector of constants. A linear function \\(a&#39;\\boldsymbol\\beta\\) is estimable if and only if there exists a vector \\(r\\) such that \\(a = \\mathbf{X}&#39;\\mathbf{X}r\\). Each function \\(x_i&#39;\\boldsymbol\\beta\\) is estimable where \\(x_i\\) is the \\(i\\)th row of \\(\\mathbf{X}\\). 8.4.1 Contrasts Linear combinations of parameters. Often connected to hypothesis tests where \\(H_0: \\mathbf{K}\\boldsymbol\\beta = 0\\). 8.4.2 Adjusting degrees of freedom for mixed models From Assignment 3: In class we learned that degrees of freedom represent the number of independent pieces of information. In the ANOVA tables these appear as whole numbers, but when we run mixed models in R I sometimes see fractional values (for example, 27.4). Why do mixed models produce non-integer degrees of freedom, and could you also provide a simpler or more intuitive way to think about what degrees of freedom really mean? Answer: According to my understanding, in ANOVA, degrees of freedom come directly from how many observations are available and how many parameters are estimated, which gives whole numbers. Mixed models include both random and fixed effects and the structure of data involves often correlations among the observations for example: repeated measures on the same subject or split-plot designs). In this case, the information available to estimate parameters is not clearly separated as ANOVA. So, degrees of freedom in mixed models represent an effective amount of independent information, this might be the reason they can be fractional. Contrasts for balanced &amp; complete designs have straightforward calculation of degrees of freedom. For unbalanced designs, incomplete blocks, missing data, and cases with more complex variance-covariance structure, the ANOVA shell does not give us exact degrees of freedom. Downward bias of the variance of an estimable function, leading to: too narrow confidence intervals inflated type I error rates Bias adjustment methods estimate the approximate degrees of freedom, which might be a non-integer number. Satterthwaite approximation Paper Let \\(\\frac{X^2_{num}/\\nu_1}{X_2^\\star/\\nu^\\star_2}\\) be the ratio of interest, where \\(X_{num}^2 \\sim \\chi^2_{\\nu_1}\\) and \\(X_2^\\star\\) is a linear combination of chi-square random variables all independent of \\(X_{num}^2\\), then \\(X_2^\\star \\sim\\) approximately \\(\\chi^2_{\\nu^\\star_2}\\), where \\[\\nu_2^\\star \\equiv \\frac{(\\sum_m c_m X_m^2)^2}{\\sum_m\\frac{(c_m X_m^2)}{df_m}}.\\] Kenward-Roger approximation Paper Based on REML Again, expept for variance-component-only LMMs with balanced data and their compound symmetry marginal model equivalents, the estimated covariance of \\(\\beta \\mathbf{K}\\) will tend to be underestimated. KR somewhat more conservative and reliable than Satterthwaite (source). Integrated in most statistical software. R implementation emmeans uses Kenward-Roger by default. [see vignette] Everything is more tricky in GLMMs (df is Inf in most applications). 8.5 Applied example II From Milliken and Johnson (2009). A baker wanted to determine the effect that the amount of fat in a recipe of cookie dough would have on the texture of the cookie. She also wanted to determine if the temperature (°F) at which the cookies were baked would have an influence on the texture of the surface. The texture of the cookie is measured by determining the amount of force (g) required to penetrate the cookie surface. Experimentation: The process she used was to make a batch of cookie dough for each of the four recipes every day, and baked one cookie from each recipe in the oven together. She carried this process out each of four days when she baked cookies at three different temperatures. ANOVA table for the cookie split-plot experiment. table { width: 100%; border-collapse: collapse; margin: 20px 0; } th, td { border: 1px solid #ddd; padding: 8px; text-align: left; } th { background-color: #f4f4f4; font-weight: bold; } tr:nth-child(even) { background-color: #f9f9f9; } Source df Block \\(r-1 = 3\\) Temperature \\(t-1\\) = 2 Error(oven) \\((r-1)(t-1) = 6\\) Fat (%) \\(f-1 = 3\\) \\(T \\times F\\) \\((t-1)(f-1) = 6\\) Error(split plot) \\(t(r-1)(f-1) = 27\\) The corresponding model is \\[y_{ijk} = \\mu + T_i + F_j + TF_{ij} + d_k + w_{i(k)} + \\varepsilon_{ijk} , \\\\ d_k \\sim N(0, \\sigma^2_d), \\\\ w_{i(k)} \\sim N(0, \\sigma^2_w), \\\\ \\varepsilon_{ijk} \\sim N(0, \\sigma^2_\\varepsilon), \\] and can be fitted to the data with the following R code: m_splitplot &lt;- lmer(force ~ 1 + Temperature*fat_perc + (1|Day/Temperature), data = df) The degrees of freedom for the denominator are not so straightforward anymore. emmeans(m_splitplot, ~ Temperature:fat_perc, contr = list(c(1, -1, rep(0, 10)))) ## $emmeans ## Temperature fat_perc emmean SE df lower.CL upper.CL ## 350 2 8.05 0.534 10 6.86 9.24 ## 375 2 10.00 0.534 10 8.81 11.19 ## 400 2 12.12 0.534 10 10.93 13.32 ## 350 4 7.42 0.534 10 6.23 8.62 ## 375 4 9.43 0.534 10 8.23 10.62 ## 400 4 12.35 0.534 10 11.16 13.54 ## 350 6 7.33 0.534 10 6.13 8.52 ## 375 6 9.18 0.534 10 7.98 10.37 ## 400 6 12.25 0.534 10 11.06 13.44 ## 350 8 7.00 0.534 10 5.81 8.19 ## 375 8 8.95 0.534 10 7.76 10.14 ## 400 8 11.93 0.534 10 10.73 13.12 ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## c(1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0) -1.95 0.731 6.79 -2.667 0.0331 ## ## Degrees-of-freedom method: kenward-roger 8.6 Wednesday Come early and bring your computer. Read Ch 6 in Stroup et al (2024). "],["semester-project-1.html", "Day 9 Semester Project 9.1 Learning objectives 9.2 Partial deadlines", " Day 9 Semester Project Semester projects may deal with any topic that interests you [the student], as long as it is approved by the instructor. Broadly, projects are expected to (i) identify a research problem, (ii) develop an appropriate workflow that is appropriate for solving that problem, and (iii) create a report and reproducible tutorial. 9.1 Learning objectives Be able to identify an experiment design that is appropriate for answering a given research question and discuss the strengths and weaknesses of that design for answering the question. Be able to write the materials and methods section of a paper/thesis, including the statistical model that corresponds the experiment design. 9.2 Partial deadlines 9.2.1 Project proposal - Due Friday September 24 at 11:59pm CT Write a page-long project proposal that states your research problem and the objective of your project. An example of an appropriate project proposal can be found here. 9.2.2 Written report for peer review - Due Thursday November 27 at 11:59pm CT for peer review 9.2.3 Oral presentation - Somewhere between December 3 - December 12 Prepare a 15 minute presentation of the core aspects of your project. Presentations should include at least: Motivation, Methods, including a clear and complete description of the statistical model and code, Discussion of strengths and weaknesses. 9.2.4 Written report - Due Friday December 19 at 11:59pm CT for peer review Submit a manuscript including: Introduction (background &amp; justification of the problem) Methods, including: A complete ANOVA table describing the degrees of freedom associated to mean comparisons, A clear and complete description of the statistical model, R code to implement that model, Results Discussion including strengths and weaknesses of the design/analysis, Conclusions. "]]
