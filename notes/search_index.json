[["index.html", "STAT 870 - Analysis of Messy Data Day 1 Welcome to STAT 870! 1.1 About this course: 1.2 Learning goals 1.3 Overview of the course 1.4 What is messy data anyways? 1.5 Linear Models review 1.6 On notation 1.7 Homework &amp; Announcements", " STAT 870 - Analysis of Messy Data Josefina Lacasa Fall 2025 Day 1 Welcome to STAT 870! August 25th, 2025 1.1 About this course: About me About you: Figure 1.1: Students of this class In rounds: What’s your major and what do you expect to learn in 870? 1.1.1 Logistics Website Syllabus Statistical programming requirements Rough mindmap of the course (on whiteboard) Semester project Grades: A (100-89.999999999(!!!)), B (89.99-79.99), C (79.99-69.99), D (69.99-59.99), F (&lt;59.99). Attendance policies &amp; kahoots 1.2 Learning goals By the end of this course, you should be able to: Identify the treatment design, experiment design, experimental unit and observational unit of simple and complex designed experiments. Distinguish the benefits/disadvantages of different experiment designs. Write the statistical model that corresponds to data generated by designed experiments. Write the Materials and Methods section in a paper (or thesis) that describes the designed experiment. 1.3 Overview of the course Figure 1.2: Mindmap 1.4 What is messy data anyways? Complex dependence structures Complex structure in covariates 1.5 Linear Models review Perhaps the most common model of all time (default in most software) is \\[y_{i} = \\mu_i + \\varepsilon_i, \\ \\varepsilon_i \\sim N(0, \\sigma^2),\\] where: \\(y_{i}\\) is the observed value for the \\(i\\)th observation, \\(\\mu_i\\) is the expected value for the \\(i\\)th observation, \\(\\varepsilon_i\\) is the residual (i.e., the difference between observed and expected). All residuals are iid normal. This example uses the model equation form. The model above can also be written usign the probability distribution form. The probability distribution form is much more flexible because it is compatible with other distributions beyond the normal. It goes like this: \\[y_{i} \\sim N(\\mu_i, \\sigma^2),\\] where the elements are the same as described above. Likewise, we can use the vectorized notation of the probability distribution form, and say that \\[\\mathbf{y} \\sim N(\\boldsymbol\\mu, \\sigma^2 \\mathbf{I}),\\] where: \\(\\mathbf{y} \\equiv [y_1, y_2, ..., y_n]&#39;\\) is the vector of observed values, \\(\\boldsymbol\\mu \\equiv [\\mu_1, \\mu_2, ..., \\mu_n]&#39;\\) is the vector of expected values, \\(\\sigma^2 \\mathbf{I}\\) is the variance-covariance matrix. Note that, more generally, we can call the variance-covariance matrix \\(\\boldsymbol{\\Sigma}\\) or \\(\\mathbf{V}\\). Note that \\[\\sigma^2 \\mathbf{I} = \\sigma^2 \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\dots &amp; 1 \\end{bmatrix} = \\begin{bmatrix} \\sigma^2 &amp; 0 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; \\sigma^2 &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; 0 &amp; \\sigma^2 &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\dots &amp; \\sigma^2 \\end{bmatrix}.\\] The assumptions behind this model are: Normal distribution of the data Constant variance Independence Linearity In this course, we will mostly deal with cases where: The assumption of independence does not hold (basic and complex designed experiments), The assumption of normality does not make sense (e.g., data are counts, proportions, or stricly positive), The assumption of constant variance does not hold/make sense (e.g., larger biomass is associated to larger variance of said biomass). We will approach most problems with a 3-step approach (Chapter 2 in Stroup et al.): What is the distribution of the data? What is the link function? What is the blueprint of the design/data? Figure 1.3: Common variable distributions. Page 60 in Stroup et al. (2024) 1.6 On notation scalars: \\(y\\), \\(\\sigma\\), \\(\\beta_0\\) vectors: \\(\\mathbf{y} \\equiv [y_1, y_2, ..., y_n]&#39;\\), \\(\\boldsymbol{\\beta} \\equiv [\\beta_1, \\beta_2, ..., \\beta_p]&#39;\\), \\(\\boldsymbol{u}\\) matrices: \\(\\mathbf{X}\\), \\(\\Sigma\\) probability distribution: \\(y \\sim N(0, \\sigma^2)\\), \\(\\mathbf{y} \\sim N(\\boldsymbol{0}, \\sigma^2\\mathbf{I})\\). 1.7 Homework &amp; Announcements Install R and RStudio. Submit Assignment 1 by next Monday. Read Chapter 2 in Stroup et al. (2024) Applied Linear Mixed Models Workshop - Sept 13 &amp; 14 "],["designed-experiments-review.html", "Day 2 Designed Experiments Review 2.1 Announcements 2.2 Designed experiments 2.3 Homework &amp; Announcements", " Day 2 Designed Experiments Review August 27th 2.1 Announcements Assignment 1 is due next Wednesday. Applied Linear Mixed Models Workshop 09/13-09/14. Workshop is full. Email me if you still want to join. 2.2 Designed experiments The golden rules of designed experiments: Replication Randomization Local control Figure 2.1: Principles for conducting valid and efficient experiments. From Box, Hunter and Hunter (2005). About George Box I - A Conversation with George Box [link] About George Box II - An Accidental Statistician: The Life and Memories of George E. P. Box [link] 2.2.1 Treatment structure Solely about the treatment. Most likely connected to the research question and thus defined by the subject matter expert. Figure 2.2: Schematic representation of a one-way treatment structure. A one-way treatment structure means that the expected value of the observations, \\(y\\), is affected by one treatment factor with \\(k\\) levels: \\[g(\\mu_{ij}) = \\eta_{ij} = \\eta_{0} + T_i,\\] where: \\(\\mu_{ij}\\) is the expected value of \\(y_{ij}\\), the observation of the \\(i\\)th treatment and \\(j\\)th repetition, \\(g(\\cdot)\\) is the link function, that ensures that the mean is within the support of the chosen distribution, \\(\\eta_{ij}\\) is the linear predictor of \\(g(\\mu_{ij})\\), \\(\\eta_{0}\\) is the overall mean, and \\(T_i\\) is the effect of the \\(i\\)th level of the treatment \\(T\\). Figure 2.3: Schematic representation of a two-way factorial treatment structure. A two-way treatment structure is similar to a one-way treatment structure, only now the expected value is affected by two treatment factors: \\[g(\\mu_{ijk}) = \\eta_{ijk} = \\eta_{0} + T_i + G_j + (TG)_{ij} ,\\] where: \\(\\mu_{ijk}\\) is the expected value of \\(y_{ij}\\), the observation of the \\(i\\)th treatment \\(T\\), \\(j\\)th treatment \\(G\\), and \\(k\\) repetition, \\(g(\\cdot)\\) is the link function, that ensures that the mean is within the support of the chosen distribution, \\(\\eta_{ijk}\\) is the linear predictor of \\(g(\\mu_{ijk})\\), \\(\\eta_{0}\\) is the overall mean, \\(T_i\\) is the effect of the \\(i\\)th level of the treatment \\(T\\), \\(G_j\\) is the effect of the \\(j\\)th level of the treatment \\(G\\), and \\((TG)_{ij}\\) is the interaction between the \\(i\\)th level of treatment \\(T\\), and \\(j\\)th level of treatment \\(G\\). Figure 2.4: Schematic representation of a tree-way factorial treatment structure. 2.2.2 Design structure The experiment design describes the data generating process (“blueprint” of the data). You’ll notice that the models above only consider the treatment sources of variability to explain variations in the observed values. How were the treatments above (logistically) applied? Useful questions: What is the experimental unit? What is the blueprint of the design? (which observations are similar to what) 2.3 Homework &amp; Announcements Submit Assignment 1 by next Wednesday. Applied Linear Mixed Models Workshop is full. Email me if you still want to join. "],["designed-experiments-review-1.html", "Day 3 Designed Experiments Review 3.1 Announcements 3.2 Semester project 3.3 Designed experiments 3.4 Applied example: fungicide effects on barley genotypes 3.5 Homework &amp; Announcements", " Day 3 Designed Experiments Review September 3rd 3.1 Announcements Assignment 1 is due today! Assignment 2 is posted and due next week. Applied Linear Mixed Models Workshop 09/13-09/14. Workshop is full. Email me if you still want to join. Read Chapter 2 of Stroup’s GLMM book (this chapter only, 1ed&gt;2ed). 3.2 Semester project Proposal due September 24th – see example 1 | see example 2 Must contain: Background, Objectives. Data for the project must be ready to go. Final product: Manuscript + Tutorial. Working in teams vs. solo. Start as soon as possible! 3.3 Designed experiments Let’s review the basics of designed experiments: Research question Planning the experiment Treatment structure - directly dependent on the research question Design structure - mostly depending on logistics. May depend on the research question. 3.3.1 Design structures While the treatment structure defines what effects we will try and estimate as precisely as possible, the design structure tells us the story of how those treatments were applied. Some of the most common designed experiments include completely randomized design (CRD), randomized complete block design (RCBD), split-plot design, relevant variations to the designs above: generalized randomized complete block design, strip-plot design, repeated measures designs. Figure 3.1: Schematic description of an experiment with a completely randomized design Figure 3.2: Schematic description of an experiment with a randomized complete block design Figure 3.3: Schematic description of a field experiment with a split-plot design 3.4 Applied example: fungicide effects on barley genotypes Let’s consider an experiment that aimed to study the effect of fungicide and barley genotypes on grain yield. The study includes two levels of fungicide, and 70 different genotypes, considering all their possible combinations. What is the treatment structure of the experiment? Do we know the design structure of the experiment? Now, the actual experiment was run like this: library(tidyverse) library(agridat) library(lme4) library(emmeans) df_fungicide &lt;- agridat::durban.splitplot df_fungicide %&gt;% ggplot(aes(bed, row))+ coord_fixed()+ geom_tile(aes(fill = fung))+ geom_tile(aes(), color = &quot;black&quot;, fill = NA) What is the treatment structure? What are the experimental units? How many independent observations per treatment factor? What is the design structure? Download R code 3.5 Homework &amp; Announcements Assignment 1 due today. Applied Linear Mixed Models Workshop is full. Email me if you still want to join. "],["linear-mixed-models.html", "Day 4 Linear mixed models 4.1 Announcements 4.2 Statistical models 4.3 The general linear model 4.4 Linear mixed models applied to designed experiments 4.5 Homework", " Day 4 Linear mixed models September 8th 4.1 Announcements Assignment 1 grades are posted. Remember that you have one chance to resubmit for full points. Assignment 2 is due on Wednesday. Last chance to join the mixed models workshop this weekend. Semester project: Proposal due September 24th – see example 1 | see example 2 Must contain: Background, Objectives. Data for the project must be ready to go. Final product: Manuscript + Tutorial. Working in teams vs. solo. Start as soon as possible! This week Today: intro to mixed models + blocks modeling discussion Wednesday: mixed models applied to the fungicide experiment + Kahoot 4.2 Statistical models Statistical models can be typically defined as the combination of a deterministic component (e.g., the linear predictor), and a random component (e.g., the distribution of the data). Learn more about this way of describing statistical models in Stroup et al. (2024) page 4 (and all of Chapter 1 in general). 4.3 The general linear model The general linear model is one of the most commonly used statistical models. We call it “general” because it assumes that the data arise from a normal distribution. [Note: Do not mix up “general” (normal distribution) with “generalized” (multiple possible distributions).] We call it “linear” because the parameters enter the model linearly. For example, the predictor \\(\\beta_0 +\\beta_1 x + \\beta_2 x^2\\) corresponds to a linear model, because the parameters enter linearly. The predictor \\(\\alpha \\exp(\\beta x)\\) corresponds to a nonlinear model, because the parameters enter as the power of the data. The general linear model can be written as \\[\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol\\varepsilon, \\\\ \\boldsymbol{\\varepsilon}\\sim N(\\boldsymbol{0}, \\mathbf{\\Sigma}),\\] where \\(\\mathbf{y}\\) is the vector of the data, \\(\\mathbf{X}\\) is the matrix containing the data, \\(\\boldsymbol{\\beta}\\) is a vector containing all parameters, \\(\\boldsymbol\\varepsilon\\) is a vector containing the residuals (i.e., the difference between observed and predicted). Note that \\(\\boldsymbol\\varepsilon\\) arises from a multivariate normal distribution, with mean zero and a variance-covariance matrix \\(\\mathbf{\\Sigma} = \\sigma^2 \\mathbf{I}\\). Recall some cool properties: The least squares estimate of \\(\\beta\\), \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\), is also the the MLE, is unbiased. \\(\\hat\\sigma^2 = \\frac{SSE}{df_e}\\), not the MLE, is the unbiased estimate of \\(\\sigma^2\\). \\(CI(\\hat\\mu_{ij}) = \\hat\\mu_{ij} \\pm t_{df, \\frac{\\alpha}{2}} \\cdot se(\\hat{\\mu}_{ij})\\), where \\(\\sigma^2_e\\) is the residual variance, and \\(r\\) is the number of repetitions. Under a CRD, \\(se(\\mu_{ij}) = \\sqrt{\\frac{\\sigma^2_e}{r}}\\), where \\(\\sigma^2_e\\) is the residual variance, and \\(r\\) is the number of repetitions. ANOVA tables help us distinguish the most important sources of variability in the data. See Gelman (2005) Essentially, an analysis of variance (ANOVA) divides the predictors into bins and analyzes whether each bin is relevant to explain variability in the data. Typically, ANOVA tables show the sources of variation, their degrees of freedom, an F statistic and, sometimes, a p-value. Sources of variation may come from treatment factors, or elements of the design. Degrees of freedom tells us how many independent variables there are. The F statistic serves as a multivariate t statistic, since it is used to test multiple predictors. 4.3.1 Applied example Swine nutritionists are studying the effect of dietary treatments on the % of fecal dry matter, and whether said effects change at different points in time. In this experiment, 450 pigs were allocated in pens of 5 pigs. Each pen is independently assigned with dietary treatment. There are 6 dietary treatments: negative C, positive C, and 4 different pre-commercial additives. So, there are 45 pens per treatment. One (randomly selected) pig per pen was observed at 3 points in time. What is the experimental unit? What is the observational unit? What is the treatment structure? What is the design structure? A reasonable model could be \\[y_{ijk} \\sim N(\\mu_{ijk}, \\sigma^2),\\] \\[\\mu_{ijk} = \\eta_{ijk},\\] \\[\\eta_{ijk} = \\eta_0 + D_i + T_j + (DT)_{ij},\\] where: \\(y_{ijk}\\) is the observed dry matter (%) for the \\(i\\)th dietary treatment, \\(j\\)th time, and \\(k\\)th repetition, \\(\\mu_{ijk}\\) is the expected value for the \\(i\\)th dietary treatment, \\(j\\)th time, and \\(k\\)th repetition, \\(\\sigma^2\\) is the residual variance, \\(\\eta_{ijk}\\), the linear predictor is equivalent to the expected value because the link function is the identity function, \\(\\eta_0\\) is the overall mean of the linear predictor, \\(F_i\\) is the effect of the \\(i\\)th dietary treatment, \\(G_j\\) is the effect of the \\(j\\)th time, \\((FG)_{ij}\\) is the interaction for the \\(i\\)th dietary treatment and \\(j\\)th time A reasonable ANOVA shell could be: Table 4.1: ANOVA table for a CRD with a two-way factorial treatment structure. Source df Diet d-1 Time t-1 D x T (d-1)(t-1) Error N-(d*t) Total N-1 Table 4.1: ANOVA table for the diet CRD. Source df Diet 5 Time 2 D x T 10 Error 252 Total 269 Let’s implement that model: # load the data pigs_crd &lt;- read.csv(&quot;../../data_confid/fecalm_swine_crd.csv&quot;) pigs_crd$Day &lt;- as.factor(pigs_crd$Day) # fit the model m &lt;- lm(F.Dry.matter ~ Day*Trt, data = pigs_crd) Model checks plot(predict(m), abs(resid(m))) car::qqPlot(m) ## [1] 109 148 # anova car::Anova(m, type = 2, test.statistic = &quot;F&quot;) ## Anova Table (Type II tests) ## ## Response: F.Dry.matter ## Sum Sq Df F value Pr(&gt;F) ## Day 850.7 2 8.6720 0.0002279 *** ## Trt 896.5 5 3.6556 0.0032772 ** ## Day:Trt 673.1 10 1.3723 0.1934086 ## Residuals 12360.9 252 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can even get the standard errors and, together with the degrees of freedom, get the 95% confidence intervals. library(emmeans) # get the s.e. by hand n.reps &lt;- 45 sigma2.hat &lt;- sigma(m)^2 # here you have the s.e. (se.mu &lt;- sqrt(sigma2.hat/n.reps)) ## [1] 1.044041 # get the CI # get the mean mu &lt;- as.data.frame(emmeans(m, ~Trt, at = list(Trt = &quot;A&quot;)))[,2] # get the t statistic for the CI t.star &lt;- qt(p = .975, df = m$df.residual) (lower.CI_TrtA &lt;- mu - t.star*se.mu) ## [1] 16.40749 (upper.CI_TrtA &lt;- mu + t.star*se.mu) ## [1] 20.5198 mg_means &lt;- emmeans(m, ~Trt) head(mg_means) ## Trt emmean SE df lower.CL upper.CL ## A 18.5 1.04 252 16.4 20.5 ## B 23.6 1.04 252 21.5 25.6 ## C 20.1 1.04 252 18.1 22.2 ## D 18.3 1.04 252 16.3 20.4 ## E 19.8 1.04 252 17.8 21.9 ## F 18.6 1.04 252 16.5 20.6 ## ## Results are averaged over the levels of: Day ## Confidence level used: 0.95 4.4 Linear mixed models applied to designed experiments When the independence assumption cannot be taken for granted, especially if we know the dependence patterns in the data, we must account for said dependence pattern. Take the previous example, but in a blocked design instead of a CRD. The blocks are basically rooms that hold 90 pens. This means that the observations are not really independent anymore. After all, it’s hard to find rooms that fit 90 pens! Consider an experiment with the same treatments, same number of EUs, but ran in 3 different rooms with 30 pens each. Each room has 5 pens of each dietary treatment. What is the experimental unit? What is the observational unit? What is the treatment structure? What is the design structure? Let’s re-do the statistical model and ANOVA shell: A reasonable model could be \\[y_{ijkl}|b_k \\sim N(\\mu_{ijk}, \\sigma^2),\\] \\[\\mu_{ijkl} = \\eta_{ijkl},\\] \\[\\eta_{ijkl} = \\eta_0 + D_i + T_j + (DT)_{ij} + b_k,\\] where: \\(y_{ijk}\\) is the observed dry matter (%) for the \\(i\\)th dietary treatment, \\(j\\)th time, and \\(k\\)th room and \\(l\\)th repetition, \\(\\mu_{ijk}\\) is the expected value for the \\(i\\)th dietary treatment, \\(j\\)th time, and \\(k\\)th room and \\(l\\)th repetition, \\(\\sigma^2\\) is the residual variance, \\(\\eta_{ijk}\\), the linear predictor is equivalent to the expected value because the link function is the identity function, \\(\\eta_0\\) is the overall mean of the linear predictor, \\(F_i\\) is the effect of the \\(i\\)th dietary treatment, \\(G_j\\) is the effect of the \\(j\\)th time, \\((FG)_{ij}\\) is the interaction for the \\(i\\)th dietary treatment and \\(j\\)th time, \\(b_k\\) is the effect of the \\(k\\)th room, \\(b_k \\sim N(0, \\sigma^2_b)\\). A reasonable ANOVA shell could be: Table 4.2: ANOVA table for a GRCBD with a two-way factorial treatment structure. Source df Room r-1 Diet d-1 Time t-1 D x T (d-1)(t-1) Error N-(d*t)-r Total N-1 Table 4.2: ANOVA table for the diet GRCBD. Source df Room 2 Diet 5 Time 2 D x T 10 Error 250 Total 269 Let’s implement that model: # load the data pigs_grcbd &lt;- read.csv(&quot;../../data_confid/fecalm_swine_grcbd.csv&quot;) pigs_grcbd$Day &lt;- as.factor(pigs_grcbd$Day) pigs_grcbd$Room &lt;- as.factor(pigs_grcbd$Room) # fit the model library(lme4) m_random &lt;- lmer(F.Dry.matter ~ Day*Trt + (1|Room), data = pigs_grcbd) Model checks plot(predict(m_random), abs(resid(m_random))) # anova car::Anova(m_random, type = 2, test.statistic = &quot;F&quot;) ## Analysis of Deviance Table (Type II Wald F tests with Kenward-Roger df) ## ## Response: F.Dry.matter ## F Df Df.res Pr(&gt;F) ## Day 10.7115 2 250 3.441e-05 *** ## Trt 4.7730 5 250 0.0003483 *** ## Day:Trt 1.1153 10 250 0.3509474 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Notice that the 252 degrees of freedom (aka independent observations) from the CRD are now spread across different levels, connected to different variance components, blocks and random error. 4.4.1 General notation for linear mixed models The standard notation for mixed models is \\[\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{Z}\\boldsymbol{u} + \\boldsymbol\\varepsilon, \\\\ \\boldsymbol{u}\\sim N(\\boldsymbol{0}, \\mathbf{G}), \\\\ \\boldsymbol{\\varepsilon}\\sim N(\\boldsymbol{0}, \\mathbf{R}),\\] where: \\(\\mathbf{y}\\) is the vector containing the data, \\(\\boldsymbol{\\beta}\\) is the vector containing the model fixed effects (usually treatments), \\(\\mathbf{X}\\) is the matrix containing information about the treatment allocation, \\(\\boldsymbol{u}\\) is the vector containing the model random effects (usually elements of the design). Note that \\(\\boldsymbol{u}\\) arises from a normal distribution with mean 0 and variance-covariance matrix \\(\\mathbf{G})\\). \\(\\mathbf{Z}\\) is the matrix containing information about the design, \\(\\boldsymbol\\varepsilon\\) is the model residual. Note that \\(\\boldsymbol{\\varepsilon}\\) arises from a normal distribution with mean 0 and variance-covariance matrix \\(\\mathbf{R})\\). Note that the conditional distribution of \\(\\mathbf{y}\\) is \\[\\mathbf{y} \\vert \\boldsymbol{u} \\sim N(\\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{Z}\\boldsymbol{u},\\mathbf{R}).\\] The marginal distribution of \\(\\mathbf{y}\\) is \\[\\mathbf{y} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\mathbf{Z}\\mathbf{G}\\mathbf{Z}&#39; + \\mathbf{R}).\\] So, essentially what happened before was that the treatment effects haven’t changed, their effect sizes and means haven’t changed, but the variance-covariance matrix did change. A couple things did change: Shrinkage Estimation of \\(\\sigma^2\\) Estimation of \\(se(\\hat\\mu)\\) Degrees of freedom change 4.4.2 Class discussion: Blocks - random or fixed? Treatment point estimates don’t change whether blocks are fixed or random. m_fixed &lt;- lm(F.Dry.matter ~ Day*Trt + Room, data = pigs_grcbd) mg_means_fixed &lt;- emmeans(m_fixed, ~Trt, contr = list(c(1, -1, 0, 0, 0, 0))) ## NOTE: Results may be misleading due to involvement in interactions mg_means_random &lt;- emmeans(m_random, ~Trt, contr = list(c(1, -1, 0, 0, 0, 0))) ## NOTE: Results may be misleading due to involvement in interactions as.data.frame(mg_means_fixed$emmeans) %&gt;% mutate(blocks = &quot;fixed&quot;) %&gt;% bind_rows(as.data.frame(mg_means_random$emmeans) %&gt;% mutate(blocks = &quot;random&quot;)) %&gt;% ggplot(aes(Trt, emmean))+ geom_errorbar(aes(ymin = emmean-SE, ymax = emmean+SE, color = blocks), position = position_dodge(width = .2), width = 0)+ geom_text(aes(x = 4.5, y = 22.75), label = &quot;s.e.(difference)&quot;)+ geom_errorbar(aes(x = 5.4, y = 22, ymin = 22 , ymax = 22 + SE), width = 0.1, data = as.data.frame(mg_means_random$contrasts))+ geom_errorbar(aes(x = 5.2, y = 22, ymin = 22 , ymax = 22 + SE), width = 0.1, data = as.data.frame(mg_means_fixed$contrasts))+ geom_point(aes(color = blocks), position = position_dodge(width = .2))+ theme_classic()+ labs(color = &quot;Blocks modeled as&quot;, y = &quot;Fecal DM (%)&quot;, x = &quot;Treatment&quot;)+ theme(legend.position = &quot;bottom&quot;) More references: Dixon (2016). Gelman (2005) 4.5 Homework Read Chapter 2 in Stroup’s GLMM book (1st ed. superior to 2nd ed.), particularly the “What Would Fisher Do” ANOVA shells. "],["linear-mixed-models-1.html", "Day 5 Linear mixed models 5.1 Announcements 5.2 Linear mixed models 5.3 Next week", " Day 5 Linear mixed models September 10th 5.1 Announcements Assignment 2 is due today! 5.2 Linear mixed models Mixed models come in handy when the (in)famous independence assumption cannot be taken for granted, especially if we know the dependence patterns in the data. We typically know the dependence patterns in the data when running an experiment. For example, the fungicide-barley experiment. Let’s re-do the statistical model and ANOVA shell: A reasonable model could be \\[y_{ijk}|b_k, u_{i(k)} \\sim N(\\mu_{ijk}, \\sigma_e^2),\\] \\[\\mu_{ijk} = \\eta_{ijk},\\] \\[\\eta_{ijk} = \\eta_0 + F_i +G_j + (FG)_{ij}+b_k+ u_{i(k)},\\] where: \\(y_{ijk}\\) is the observed yield for the \\(i\\)th fungicide treatment, \\(j\\)th genotype treatment, and \\(k\\)th repetition, \\(\\mu_{ijk}\\) is the expected value for the \\(i\\)th fungicide treatment, \\(j\\)th genotype treatment, and \\(k\\)th repetition, \\(\\sigma^2\\) is the residual variance, \\(\\eta_{ijk}\\), the linear predictor is equivalent to the expected value because the link function is the identity function, \\(\\eta_0\\) is the overall mean of the linear predictor, \\(F_i\\) is the effect of the \\(i\\)th fungicide treatment, \\(G_j\\) is the effect of the \\(j\\)th genotype treatment, \\((FG)_{ij}\\) is the interaction for the \\(i\\)th fungicide treatment and \\(j\\)th genotype treatment, \\(b_k\\) is the effect of the \\(k\\)th block, \\(b_k \\sim N(0, \\sigma^2_b)\\) \\(u_{i(k)}\\) is the effect of the whole plot corresponding to the \\(i\\)th fungicide treatment in the \\(k\\)th block, \\(u_{i(k)} \\sim N(0, \\sigma^2_u)\\). A reasonable ANOVA shell could be: Table 5.1: This would be the ANOVA under a CRD. Source df - - Fungicide f-1 = 1 - - Genotype g-1 = 69 Fung x Gen (f-1)(g-1) = 69 Error N-fg = 420 Total N-1 Table 5.1: ANOVA table for the fungicide-barley split-plot design. See also Table 24.9 from Milliken and Johnson as a helpful reference. Source df Block b-1 Fungicide f-1 Fungicide(Block) (b-1)(f-1) Genotype g-1 Fung x Gen (f-1)(g-1) Gen(Block x Fung) f(b-1)(g-1) Total N-1 Table 5.1: ANOVA table for the fungicide-barley split-plot design. Source df Block 3 Fungicide 1 Fungicide(Block) 3 Genotype 69 Fung x Gen 69 Gen(Block x Fung) 414 Total 559 Notice that the 420 degrees of freedom (aka independent observations) from the CRD ANOVA are now spread across different levels, connected to different variance components: blocks, whole plots, and random error. This means that there is a different number of independent observations for Fungicide compared to Genotype. The variance components will affect the inference distinctly: Standard errors for comparisons at the whole plot: \\(se(\\mu_{i\\cdot}-\\mu_{i&#39;\\cdot}) = \\sqrt{\\frac{2(\\sigma^2_e + g\\sigma^2_u)}{gb}}\\) Standard errors for comparisons at the split plot: \\(se(\\mu_{\\cdot j}-\\mu_{\\cdot j&#39;}) = \\sqrt{\\frac{2 \\sigma^2_e }{fb}}\\) Let’s apply that model: library(lme4) # load the data url &lt;- &quot;https://raw.githubusercontent.com/stat870/fall2025/refs/heads/main/data/fung_barley_sp.csv&quot; fung_sp &lt;- read.csv(url) fung_sp %&gt;% ggplot(aes(bed, row))+ coord_fixed()+ theme_minimal()+ geom_tile(aes(fill = fung))+ geom_tile(aes(), color = &quot;black&quot;, fill = NA) # fit the model m &lt;- lmer(yield ~ fung*gen + (1|block/fung), data = fung_sp) # anova car::Anova(m, type = 2, test.statistic = &quot;F&quot;) ## Analysis of Deviance Table (Type II Wald F tests with Kenward-Roger df) ## ## Response: yield ## F Df Df.res Pr(&gt;F) ## fung 40.2718 1 3 0.007915 ** ## gen 7.2008 69 414 &lt; 2.2e-16 *** ## fung:gen 0.9331 69 414 0.629010 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 5.2.1 Why do we call these multi-level models? The data are clustered in groups that were generated under similar conditions. What are those groups in the barley example? Figure 5.1: Intuitive visualization of a split-plot design model as a hierarchical (multilevel) model. 5.2.2 Degrees of freedom in a mixed model Degrees of freedom for means are not that easy and straightforward to compute under mixed models anymore. There are approximate methods to calculate the degrees of freedom, like Satterthwaite or Kenward-Roger. mg_means &lt;- emmeans(m, ~fung:gen) head(mg_means) ## fung gen emmean SE df lower.CL upper.CL ## F1 G01 5.24 0.174 31.4 4.88 5.59 ## F2 G01 4.58 0.174 31.4 4.23 4.94 ## F1 G02 5.38 0.174 31.4 5.02 5.73 ## F2 G02 4.75 0.174 31.4 4.39 5.11 ## F1 G03 6.53 0.174 31.4 6.18 6.89 ## F2 G03 5.62 0.174 31.4 5.26 5.97 ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 However, computing the degrees of freedom for the comparisons is much more straightforward (esp. because this a balanced design). Take a look at the ANOVA table, and the experimental units. sigma2_e &lt;- sigma(m)^2 sigma2_wp &lt;- as.data.frame(VarCorr(m))[1,]$vcov sigma2_b &lt;- as.data.frame(VarCorr(m))[2,]$vcov levels_wp &lt;- dplyr::n_distinct(fung_sp$fung) levels_sp &lt;- dplyr::n_distinct(fung_sp$gen) reps &lt;- dplyr::n_distinct(fung_sp$block) Differences between levels of the factor at the whole plot – \\(se(\\mu_{i\\cdot}-\\mu_{i&#39;\\cdot}) = \\sqrt{\\frac{2(\\sigma^2_e + g\\sigma^2_u)}{gb}}\\). # get the s.e. for comparisons between fungicide treatments by hand sqrt( 2*(sigma2_e + levels_sp*sigma2_wp) / (levels_sp*reps)) ## [1] 0.086331 emmeans(m, ~fung, contr = list(c(1, -1)))$contr ## NOTE: Results may be misleading due to involvement in interactions ## contrast estimate SE df t.ratio p.value ## c(1, -1) 0.548 0.0863 3 6.346 0.0079 ## ## Results are averaged over the levels of: gen ## Degrees-of-freedom method: kenward-roger Differences between levels of the factor at the split plot – \\(se(\\mu_{\\cdot j}-\\mu_{\\cdot j&#39;}) = \\sqrt{\\frac{2 \\sigma^2_e }{fb}}\\). # get the s.e. for comparisons between genotype treatments by hand sqrt( 2*(sigma2_e ) / (levels_wp*reps)) ## [1] 0.1405927 emmeans(m, ~gen, contr = list(c(1, -1, rep(0, 68))))$contr ## NOTE: Results may be misleading due to involvement in interactions ## contrast estimate SE df t.ratio p.value ## c(1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -0.152 0.141 414 -1.085 0.2787 ## ## Results are averaged over the levels of: fung ## Degrees-of-freedom method: kenward-roger 5.3 Next week Monday: shrinkage, use of BLUPs versus BLUEs, REML Wednesday: GLMMs "],["linear-mixed-models-3.html", "Day 6 Linear Mixed Models 6.1 Announcements 6.2 Mixed Models Review 6.3 Estimation of parameters in Mixed Models 6.4 R demo", " Day 6 Linear Mixed Models September 15th 6.1 Announcements Linear mixed models workshop Assignment 3 is posted. See Google docs here. Assignment 2 grades will be posted this evening. Project proposal due Sept 24 (next week) Schedule an appointment Talk to your classmates to find a partner What’s that wooden cup I drink from: Lionel Messi drinking mate after winning the World Cup in 2022. Yerba mate Wikipedia Yerba Mate Tea (Ilex paraguariensis): A Comprehensive Review on Chemistry, Health Implications, and Technological Considerations. Heck and De Mejia, 2007. 6.2 Mixed Models Review Distributions of the data Linear predictor Models for some of the parameters (e.g., \\(u \\sim N(0, \\sigma^2_u)\\)). Mixed models are hierarchical models Sharing information between groups 6.3 Estimation of parameters in Mixed Models 6.3.1 Analysis of variance (ANOVA) ANOVA can be considered a special case of the linear model. Each row of the ANOVA table corresponds to a source of variation and the variance of a corresponding to a set of regression coefficients. ANOVA table for split-plot table { width: 100%; border-collapse: collapse; margin: 20px 0; } th, td { border: 1px solid #ddd; padding: 8px; text-align: left; } th { background-color: #f4f4f4; font-weight: bold; } tr:nth-child(even) { background-color: #f9f9f9; } Source df Sum of Squares (Partial or type III) Mean Square Expected Mean Square Block \\[r-1\\] \\[SS_b\\] \\[\\frac{SS_b}{df_b}\\] \\[\\sigma^2_{\\varepsilon}+c\\sigma^2_w+ac\\sigma^2_b\\] A \\[a-1\\] \\[SS_A\\] \\[\\frac{SS_A}{df_A}\\] \\[\\sigma^2_{\\varepsilon}+c\\sigma^2_w+\\phi^2(\\alpha)\\] Error(whole plot) \\[(r-1)(a-1)\\] \\[SS_w\\] \\[\\frac{SS_w}{df_w}\\] \\[\\sigma^2_{\\varepsilon}+c\\sigma^2_w\\] C \\[c-1\\] \\[SS_C\\] \\[\\frac{SS_C}{df_C}\\] \\[\\sigma^2_{\\varepsilon}+\\phi^2(\\gamma)\\] A x C \\[(a-1)(c-1)\\] \\[SS_{AC}\\] \\[\\frac{SS_{AC}}{df_{AC}}\\] \\[\\sigma^2_{\\varepsilon}+\\phi^2(\\alpha \\gamma)\\] Error(split plot) \\[a(r-1)(c-1)\\] \\[SS_e\\] \\[\\frac{SS_e}{df_e}\\] \\[\\sigma^2_{\\varepsilon}\\] See Gelman (2005). 6.3.2 Estimation of fixed effects Fixed effects are normally estimated using least squares (\\(\\hat{\\boldsymbol{\\beta}}_{LSE}\\)) or maximum likelihood estimation (\\(\\hat{\\boldsymbol{\\beta}}_{MLE}\\)). Under the assumption of a normal distribution, \\(\\hat{\\boldsymbol{\\beta}}_{LSE}=\\hat{\\boldsymbol{\\beta}}_{MLE} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\). Least squares estimation (LSE) does not make distributional assumptions, it’s basically just optimizing a function of the squared differences between the estimations and the observations. \\[(\\mathbf{y}-\\mathbf{X}\\hat{\\boldsymbol{\\beta}})^2 = (\\mathbf{y}-\\mathbf{X}\\hat{\\boldsymbol{\\beta}})^T(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\hat{\\beta}}) = \\mathbf{y}^T\\mathbf{y} - 2\\mathbf{y}^T\\mathbf{X}\\hat{\\boldsymbol{\\beta}} + \\hat{\\boldsymbol{\\beta}}^T \\mathbf{X}^T \\mathbf{X} \\hat{\\boldsymbol{\\beta}}\\] Set first derivative to zero \\[\\underset{\\boldsymbol{\\hat{\\beta}}}{\\mathrm{argmin}}\\, (\\mathbf{y}-\\mathbf{X}\\hat{\\boldsymbol{\\beta}})^2 = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\] Because LSE does not make distributional assumptions, there’s no variance parameter associated to our model, and we don’t get any uncertainty estimates. Maximum likelihood estimation (MLE) does make distributional assumptions. \\[\\hat{\\boldsymbol{\\beta}} = \\underset{\\boldsymbol{\\hat{\\beta}}} {\\mathrm{argmax}}\\, \\mathcal{L}_n(\\boldsymbol{\\hat{\\beta}};\\mathbf{y})\\] Computationally, we normally target the log-likelihood: \\[\\hat{\\boldsymbol{\\beta}} = \\underset{\\boldsymbol{\\hat{\\beta}}} {\\mathrm{argmin}}\\, - \\ln \\mathcal{L}_n(\\boldsymbol{\\hat{\\beta}};\\mathbf{y})\\] Fixed effects are often called BLUEs: Best: minimum variance Linear Unbiased Estimator 6.3.3 Estimation of variance components Variance components may be estimated with a range of different methods. Maximum Likelihood Estimation of variance components provides downward biased estimates for small data (most experimental data). \\(\\ell_{ML}(\\boldsymbol{\\sigma; \\boldsymbol{\\beta}, \\mathbf{y}}) = - (\\frac{n}{2}) \\log(2\\pi)-(\\frac{1}{2}) \\log ( \\vert \\mathbf{V}(\\boldsymbol\\sigma) \\vert ) - (\\frac{1}{2}) (\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta})^T[\\mathbf{V}(\\boldsymbol\\sigma)]^{-1}(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\beta})\\) Restricted Maximum Likelihood (REML) avoids MLE’s downward bias under small data and is thus is the default in most mixed effects models. In REML, the likelihood is maximized after accounting for the model’s fixed effects. In REML, \\(\\ell_{REML}(\\boldsymbol{\\sigma};\\mathbf{y}) = - (\\frac{n-p}{2}) \\log (2\\pi) - (\\frac{1}{2}) \\log ( \\vert \\mathbf{V}(\\boldsymbol\\sigma) \\vert ) - (\\frac{1}{2})log \\left( \\vert \\mathbf{X}^T[\\mathbf{V}(\\boldsymbol\\sigma)]^{-1}\\mathbf{X} \\vert \\right) - (\\frac{1}{2})\\mathbf{r}[\\mathbf{V}(\\boldsymbol\\sigma)]^{-1}\\mathbf{r}\\), where \\(p = rank(\\mathbf{X})\\), \\(\\mathbf{r} = \\mathbf{y}-\\mathbf{X}\\hat{\\boldsymbol{\\beta}}_{ML}\\). Start with initial values for \\(\\boldsymbol{\\sigma}\\), \\(\\tilde{\\boldsymbol{\\sigma}}\\). Compute \\(\\mathbf{G}(\\tilde{\\boldsymbol{\\sigma}})\\) and \\(\\mathbf{R}(\\tilde{\\boldsymbol{\\sigma}})\\). Obtain \\(\\boldsymbol{\\beta}\\) and \\(\\mathbf{b}\\). Update \\(\\tilde{\\boldsymbol{\\sigma}}\\). Repeat until convergence. Variance components may be \\(&lt;0\\); in that case they are set to zero. Method of moments (MoM) is derived from ANOVA tables that include the expected mean squares. Use actual sums of squares and expected mean squares (EMS) from ANOVA to clear for the different variance components. e.g., \\(\\tilde{\\sigma}^2_{w} = \\frac{SSw - SS_e}{c}\\) (See table above). However, if \\(\\tilde{\\sigma}^2_{w} &lt; 0\\), \\(\\hat{\\sigma}^2_{w} = 0\\). Issues with variance estimates equal to zero Type I error inflation Frey et al. (2024) 6.3.4 Estimating random effects Random effects arise from a multivariate normal distribution \\[\\mathbf{u} \\sim MVN (\\boldsymbol{0}, \\mathbf{G}),\\] where \\(\\mathbf{G} = \\begin{bmatrix} \\sigma^2_b \\mathbf{I} &amp; 0 \\\\ 0&amp; \\sigma^2_w \\mathbf{I} \\end{bmatrix}\\). Random effects are shrunk. Shrinkage depends on magnitude of variance estimate. Random effects are often called BLUPs: Best: minimum variance, Linear Unbiased Predictor 6.3.5 Degrees of freedom in mixed models Denominator degrees of freedom may not be exactly derived from ANOVA tables. Increase the complexity of covariance structure, and the df will be more different to the ones on the ANOVA table. Degrees of freedom are approximated under mixed models for bias correction. Satterthwaite approximation Kenward-Roger approximation 6.4 R demo Download R code "],["generalized-linear-mixed-models.html", "Day 7 Generalized Linear Mixed Models 7.1 Announcements 7.2 Review about the (general) linear mixed model 7.3 Generalized linear mixed models 7.4 Reading", " Day 7 Generalized Linear Mixed Models September 17th 7.1 Announcements Assignment 2 grades are posted Assignment 3 due on Sunday This blog post on “bad science” 7.2 Review about the (general) linear mixed model One of the most common notations is the model equation form: \\[y_{ij} = \\mu + T_i + \\varepsilon_{ij} , \\\\ \\varepsilon_{ij}\\sim N(0, \\sigma^2),\\] which is very much restrictive. 7.3 Generalized linear mixed models Now, if we relax the assumption of the normal distributions, there are several other probability distributions that could describe how the data are generated. GLMMs are linear models for variables from any distribution from the exponential family. Exponential family: pdf can be written as \\[f(y\\vert \\theta) = \\exp \\begin{bmatrix} \\frac{y\\theta- b(\\theta)}{a(\\theta)} + c(y, \\phi) \\end{bmatrix} ,\\] where \\(y\\) is the random variable (e.g., the response), \\(\\theta\\) is a canonical parameter (related to the mean) and \\(\\phi\\) is a scale parameter (related to the dispersion). Figure 7.1: Common variable distributions. Page 60 in Stroup et al. (2024) Probability distributions 7.3.1 Three steps to modeling data Define the linear predictor \\(\\boldsymbol\\eta = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{Z}\\mathbf{u}\\), Define the probability distribution for the data, \\(\\mathbf{y}|\\mathbf{u}\\), Define the link function \\(g(\\cdot)\\), \\(\\boldsymbol\\eta = g(\\mathbf{y}|\\mathbf{u})\\). 7.3.2 Implications for model fitting Least Squares Estimator is no longer Maximum Likelihood Estimator Variance is no longer \\(\\hat\\sigma^2 = \\frac{SSE}{df_e}\\) The whole concept of degrees of freedom is more diffuse ANOVA shells are still useful to analyze designs are number of independent, true, replicates The \\(Var(\\mathbf{y}|\\mathbf{u})\\) may be specified, but not specify the full likelihood (check out the properties above) Quasi-likelihood for modeling overdispersion or repeated measures in GLMMs: \\(E(\\mathbf{y} \\vert \\mathbf{b}) = \\boldsymbol{\\mu}\\vert \\mathbf{b}\\) \\(Var(\\mathbf{y} \\vert \\mathbf{b}) = \\mathbf{V}_{\\mu}^{1/2}\\mathbf{A}\\mathbf{V}_{\\mu}^{1/2}\\) 7.4 Reading Stats majors: Chapter 5 in Stroup et al. (2024) Non-stats majors: LMMs and GLMMs chapter by Ben Bolker The value of generalized linear mixed models for data analysis in the plant sciences Generalized Linear Mixed Models in Dairy Cattle Breeding "],["inference-estimability-degrees-of-freedom-hypothesis-tests-contrasts.html", "Day 8 Inference – Estimability, degrees of freedom, hypothesis tests, contrasts 8.1 Announcements 8.2 Review: Hypothesis tests, t-tests, F tests 8.3 Applied example 8.4 Estimability 8.5 Applied example II 8.6 Wednesday", " Day 8 Inference – Estimability, degrees of freedom, hypothesis tests, contrasts 8.1 Announcements Highlight questions from Assignment 3: Why are degrees of freedom not always integer numbers? What is the difference between sums of squares? What is the best size of blocks when designing an experiment? Figure 8.1: Overview of today’s class. 8.2 Review: Hypothesis tests, t-tests, F tests 8.2.1 Hypothesis tests Classic statistics have been deeply associated to falsifiability in the advancement of Science. Hugely influenced by Karl Popper, the advancement of Science can be understood as a string of hypotheses that are constantly evaluated. In statistics, hypothesis tests evaluate whether a given statistic (i.e., a function of the data) is too extreme for the null hypothesis to be true. Normally, the null hypothesis is business as usual, and we want to evaluate if we just made a scientific discovery, or if there’s not enough evidence to say that there’s something different going on. Figure 8.2: Types of errors in hypothesis tests. 8.2.2 On p-values The p value is the probability of observing the [t/F] statistic under the null hypothesis. Often, we assume \\(\\alpha = 0.05\\), and reject \\(H_0\\) if \\(p&lt;\\alpha\\). ASA’s statement on p-values Scientists rise up against statistical significance Discussion Counterargument: In defense of p-values Gelman “Let us have the serenity to embrace the variation that we cannot reduce, the courage to reduce the variation we cannot embrace, and the wisdom to distinguish one from the other.” [see talk] 8.2.3 t-tests Usually used to test whether one value/parameter/linear combination of parameters is different to a certain number. Steps: Define the target quantity \\(\\theta\\). For example, \\(\\theta = \\beta_1-\\beta_2\\). After observing the data, estimate \\(\\hat{\\boldsymbol\\beta}\\) and \\(se(\\hat{\\boldsymbol\\beta})\\). Calculate the test \\(t\\) statistic as \\(t^\\star = \\frac{\\hat\\theta}{se(\\hat\\theta)}\\). Compare \\(t^{\\star}\\) to \\(t_{1-\\frac{\\alpha}{2}, dfe}\\). Why \\(t\\) distribution? \\(N\\) for known se, \\(t\\) for \\(\\hat{se}\\). Note that \\(t^\\star\\) depends on: sample size (\\(dfe\\)) experiment design (other df) the variability \\(\\sigma^2\\) associated to that system. Note that \\(t_{crit}\\) depends on \\(dfe\\). 8.2.4 F-tests are multivariate t-tests Test the relevance of multiple factors at once. Consistent with the “bins” concept of ANOVA. Usually used to test whether a group of factor is relevant to explain variability in the data. Why \\(F\\) distribution? Ratio between two independent \\(\\chi^2\\) distributions divided by their df. Steps: Calculate Mean Squared Errors for the different sources of variability \\(= \\frac{SSH}{df}\\). Calculate the F statistic as the ratio between mean squares, \\(=\\frac{MSH}{MSE}\\). Compute \\(P(F&gt;F^\\star)\\), using the degrees of freedom of both mean squares, df of the numerator and df of the denominator. Note, again, the relationship between the p value and sample size and \\(\\sigma^2\\). Note, again, that the \\(P(F&gt;F^\\star)\\) depends on the df of the denominator. 8.3 Applied example From Milliken and Johnson (2009). A baker wanted to determine the effect that the amount of fat in a recipe of cookie dough would have on the texture of the cookie. She also wanted to determine if the temperature (°F) at which the cookies were baked would have an influence on the texture of the surface. The texture of the cookie is measured by determining the amount of force (g) required to penetrate the cookie surface. Experimentation: The process she used was to make a batch of cookie dough for each of the four recipes every day, and baked them one by one in the oven in different batches. She carried this process out each of four days when she baked cookies at three different temperatures. library(tidyverse) library(lme4) library(emmeans) df &lt;- read.csv(&quot;../data/cookies.csv&quot;) df$Temperature &lt;- as.factor(df$Temperature) df$fat_perc &lt;- as.factor(df$fat_perc) df$Day &lt;- as.factor(df$Day) df %&gt;% ggplot(aes(Temperature, force))+ theme_classic()+ scale_fill_manual(values = c(&quot;#DCD6F7&quot;, &quot;#B4869F&quot;, &quot;#985F6F&quot;, &quot;#4E4C67&quot;))+ labs(x = &quot;Temperature (°F)&quot;, fill = &quot;Fat (%)&quot;, y = &quot;Force (g)&quot;)+ theme(aspect.ratio = .5)+ geom_boxplot(aes(group = paste(Temperature, fat_perc), fill = factor(fat_perc))) ANOVA table for cookie RCBD ANOVA table for cookie RCBD table { width: 100%; border-collapse: collapse; margin: 20px 0; } th, td { border: 1px solid #ddd; padding: 8px; text-align: left; } th { background-color: #f4f4f4; font-weight: bold; } tr:nth-child(even) { background-color: #f9f9f9; } Source df Block \\(r-1 = 3\\) Temp \\(t-1 = 2\\) Fat \\(f-1 = 3\\) \\(T \\times F\\) \\((t-1)(f-1) = 6\\) Error \\(N-tf-(r-1) = 33\\) The corresponding model is \\[y_{ijk} = \\mu + T_i + F_j + TF_{ij} + d_k + \\varepsilon_{ijk} , \\\\ d_k \\sim N(0, \\sigma^2_d), \\\\ \\varepsilon_{ijk} \\sim N(0, \\sigma^2_\\varepsilon), \\] and can be fitted to the data with the following R code: m &lt;- lmer(force ~ 1 + Temperature*fat_perc + (1|Day), data = df) The degrees of freedom for the denominator are straightforward. 8.3.1 t-test Objective: compare treatment means between 375 degrees temperature and 4% fat and 400 degrees temperature and 4% fat. Remember \\(t^\\star = \\frac{\\hat\\theta}{se(\\hat\\theta)}\\) \\(\\hat\\theta = \\mu_{22} - \\mu_{32}\\) \\(se(\\hat\\theta) = \\sqrt{\\frac{2 \\sigma^2_\\varepsilon}{r}}\\) R code to get t statistics by hand d &lt;- data.frame(Temperature = as.factor(c(375, 400)), fat_perc = factor(4)) hat.mu &lt;- predict(m, newdata = d, re.form = NA) hat.theta &lt;- hat.mu[2]-hat.mu[1] hat.sigma2 &lt;- sigma(m)^2 hat.sigma2_b &lt;- as.data.frame(VarCorr(m))[1,4] reps &lt;- n_distinct(df$Day) se.hat.theta &lt;- sqrt((2* hat.sigma2)/reps) emmeans(m, ~Temperature:fat_perc, contr = list(c(0, 0, 0, 0, -1, 1, rep(0, 6))))$contrasts ## contrast estimate SE df t.ratio p.value ## c(0, 0, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0) 2.92 0.633 33 4.621 0.0001 ## ## Degrees-of-freedom method: kenward-roger hat.theta ## 2 ## 2.925 se.hat.theta ## [1] 0.6330342 t.star &lt;- hat.theta/se.hat.theta t.star ## 2 ## 4.620603 dt(t.star, df = 33) ## 2 ## 8.202921e-05 8.3.2 F-test Objective: find out if temperature, in general, affects the texture of the cookie. \\(H_0:\\) all temperatures have the same effect on texture. \\(H_a:\\) at least one temperature has a different effect on texture. \\(F^\\star = \\frac{MS_{Temp}}{MS_{resid}}\\) \\(F_{critical} = F_{1-\\alpha, df_{numerator}, df_{denominator}}\\) R code to get F statistics by hand # create models that account for different sources of variability m.full &lt;- lmer(force ~ 1 + Temperature*fat_perc + (1|Day), data = df) m.TF &lt;- lmer(force ~ 1 + Temperature + fat_perc + (1|Day), data = df) m.T &lt;- lmer(force ~ 1 + Temperature + (1|Day), data = df) m.F &lt;- lmer(force ~ 1 + fat_perc + (1|Day), data = df) m.n &lt;- lmer(force ~ 1 + (1|Day), data = df) # SS II residual sum(resid(m.full)^2) ## [1] 26.84223 # SS II TxF sum((predict(m.full, re.form = NA) - predict(m.TF, re.form = NA))^2) ## [1] 1.505 # SS II F sum((predict(m.TF, re.form = NA) - predict(m.T, re.form = NA))^2) ## [1] 3.665 # SS II T sum((predict(m.TF, re.form = NA) - predict(m.F, re.form = NA))^2) ## [1] 179.5317 # P(F&gt;Fcrit) for temperature F.T &lt;- (179.5317/2)/(26.44833/33) 1 - pf(F.T, df1 = 2, df2 = 33) ## [1] 1.998401e-15 # P(F&gt;Fcrit) for fat% F.F &lt;- (3.665/3)/(26.44833/33) 1 - pf(F.F, df1 = 3, df2 = 33) ## [1] 0.2264496 # P(F&gt;Fcrit) for TxF F.TF &lt;- (1.505/6)/(26.44833/33) 1 - pf(F.TF, df1 = 6, df2 = 33) ## [1] 0.9256322 8.4 Estimability A quantity is estimable if it is possible to find a linear combination of the data that provides an unbiased and unique estimate of that quantity. For matrix models, linear estimable functions of \\(\\boldsymbol\\beta\\) take on the form of linear combinations of the parameter vector such as \\(a&#39;\\boldsymbol\\beta\\) where a is a \\(p\\times1\\) vector of constants. A linear function \\(a&#39;\\boldsymbol\\beta\\) is estimable if and only if there exists a vector \\(r\\) such that \\(a = \\mathbf{X}&#39;\\mathbf{X}r\\). Each function \\(x_i&#39;\\boldsymbol\\beta\\) is estimable where \\(x_i\\) is the \\(i\\)th row of \\(\\mathbf{X}\\). 8.4.1 Contrasts Linear combinations of parameters. Often connected to hypothesis tests where \\(H_0: \\mathbf{K}\\boldsymbol\\beta = 0\\). We specify the matrix \\(\\mathbf{K}\\) that combines elements in \\(\\hat{\\boldsymbol\\beta}\\). For example, if \\(\\mathbf{K} = \\begin{bmatrix} 1 &amp; -1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; -1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{bmatrix}\\), we are estimating the contrast between [2%Fat, 350°F] and [2%Fat, 375°F] (row 1), and [4%Fat, 375°F] and [4%Fat, 400°F]. emmeansalso gives us some SE, a t value and p value associated to a hypothesis test: \\(H_0:\\) each row =0. emmeans(m, ~Temperature:fat_perc, contr = list(c( 1, -1, rep(0, 10)), c(0, 0, 0, 0, -1, 1, rep(0, 6)))) ## $emmeans ## Temperature fat_perc emmean SE df lower.CL upper.CL ## 350 2 8.05 0.534 18.2 6.93 9.17 ## 375 2 10.00 0.534 18.2 8.88 11.12 ## 400 2 12.12 0.534 18.2 11.00 13.25 ## 350 4 7.42 0.534 18.2 6.30 8.55 ## 375 4 9.43 0.534 18.2 8.30 10.55 ## 400 4 12.35 0.534 18.2 11.23 13.47 ## 350 6 7.33 0.534 18.2 6.20 8.45 ## 375 6 9.18 0.534 18.2 8.05 10.30 ## 400 6 12.25 0.534 18.2 11.13 13.37 ## 350 8 7.00 0.534 18.2 5.88 8.12 ## 375 8 8.95 0.534 18.2 7.83 10.07 ## 400 8 11.93 0.534 18.2 10.80 13.05 ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## c(1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0) -1.95 0.633 33 -3.080 0.0041 ## c(0, 0, 0, 0, -1, 1, 0, 0, 0, 0, 0, 0) 2.92 0.633 33 4.621 0.0001 ## ## Degrees-of-freedom method: kenward-roger 8.4.2 Adjusting degrees of freedom for mixed models From Assignment 3: In class we learned that degrees of freedom represent the number of independent pieces of information. In the ANOVA tables these appear as whole numbers, but when we run mixed models in R I sometimes see fractional values (for example, 27.4). Why do mixed models produce non-integer degrees of freedom, and could you also provide a simpler or more intuitive way to think about what degrees of freedom really mean? Answer: According to my understanding, in ANOVA, degrees of freedom come directly from how many observations are available and how many parameters are estimated, which gives whole numbers. Mixed models include both random and fixed effects and the structure of data involves often correlations among the observations for example: repeated measures on the same subject or split-plot designs). In this case, the information available to estimate parameters is not clearly separated as ANOVA. So, degrees of freedom in mixed models represent an effective amount of independent information, this might be the reason they can be fractional. Contrasts for balanced &amp; complete designs have straightforward calculation of degrees of freedom. For unbalanced designs, incomplete blocks, missing data, and cases with more complex variance-covariance structure, the ANOVA shell does not give us exact degrees of freedom. Downward bias of the variance of an estimable function, leading to: too narrow confidence intervals inflated type I error rates Bias adjustment methods estimate the approximate degrees of freedom, which might be a non-integer number. Satterthwaite approximation Paper Let \\(\\frac{X^2_{num}/\\nu_1}{X_2^\\star/\\nu^\\star_2}\\) be the ratio of interest, where \\(X_{num}^2 \\sim \\chi^2_{\\nu_1}\\) and \\(X_2^\\star\\) is a linear combination of chi-square random variables all independent of \\(X_{num}^2\\), then \\(X_2^\\star \\sim\\) approximately \\(\\chi^2_{\\nu^\\star_2}\\), where \\[\\nu_2^\\star \\equiv \\frac{(\\sum_m c_m X_m^2)^2}{\\sum_m\\frac{(c_m X_m^2)}{df_m}}.\\] Kenward-Roger approximation Paper Based on REML Focused on \\((\\mathbf{X}^T\\mathbf{V}^{-1} \\mathbf{X})^{-1}\\) (the variance of \\(\\hat{\\boldsymbol\\beta}\\)) Again, expept for variance-component-only LMMs with balanced data and their compound symmetry marginal model equivalents, the estimated covariance of \\(\\beta \\mathbf{K}\\) will tend to be underestimated. KR somewhat more conservative and reliable than Satterthwaite (source). Integrated in most statistical software. R implementation emmeans uses Kenward-Roger by default. [see vignette] Everything is more tricky in GLMMs (df is Inf in most applications). 8.5 Applied example II From Milliken and Johnson (2009). A baker wanted to determine the effect that the amount of fat in a recipe of cookie dough would have on the texture of the cookie. She also wanted to determine if the temperature (°F) at which the cookies were baked would have an influence on the texture of the surface. The texture of the cookie is measured by determining the amount of force (g) required to penetrate the cookie surface. Experimentation: The process she used was to make a batch of cookie dough for each of the four recipes every day, and baked one cookie from each recipe in the oven together. She carried this process out each of four days when she baked cookies at three different temperatures. ANOVA table for the cookie split-plot experiment. table { width: 100%; border-collapse: collapse; margin: 20px 0; } th, td { border: 1px solid #ddd; padding: 8px; text-align: left; } th { background-color: #f4f4f4; font-weight: bold; } tr:nth-child(even) { background-color: #f9f9f9; } Source df EMS Block \\(r-1 = 3\\) \\[\\sigma^2_{\\varepsilon}+f\\sigma^2_w+tf\\sigma^2_d\\] Temperature \\(t-1\\) = 2 \\[\\sigma^2_{\\varepsilon}+f\\sigma^2_w+\\phi^2(\\alpha)\\] Error(oven) \\((r-1)(t-1) = 6\\) \\[\\sigma^2_{\\varepsilon}+f\\sigma^2_w\\] Fat (%) \\(f-1 = 3\\) \\[\\sigma^2_{\\varepsilon}+\\phi^2(\\gamma)\\] \\(T \\times F\\) \\((t-1)(f-1) = 6\\) \\[\\sigma^2_{\\varepsilon}+\\phi^2(\\alpha \\gamma)\\] Error(split plot) \\(t(r-1)(f-1) = 27\\) \\[\\sigma^2_{\\varepsilon}\\] The corresponding model is \\[y_{ijk} = \\mu + T_i + F_j + TF_{ij} + d_k + w_{i(k)} + \\varepsilon_{ijk} , \\\\ d_k \\sim N(0, \\sigma^2_d), \\\\ w_{i(k)} \\sim N(0, \\sigma^2_w), \\\\ \\varepsilon_{ijk} \\sim N(0, \\sigma^2_\\varepsilon), \\] and can be fitted to the data with the following R code: m_splitplot &lt;- lmer(force ~ 1 + Temperature*fat_perc + (1|Day/Temperature), data = df) The degrees of freedom for the denominator are not so straightforward anymore. emmeans(m_splitplot, ~ Temperature:fat_perc, contr = list(c(1, -1, rep(0, 10)))) ## $emmeans ## Temperature fat_perc emmean SE df lower.CL upper.CL ## 350 2 8.05 0.534 10 6.86 9.24 ## 375 2 10.00 0.534 10 8.81 11.19 ## 400 2 12.12 0.534 10 10.93 13.32 ## 350 4 7.42 0.534 10 6.23 8.62 ## 375 4 9.43 0.534 10 8.23 10.62 ## 400 4 12.35 0.534 10 11.16 13.54 ## 350 6 7.33 0.534 10 6.13 8.52 ## 375 6 9.18 0.534 10 7.98 10.37 ## 400 6 12.25 0.534 10 11.06 13.44 ## 350 8 7.00 0.534 10 5.81 8.19 ## 375 8 8.95 0.534 10 7.76 10.14 ## 400 8 11.93 0.534 10 10.73 13.12 ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## c(1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0) -1.95 0.731 6.79 -2.667 0.0331 ## ## Degrees-of-freedom method: kenward-roger 8.6 Wednesday Come early and bring your computer. Read Ch 6 in Stroup et al (2024). "],["practice.html", "Day 9 Practice 9.1 Announcements 9.2 In-class activity", " Day 9 Practice 9.1 Announcements Project proposal is due tomorrow. Today, instead of a Kahoot!, we’ll have an in-class activity. 9.2 In-class activity Gather in teams of maximum 5 people. You are consultant statisticians. A researcher has come to you to request help analyzing data generated by a designed experiment. They described the experiment in the following email: I hope you are doing well. I am writing to request assistance with my statistical analysis for an aquaculture clinical field trial in the Bay of Fundy, Canada. Please find a description of the study below. The objective of our research was to compare the performance of different vaccines on the growth and survival of Atlantic salmon under standard production conditions. On day 0 of the study, we applied 4 different vaccines to a total of 100 randomly selected fish. Salmons were individually tagged in February 2005 and followed through to harvest in August 2007. All fish were always kept in the same cage. We recorded (1) presence of jaw deformity, and (2) weight throughout the trial. As jaw deformities are formed at an early stage of growth and are not healed, detection of a jaw deformity at any sampling event implied that the fish was labelled as having a jaw deformity throughout the entire growth period. The outcome of interest is the weight, as weights increase from means of 60 g at day 0, to 5,700 g at harvest. However, it is also important to know whether there are differences in propensity to presenting jaw deformity among vaccines. I need help with the following: Describing the experiment design for materials and methods. Analyzing the data and determining: If there are differences between vaccines, especially at harvest, If there is a “better vaccine” for final weight, If there is difference in jaw deformity between vaccine treatments. Download the Rmd that will give you access to the data. FINISH BY 12:20PM &amp; class debate Read the data, analyze it, and answer the researcher’s questions. Mention one way they could improve the power of their experiment to detect differences in final weight between treatments. You may ask the researcher for missing information or for clarifications, if you think there’s information missing (ask Josefina). Criteria for Kinder competition: Getting the correct answer Time "],["repeated-measures-practice-ii.html", "Day 10 Repeated Measures, Practice II 10.1 Announcements 10.2 Repeated measures 10.3 Last week’s practice 10.4 Task for today", " Day 10 Repeated Measures, Practice II 10.1 Announcements Comments on project proposals. Zoom classes on 10/13 and 10/15. 10.2 Repeated measures Repeated measures designs as a special case of split-plot design. Split-plot designs: Figure 10.1: Schematic description of a field experiment with a split-plot design Model behind split plots: \\[y_{ijk} \\vert \\boldsymbol{u} \\sim P(\\mu_{ijk}, \\phi),\\\\ g(\\mu_{ijk}) = \\eta_{ijk} = \\eta_0 + A_i + C_j +AC_{ij} + b_k + u_{i(k)},\\\\ b_k \\sim N(0, \\sigma^2_b),\\\\ u_{i(k)} \\sim N(0, \\sigma^2_u),\\] where: \\(y_{ijk}\\) is the observed value for the \\(i\\)th level of treatment factor A, the \\(j\\)th level of treatment factor B in the \\(k\\)th block, that arises from a probability distribution \\(P\\), where: \\(\\mu_{ijk}\\) is the expected value for that observation, \\(\\eta_{ijk}\\) is the linear predictor for \\(\\mu\\) after applying the link function \\(g(\\cdot)\\), \\(\\phi\\) is the dispersion parameter, \\(\\eta_0\\) is the overall mean for the linear predictor, \\(A_i\\) is the effect of the \\(i\\)th level of treatment factor A, \\(C_j\\) is the effect of the \\(j\\)th level of treatment factor B \\(AC_{ij}\\) is the interaction between the \\(i\\)th level of treatment factor A and the \\(j\\)th level of treatment factor B in the \\(k\\)th block, \\(b_k\\) is the effect of the \\(k\\)th block, \\(u_{i(k)}\\) is the effect of the whole plot of the \\(i\\)th level of treatment factor A within block \\(k\\). Under a Gaussian distribution, we can also write \\[y_{ijk} = \\mu_0 + A_i + C_j +AC_{ij} + b_k + u_{i(k)} + \\varepsilon_{ijk},\\\\ b_k \\sim N(0, \\sigma^2_b),\\\\ u_{i(k)} \\sim N(0, \\sigma^2_u), \\\\ \\varepsilon_{ijk} \\sim N(0, \\sigma^2_\\varepsilon).\\] Assumptions for residuals in marginal distributions Repeated measures: Figure 10.2: Schematic description of a field experiment with repeated measures 10.2.1 Correlation functions Let \\(\\mathbf{v}_{i k}\\) be the vector for the \\(k\\)th individual under the \\(i\\)th vaccine treatment, at the different timepoints of the experiment. \\[\\mathbf{v}_{i k} \\sim MVN(\\boldsymbol{0}, \\boldsymbol{\\Sigma}_{ik})\\] Independent observations \\[\\boldsymbol{\\Sigma}_{ik} = \\sigma^2 \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\\\ \\end{bmatrix}\\] Compound symmetry \\[\\boldsymbol{\\Sigma}_{ik} = \\sigma^2 \\begin{bmatrix} 1 &amp; \\rho &amp; \\rho &amp; \\rho &amp; \\rho \\\\ \\rho &amp; 1 &amp; \\rho &amp; \\rho &amp; \\rho \\\\ \\rho &amp; \\rho &amp; 1 &amp; \\rho &amp; \\rho \\\\ \\rho &amp; \\rho &amp; \\rho &amp; 1 &amp; \\rho \\\\ \\rho &amp; \\rho &amp; \\rho &amp; \\rho &amp; 1\\\\ \\end{bmatrix}\\] AR(1) \\[\\boldsymbol{\\Sigma}_{ik} = \\sigma^2 \\begin{bmatrix} 1 &amp; \\rho &amp; \\rho^2 &amp; \\rho^3 &amp; \\rho^4 \\\\ \\rho &amp; 1 &amp; \\rho &amp; \\rho^2 &amp; \\rho^3 \\\\ \\rho^2 &amp; \\rho &amp; 1 &amp; \\rho &amp; \\rho^2 \\\\ \\rho^3 &amp; \\rho^2 &amp; \\rho &amp; 1 &amp; \\rho \\\\ \\rho^4 &amp; \\rho^3 &amp; \\rho^2 &amp; \\rho &amp; 1\\\\ \\end{bmatrix}\\] Unstructured \\[\\boldsymbol{\\Sigma}_{ik} = \\begin{bmatrix} \\sigma^2_{1} &amp; \\sigma^2_{12} &amp; \\sigma^2_{13} &amp; \\sigma^2_{14} &amp; \\sigma^2_{15} \\\\ &amp; \\sigma^2_{2} &amp; \\sigma^2_{23} &amp; \\sigma^2_{24} &amp; \\sigma^2_{25} \\\\ &amp; &amp; \\sigma^2_{3} &amp; \\sigma^2_{34} &amp; \\sigma^2_{35} \\\\ &amp; &amp; &amp; \\sigma^2_{4} &amp; \\sigma^2_{45} \\\\ &amp; &amp; &amp; &amp; \\sigma^2_{5} \\\\ \\end{bmatrix}\\] 10.2.2 Deciding which covariance function for a given problem Over-modeling correlation compromises power and under-modeling compromises type I error control. Information criteria: Used as a metric for predictive ability and model fit. Lower value = better No general rule about AIC magnitude Akaike Information Criterion (AIC): \\(\\text{AIC} = 2p - 2 \\ln(\\hat{L})\\) AICC: correction for small sample sizes \\(\\text{AICc} = \\text{AIC} + \\frac{2p^2+2p}{n-p-1}\\) Bayesian Information Criterion (BIC) \\(\\text{BIC} = p \\log(n) -2\\log(\\hat{L})\\) [\\(p\\): number of parameters estimated in the model, \\(\\hat{L}\\): likelihood for the model, \\(\\hat{L}=p(\\mathbf{y}\\vert \\hat{\\boldsymbol{\\beta}}, M)\\)] Plot covariance as a function of distance (could be time or space). Likelihood ratio tests are not always reliable for mixed models [see Bolker’s GLMM FAQ] 10.3 Last week’s practice We know that last week’s assignment was a clinical field trial comparing the performance of different vaccines on the growth and survival of Atlantic salmon under standard production conditions. Treatment structure: two-way factorial (4 vaccines x 5 times) Design structure: completely randomized design with repeated measures and 100 reps Experimental unit: fish Response: (1) presence of jaw deformity (same for its whole life), and (2) weight. library(tidyverse) # data wrangling library(glmmTMB) # generalized linear mixed models (+ repeated measures) library(lme4) # linear mixed models library(DHARMa) # model diagnostics # load data url &lt;- &quot;https://raw.githubusercontent.com/stat870/fall2025/refs/heads/main/data/fish_vaccines.csv&quot; fish &lt;- read.csv(url) # plot the data fish %&gt;% ggplot(aes(day, wt))+ geom_point(aes(fill = factor(vaccine), group = factor(vaccine)), position = position_dodge(width = 60), shape=21, size =3.5, alpha =.7)+ labs(y = expression(Weight~(g~fish^{-1})), x = &quot;Time (days)&quot;)+ theme_classic()+ labs(fill=&quot;Vaccine treatment&quot;)+ theme(legend.position = &quot;bottom&quot;)+ scico::scale_fill_scico_d() 10.3.1 Weight Considering that the variance increases with the mean, a fair model for fish weight could be: \\[y_{ijk}|u_{ijk} \\sim Gamma(\\mu_{ijk}, \\phi),\\\\ \\log(\\mu_{ijk}) = \\eta_{ijk} = \\eta_0 + V_i + T_j + VT_{ij} + u_{ijk},\\] where: \\(y_{ijk}\\) is the fish weight of the \\(k\\)th fish under the \\(i\\)th vaccine treatment at the \\(j\\)th timepoint, and (conditional on \\(u_{ijk}\\)) arises from a Gamma distribution with mean \\(\\mu_{ijk}\\) and dispersion \\(\\phi\\), \\(\\eta_{ijk}\\) is the linear predictor, \\(\\eta_0\\) is the overall mean mean of the linear predictor, \\(V_i\\) is the effect of the \\(i\\)th vaccine treatment, \\(T_j\\) is the effect of the \\(j\\)th timepoint, \\(VT_{ij}\\) is the interaction between the \\(i\\)th vaccine treatment and the \\(j\\)th timepoint, and \\(u_{ijk}\\) is the random effect for the fish weight of the \\(k\\)th fish under the \\(i\\)th vaccine treatment at the \\(j\\)th timepoint, that is accounting for the fact that repeated measures are not independent. Gamma distribution versus log-transformation. How do we account for the fact that repeated measures are not independent? Independent observations \\[\\boldsymbol{\\Sigma}_{ik} = \\sigma^2 \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\\\ \\end{bmatrix}\\] # let vaccine be a factor fish$vaccine &lt;- as.factor(fish$vaccine) #create a new variable, day as factor fish$day_f &lt;- as.factor(fish$day) m_indep &lt;- glmmTMB(wt ~ vaccine*day_f, REML = TRUE, family = Gamma(link = &quot;log&quot;), data = fish) # dispersion parameter sigma(m_indep) ## [1] 0.215569 Compound symmetry \\[\\boldsymbol{\\Sigma}_{ik} = \\sigma^2 \\begin{bmatrix} 1 &amp; \\rho &amp; \\rho &amp; \\rho &amp; \\rho \\\\ \\rho &amp; 1 &amp; \\rho &amp; \\rho &amp; \\rho \\\\ \\rho &amp; \\rho &amp; 1 &amp; \\rho &amp; \\rho \\\\ \\rho &amp; \\rho &amp; \\rho &amp; 1 &amp; \\rho \\\\ \\rho &amp; \\rho &amp; \\rho &amp; \\rho &amp; 1\\\\ \\end{bmatrix}\\] m_cs &lt;- glmmTMB(wt ~ vaccine*day_f + (1|fish), REML = TRUE, family = Gamma(link = &quot;log&quot;), data = fish) # variance of random effects VarCorr(m_cs) ## ## Conditional model: ## Groups Name Std.Dev. ## fish (Intercept) 0.1486 # dispersion parameter sigma(m_cs) ## [1] 0.1571353 AR(1) \\[\\boldsymbol{\\Sigma}_{ik} = \\sigma^2 \\begin{bmatrix} 1 &amp; \\rho &amp; \\rho^2 &amp; \\rho^3 &amp; \\rho^4 \\\\ \\rho &amp; 1 &amp; \\rho &amp; \\rho^2 &amp; \\rho^3 \\\\ \\rho^2 &amp; \\rho &amp; 1 &amp; \\rho &amp; \\rho^2 \\\\ \\rho^3 &amp; \\rho^2 &amp; \\rho &amp; 1 &amp; \\rho \\\\ \\rho^4 &amp; \\rho^3 &amp; \\rho^2 &amp; \\rho &amp; 1\\\\ \\end{bmatrix}\\] m_ar1 &lt;- glmmTMB(wt ~ vaccine*day_f + ar1(1 + day_f|fish), REML = TRUE, family = Gamma(link = &quot;log&quot;), data = fish) # variance of random effects VarCorr(m_ar1) ## ## Conditional model: ## Groups Name Std.Dev. Corr ## fish (Intercept) 0.1382 0.448 (ar1) # dispersion parameter sigma(m_ar1) ## [1] 0.1111098 Unstructured \\[\\boldsymbol{\\Sigma}_{ik} = \\begin{bmatrix} \\sigma^2_{1} &amp; \\sigma^2_{12} &amp; \\sigma^2_{13} &amp; \\sigma^2_{14} &amp; \\sigma^2_{15} \\\\ &amp; \\sigma^2_{2} &amp; \\sigma^2_{23} &amp; \\sigma^2_{24} &amp; \\sigma^2_{25} \\\\ &amp; &amp; \\sigma^2_{3} &amp; \\sigma^2_{34} &amp; \\sigma^2_{35} \\\\ &amp; &amp; &amp; \\sigma^2_{4} &amp; \\sigma^2_{45} \\\\ &amp; &amp; &amp; &amp; \\sigma^2_{5} \\\\ \\end{bmatrix}\\] m_us &lt;- glmmTMB(wt ~ vaccine*day_f + us(1 + day_f|fish), REML = TRUE, family = Gamma(link = &quot;log&quot;), data = fish) # variance of random effects VarCorr(m_us) ## ## Conditional model: ## Groups Name Std.Dev. Corr ## fish (Intercept) 0.174360 ## day_f82 0.029283 0.086 ## day_f246 0.149262 -0.461 0.832 ## day_f505 0.228821 -0.236 0.496 0.674 ## day_f901 0.268994 -0.424 0.201 0.492 0.508 # dispersion parameter sigma(m_us) ## [1] 0.06238086 10.3.1.1 Comparing covariance functions Information criteria library(AICcmodavg) model_comparison &lt;- AIC(m_indep, m_cs, m_ar1, m_us) model_comparison$AICc &lt;- c( AICc(m_indep), AICc(m_cs), AICc(m_ar1), AICc(m_us) ) model_comparison$BIC &lt;- BIC(m_indep, m_cs, m_ar1, m_us)$BIC model_comparison ## df AIC AICc BIC ## m_indep 21 24587.91 24588.38 24705.53 ## m_cs 22 23996.17 23996.68 24119.39 ## m_ar1 23 23923.06 23923.62 24051.88 ## m_us 36 23355.32 23356.68 23556.96 Visualizing Use fitted variance covariance matrix from unstructured covariance matrix. varcor_us %&gt;% ggplot(aes(time2-time1, cor))+ geom_line(aes(group = time1, linetype = factor(time1)))+ geom_point(aes(shape = factor(time1)), size =3)+ labs(x = &quot;Distance&quot;, shape = &quot;From Time&quot;, linetype = &quot;From Time&quot;, y = &quot;Covariance of Within Subject Effects&quot;)+ theme_minimal()+ theme(legend.position = &quot;bottom&quot;) Discuss what would be ideal under the different assumptions. Residual diagnostics simulateResiduals(m_us, plot = T) ## Object of Class DHARMa with simulated residuals based on 250 simulations with refit = FALSE . See ?DHARMa::simulateResiduals for help. ## ## Scaled residual values: 0.864 0.796 0.796 0.7 0.612 0.932 0.992 0.964 0.512 0.288 0.764 0.6 0.696 0.172 0.492 0.548 0.56 0.46 0.732 0.88 ... Check out DHARMa documentation. 10.3.1.2 Inference library(emmeans) marginal_means &lt;- emmeans(m_us, ~ vaccine, at = list(day_f = factor(901)), type = &quot;response&quot;) marginal_means ## vaccine response SE df asymp.LCL asymp.UCL ## 1 5715 148 Inf 5432 6012 ## 2 5451 141 Inf 5181 5734 ## 3 5915 153 Inf 5623 6223 ## 4 5365 139 Inf 5100 5644 ## ## Confidence level used: 0.95 ## Intervals are back-transformed from the log scale emmeans(m_us, ~ vaccine, at = list(day_f = factor(901)), type = &quot;response&quot;, contr = list(c(-1, 0, 1, 0), c(0, -1, 1, 0), c(0, 0, 1, -1)))$contr ## NOTE: Results may be misleading due to involvement in interactions ## contrast ratio SE df null z.ratio p.value ## c(-1, 0, 1, 0) 1.04 0.0378 Inf 1 0.943 0.3458 ## c(0, -1, 1, 0) 1.09 0.0397 Inf 1 2.235 0.0254 ## c(0, 0, 1, -1) 1.10 0.0403 Inf 1 2.670 0.0076 ## ## Tests are performed on the log scale 10.3.2 Jaw deformity Considering that the jaw deformity was observed only once in the fish’s cylcle, a fair model for fish jaw deformity could be: \\[y_{ij} \\sim Binomial(\\mu_{ij}, 1),\\\\ \\text{logit}(\\mu_{ij}) = \\eta_{ij} = \\eta_0 + V_i\\] where: \\(y_{ijk}\\) is the presence (1) or absence (0) of jaw deformity on the \\(j\\)th fish under the \\(i\\)th vaccine treatment and arises from a Binomial distribution with mean \\(\\mu_{ijk}\\) and only one trial (i.e., Bernoulli distribution), \\(\\eta_{ij}\\) is the linear predictor, \\(\\eta_0\\) is the overall mean mean of the linear predictor, \\(V_i\\) is the effect of the \\(i\\)th vaccine treatment. # take a subset so that the data are not cloned 5 times fish_sub &lt;- fish %&gt;% filter(day == 901) # fit binomial model m_jaw &lt;- glmmTMB(jaw ~ vaccine, REML = TRUE, family = binomial(link = &quot;logit&quot;), data = fish_sub) # dispersion parameter sigma(m_jaw) ## [1] 1 emmeans(m_jaw, ~ vaccine, type = &quot;response&quot;, contr = list(c(-1, 0, 1, 0), c(0, -1, 1, 0), c(0, 0, 1, -1))) ## $emmeans ## vaccine prob SE df asymp.LCL asymp.UCL ## 1 0.09 0.0286 Inf 0.0475 0.164 ## 2 0.14 0.0347 Inf 0.0847 0.223 ## 3 0.12 0.0325 Inf 0.0694 0.200 ## 4 0.12 0.0325 Inf 0.0694 0.200 ## ## Confidence level used: 0.95 ## Intervals are back-transformed from the logit scale ## ## $contrasts ## contrast odds.ratio SE df null z.ratio p.value ## c(-1, 0, 1, 0) 1.379 0.642 Inf 1 0.690 0.4903 ## c(0, -1, 1, 0) 0.838 0.353 Inf 1 -0.420 0.6744 ## c(0, 0, 1, -1) 1.000 0.435 Inf 1 0.000 1.0000 ## ## Tests are performed on the log odds ratio scale 10.4 Task for today Answer the researcher’s questions and mention one way they could improve the power of their experiment to detect differences in final weight between treatments. Write an email answering the researcher’s questions and email them tonight midnight. Last week’s prompt: You are consultant statisticians. A researcher has come to you to request help analyzing data generated by a designed experiment. They described the experiment in the following email: I hope you are doing well. I am writing to request assistance with my statistical analysis for an aquaculture clinical field trial in the Bay of Fundy, Canada. Please find a description of the study below. The objective of our research was to compare the performance of different vaccines on the growth and survival of Atlantic salmon under standard production conditions. On day 0 of the study, we applied 4 different vaccines to a total of 100 randomly selected fish. Salmons were individually tagged in February 2005 and followed through to harvest in August 2007. All fish were always kept in the same cage. We recorded (1) presence of jaw deformity, and (2) weight throughout the trial. As jaw deformities are formed at an early stage of growth and are not healed, detection of a jaw deformity at any sampling event implied that the fish was labelled as having a jaw deformity throughout the entire growth period. The outcome of interest is the weight, as weights increase from means of 60 g at day 0, to 5,700 g at harvest. However, it is also important to know whether there are differences in propensity to presenting jaw deformity among vaccines. I need help with the following: Describing the experiment design for materials and methods. Analyzing the data and determining: If there are differences between vaccines, especially at harvest, If there is a “better vaccine” for final weight, If there is difference in jaw deformity between vaccine treatments. "],["repeated-measures-ii.html", "Day 11 Repeated Measures II 11.1 Announcements 11.2 Linear mixed models review 11.3 Repeated measures", " Day 11 Repeated Measures II 11.1 Announcements Zoom classes on 10/13 and 10/15. 11.2 Linear mixed models review Consider the vector notation \\[\\mathbf{y} \\sim MVN(\\mathbf{X}\\boldsymbol{\\beta}, \\mathbf{V}),\\] where: \\(\\mathbf{y}\\) is the vector of the response, \\(\\mathbf{X}\\) is the model matrix (often containing treatment allocation), \\(\\boldsymbol{\\beta}\\) is a vector containing the estimates for the effects of all variables in \\(\\mathbf{X}\\), \\(\\mathbf{V}\\) is the variance covariance matrix for \\(\\mathbf{y}\\). The specific form of \\(\\mathbf{V}\\) depends on the design. For mixed models, the linear predictor is \\[\\mathbf{y} = \\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{Z}\\boldsymbol{u} + \\boldsymbol{\\varepsilon}, \\\\ \\boldsymbol{u}\\sim N(\\boldsymbol{0}, \\mathbf{G}) \\\\ \\boldsymbol{\\varepsilon}\\sim N(\\boldsymbol{0}, \\mathbf{R}),\\] where \\(\\mathbf{Z}\\) is the matrix including the information about the random effects and \\(\\boldsymbol{u}\\) is the vector of the random effects. The marginal distribution of \\(\\mathbf{y}\\) for a normal distribution is then \\[\\mathbf{y} \\sim MVN(\\mathbf{X}\\boldsymbol{\\beta}, \\mathbf{ZGZ}&#39;+\\mathbf{R}).\\] Take a look at the variance-covariance matrix below. In this case, all residuals are independent (see yellow diagonal), and all information about groups sharing information is in the G matrix. Figure 11.1: Illustrative example of the variance-covariance matrix. Note that, for all mixed models we have been handling so far, \\[\\mathbf{G} = \\sigma^2_u\\mathbf{I} = \\begin{bmatrix} \\sigma^2_u &amp; 0 &amp; 0 &amp; &amp; 0\\\\ 0 &amp; \\sigma^2_u &amp; 0 &amp; &amp; 0\\\\ 0 &amp; 0 &amp; \\sigma^2_u &amp; &amp; 0\\\\ &amp; &amp; &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\dots &amp; \\sigma^2_u\\\\ \\end{bmatrix},\\] which, will result in the matrix above. The conditional distribution of \\(\\mathbf{y}\\) is \\[\\mathbf{y} \\vert \\boldsymbol{u} \\sim MVN(\\mathbf{X}\\boldsymbol{\\beta}+\\mathbf{Z}\\boldsymbol{u}, \\mathbf{R}).\\] This conditional distribution comes most handy for non-normal data, \\[\\mathbf{y} \\vert \\boldsymbol{u} \\sim P(\\mathbf{X}\\boldsymbol{\\beta}+\\mathbf{Z}\\boldsymbol{u}, \\mathbf{R}).\\] 11.3 Repeated measures Figure 11.2: Schematic description of a field experiment with repeated measures 11.3.1 Correlation - G side (conditional) and R side (marginal) Inference implications Power implications Applied example in R 11.3.2 Review – correlation functions Let \\(\\mathbf{v}_{i k}\\) be the vector for the \\(k\\)th individual under the \\(i\\)th vaccine treatment, at the different timepoints of the experiment. \\[\\mathbf{v}_{i k} \\sim MVN(\\boldsymbol{0}, \\boldsymbol{\\Sigma}_{ik})\\] Independent observations \\[\\boldsymbol{\\Sigma}_{ik} = \\sigma^2 \\begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1\\\\ \\end{bmatrix}\\] Compound symmetry \\[\\boldsymbol{\\Sigma}_{ik} = \\sigma^2 \\begin{bmatrix} 1 &amp; \\rho &amp; \\rho &amp; \\rho &amp; \\rho \\\\ \\rho &amp; 1 &amp; \\rho &amp; \\rho &amp; \\rho \\\\ \\rho &amp; \\rho &amp; 1 &amp; \\rho &amp; \\rho \\\\ \\rho &amp; \\rho &amp; \\rho &amp; 1 &amp; \\rho \\\\ \\rho &amp; \\rho &amp; \\rho &amp; \\rho &amp; 1\\\\ \\end{bmatrix}\\] This could mean \\[\\mathbf{G}_{ik} = \\sigma_v^2 \\begin{bmatrix} 1 &amp; \\rho &amp; \\rho &amp; \\rho &amp; \\rho \\\\ \\rho &amp; 1 &amp; \\rho &amp; \\rho &amp; \\rho \\\\ \\rho &amp; \\rho &amp; 1 &amp; \\rho &amp; \\rho \\\\ \\rho &amp; \\rho &amp; \\rho &amp; 1 &amp; \\rho \\\\ \\rho &amp; \\rho &amp; \\rho &amp; \\rho &amp; 1\\\\ \\end{bmatrix}\\] or \\[\\mathbf{R}_{ik} = \\sigma_\\varepsilon^2 \\begin{bmatrix} 1 &amp; \\rho &amp; \\rho &amp; \\rho &amp; \\rho \\\\ \\rho &amp; 1 &amp; \\rho &amp; \\rho &amp; \\rho \\\\ \\rho &amp; \\rho &amp; 1 &amp; \\rho &amp; \\rho \\\\ \\rho &amp; \\rho &amp; \\rho &amp; 1 &amp; \\rho \\\\ \\rho &amp; \\rho &amp; \\rho &amp; \\rho &amp; 1\\\\ \\end{bmatrix}\\] AR(1) \\[\\boldsymbol{\\Sigma}_{ik} = \\sigma^2 \\begin{bmatrix} 1 &amp; \\rho &amp; \\rho^2 &amp; \\rho^3 &amp; \\rho^4 \\\\ \\rho &amp; 1 &amp; \\rho &amp; \\rho^2 &amp; \\rho^3 \\\\ \\rho^2 &amp; \\rho &amp; 1 &amp; \\rho &amp; \\rho^2 \\\\ \\rho^3 &amp; \\rho^2 &amp; \\rho &amp; 1 &amp; \\rho \\\\ \\rho^4 &amp; \\rho^3 &amp; \\rho^2 &amp; \\rho &amp; 1\\\\ \\end{bmatrix}\\] Unstructured \\[\\boldsymbol{\\Sigma}_{ik} = \\begin{bmatrix} \\sigma^2_{1} &amp; \\sigma^2_{12} &amp; \\sigma^2_{13} &amp; \\sigma^2_{14} &amp; \\sigma^2_{15} \\\\ &amp; \\sigma^2_{2} &amp; \\sigma^2_{23} &amp; \\sigma^2_{24} &amp; \\sigma^2_{25} \\\\ &amp; &amp; \\sigma^2_{3} &amp; \\sigma^2_{34} &amp; \\sigma^2_{35} \\\\ &amp; &amp; &amp; \\sigma^2_{4} &amp; \\sigma^2_{45} \\\\ &amp; &amp; &amp; &amp; \\sigma^2_{5} \\\\ \\end{bmatrix}\\] "],["repeated-measures-iii.html", "Day 12 Repeated Measures III 12.1 Announcements 12.2 Where we’re standing in this course 12.3 Repeated measures 12.4 Correlation - G side (conditional) and R side (marginal) 12.5 Repeated measures in GLMMs 12.6 Appendix A: Common Response Variable (\\(y | b\\)) Distributions 12.7 Applied example using R", " Day 12 Repeated Measures III 12.1 Announcements Zoom classes on 10/13 and 10/15. Assignment 4 due Wednesday. 12.2 Where we’re standing in this course Considering the linear mixed model \\[\\mathbf{y} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\mathbf{V}),\\] where: \\(\\mathbf{y}\\) is the vector of the response, \\(\\mathbf{X}\\) is the model matrix (often containing treatment allocation), \\(\\boldsymbol{\\beta}\\) is a vector containing the estimates for the effects of all variables in \\(\\mathbf{X}\\), \\(\\mathbf{V}\\) is the variance covariance matrix for \\(\\mathbf{y}\\). The marginal distribution of \\(\\mathbf{y}\\) for a normal distribution can also be written as \\[\\mathbf{y} \\sim MVN(\\mathbf{X}\\boldsymbol{\\beta}, \\mathbf{ZGZ}&#39;+\\mathbf{R}).\\] The variance-covariance matrices below represent the variance/covariance of all \\(y\\)s. Figure 12.1: Illustrative example of the variance-covariance matrix. We consider messy data the different dependence patterns in \\(\\mathbf{G}\\) and/or \\(\\mathbf{R}\\). Note that, for all mixed models we have been handling so far, \\[\\mathbf{G} = \\sigma^2_u\\mathbf{I} = \\begin{bmatrix} \\sigma^2_u &amp; 0 &amp; 0 &amp; &amp; 0\\\\ 0 &amp; \\sigma^2_u &amp; 0 &amp; &amp; 0\\\\ 0 &amp; 0 &amp; \\sigma^2_u &amp; &amp; 0\\\\ &amp; &amp; &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; 0 &amp; \\dots &amp; \\sigma^2_u\\\\ \\end{bmatrix},\\] which, will result in some variation of the matrix above. 12.3 Repeated measures Figure 12.2: Schematic description of a field experiment with repeated measures 12.4 Correlation - G side (conditional) and R side (marginal) Typically, REPEATED (in SAS’s MIXED procedure) is equivalent to R-side (marginal) covariance in GLIMMIX. Look at marginal variance-covariance matrix on the board. What happens if we model G-side covariance? Implications on inference Similar CI for marginal means (lsmeans) Confidence intervals estimated with different DF approximations May see some discrepancy in test statistics. 12.5 Repeated measures in GLMMs Recall distributions of non-Gaussian GLMMs: Figure 12.3: Common variable distributions. Page 60 in Stroup et al. (2024) 12.6 Appendix A: Common Response Variable (\\(y | b\\)) Distributions Below is a table of commonly used distributions, their properties, and related probability density functions (PDFs). GLMMs .table-wrapper { width: 100%; overflow-x: auto; border: 1px solid #ccc; } table { width: 100%; border-collapse: collapse; margin: 20px 0; } th, td { border: 1px solid #ddd; padding: 8px; text-align: left; } th { background-color: #f4f4f4; font-weight: bold; } tr:nth-child(even) { background-color: #f9f9f9; } Distribution Range of Variable Mean Commonly Used Link(s) Variance PDF Gaussian/normal \\(-\\infty &lt; y &lt; \\infty\\) \\(\\mu\\) \\(\\eta = \\mu\\) \\(\\sigma^2\\) \\[f(y) = \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-\\frac{(y-\\mu)^2}{2\\sigma^2}}\\] Log normal \\(-\\infty &lt; y &lt; \\infty\\) \\(\\mu\\) \\(\\eta = \\log(\\mu)\\) \\(\\sigma^2\\) \\[f(y) = \\frac{1}{y\\sigma\\sqrt{2\\pi}} e^{-\\frac{(\\log y - \\mu)^2}{2\\sigma^2}}\\] Gamma \\(y &gt; 0\\) \\(\\mu\\) \\(\\eta = \\log(\\mu), \\frac{1}{\\mu}\\) \\(\\phi \\mu^2\\) \\[f(y) = \\frac{y^{\\frac{\\mu}{\\phi} - 1} e^{-\\frac{y}{\\phi}}}{\\phi^{\\mu} \\Gamma(\\mu)}\\] Exponential \\(y &gt; 0\\) \\(\\mu\\) \\(\\eta = \\log(\\mu), \\frac{1}{\\mu}\\) \\(\\mu^2\\) \\[f(y) = \\frac{1}{\\mu} e^{-\\frac{y}{\\mu}}\\] Binomial 0, \\(\\frac{1}{N}, \\dots, \\frac{N}{N}\\) \\(\\pi = \\frac{\\mu}{N}\\) Logit: \\(\\eta = \\log\\left(\\frac{\\pi}{1-\\pi}\\right)\\) \\(N\\pi(1-\\pi)\\) \\[f(y) = \\binom{N}{y} \\pi^y (1-\\pi)^{N-y}\\] Beta 0 &lt; \\(y &lt; 1\\) \\(\\mu\\) \\(\\eta = \\log\\left(\\frac{\\mu}{1-\\mu}\\right)\\) \\(\\frac{\\mu(1-\\mu)}{1+\\phi}\\) \\[f(y) = \\frac{y^{\\alpha-1}(1-y)^{\\beta-1}}{B(\\alpha, \\beta)}\\] Poisson \\(y = 0, 1, 2, \\dots\\) \\(\\lambda\\) \\(\\eta = \\log(\\lambda)\\) \\(\\lambda\\) \\[f(y) = \\frac{\\lambda^y e^{-\\lambda}}{y!}\\] Geometric \\(y = 0, 1, 2, \\dots\\) \\(\\lambda\\) \\(\\eta = \\log(\\lambda)\\) \\(\\lambda(1+\\lambda)\\) \\[f(y) = (1-\\lambda)^y \\lambda\\] Negative binomial \\(y = 0, 1, 2, \\dots\\) \\(\\lambda\\) \\(\\eta = \\log(\\lambda)\\) \\(\\lambda(1+\\phi\\lambda)\\) \\[f(y) = \\binom{y + r - 1}{y} \\lambda^r (1-\\lambda)^y\\] If we describe a non-Gaussian GLMM with \\[{y}|\\boldsymbol{u} \\sim P({\\mu}, \\phi),\\] where \\(y\\) is the response, \\(\\boldsymbol{u}\\) are the random effects, \\(\\mu\\) is the mean, and \\(\\phi\\) is the dispersion, we know that \\(\\mu\\) and \\(\\phi\\) may not be independent. G-side ~ ‘true GLMM’ R-side: quasi-likelihood models Marginal models often lead to less powerful tests. Several discussions: Philosophical, Power, and more Lee and Nelder (2004) Many unknowns for the software implementation of non-Gaussian GLMMs Stroup and Claassen (2020) 12.7 Applied example using R Get R code "],["modeling-designed-experiments-in-the-presence-of-spatial-variability.html", "Day 13 Modeling designed experiments in the presence of spatial variability 13.1 Announcements 13.2 Spatial variability", " Day 13 Modeling designed experiments in the presence of spatial variability October 8th 13.1 Announcements Zoom classes on 10/13 and 10/15. Assignment 4 due today midnight. 13.2 Spatial variability Most commonly found in field experiments. Whiteboard: going from the normal iid model to a spatial dependence model. 13.2.1 Spatial patterns Consider the model \\[\\mathbf{y} \\sim N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}),\\] where \\(\\boldsymbol{\\Sigma}\\) is the variance-covariance model. In the presence of spatial patterns, there are parametric and non-parametric tools to account for spatial dependence. Sometimes, we can clearly define the shape of the spatial dependence pattern. For example: Matérn covariance function \\[C(d; \\sigma^2, \\phi, \\kappa) = \\sigma^2 \\frac{(h/\\phi)^\\kappa K_\\kappa (h/\\phi)}{2^{\\kappa-1} \\Gamma(\\kappa)}\\] Gaussian covariance function \\[C(d; \\sigma^2, \\phi) = \\sigma^2 \\exp \\left\\{ - \\frac{d^2}{2\\phi^2} \\right\\} \\] Exponential covariance function \\[C(d; \\sigma^2, \\phi, \\kappa) = \\sigma^2 \\exp\\{-(d/\\phi)^\\kappa\\}\\] 13.2.2 Applied example Yield data from an advanced Nebraska Intrastate Nursery (NIN) breeding trial conducted at Alliance, Nebraska, in 1988/89. library(tidyverse) library(agridat) library(ggpubr) data &lt;- drop_na(stroup.nin) data %&gt;% ggplot(aes(col, row))+ geom_tile(aes(fill = rep), color = &quot;black&quot;)+ theme_pubr()+ labs(fill = &quot;Block&quot;)+ theme(axis.line = element_blank()) The statistical model: \\[y_{ij}\\sim N(\\mu_{ij}, \\sigma^2), \\\\ \\mu_{ij} = \\eta_{ij} = \\eta_0 + G_i + b_j, \\\\ b_j \\sim N(0, \\sigma^2_b).\\] Assumptions: iid, normal residuals library(lme4) m &lt;- lmer(yield ~ gen + (1|rep), data = stroup.nin) data$res &lt;- resid(m) data %&gt;% ggplot(aes(col, row))+ geom_tile(aes(fill = res), color = &quot;black&quot;)+ theme_pubr()+ scico::scale_fill_scico()+ theme(axis.line = element_blank())+ labs(fill = expression(Residual~(bu~ac^{-1}))) as.data.frame(emmeans(m, ~ gen)) %&gt;% arrange(-emmean) %&gt;% head() ## gen emmean SE df lower.CL upper.CL ## NE86503 32.6500 3.855689 66.69 24.95335 40.34665 ## NE87619 31.2625 3.855689 66.69 23.56585 38.95915 ## NE86501 30.9375 3.855689 66.69 23.24085 38.63415 ## Redland 30.5000 3.855689 66.69 22.80335 38.19665 ## Centurk78 30.3000 3.855689 66.69 22.60335 37.99665 ## NE83498 30.1250 3.855689 66.69 22.42835 37.82165 ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 library(glmmTMB) data$coordinate &lt;- numFactor(data$col, data$row) data$group &lt;- factor(1) m_spatial &lt;- glmmTMB(yield ~ gen + (1|rep) + mat(coordinate + 0 | group), data = data) data$resid_spatial &lt;- resid(m_spatial) data %&gt;% ggplot(aes(col, row))+ geom_tile(aes(fill = resid_spatial), color = &quot;black&quot;)+ theme_pubr()+ scico::scale_fill_scico()+ theme(axis.line = element_blank())+ labs(fill = expression(Residual~(bu~ac^{-1}))) as.data.frame(emmeans(m_spatial, ~ gen)) %&gt;% arrange(-emmean) %&gt;% head() ## gen emmean SE df lower.CL upper.CL ## Buckskin 33.37166 3.399254 163 26.65941 40.08391 ## NE85556 29.35020 3.395673 163 22.64502 36.05538 ## NE87619 29.09390 3.397032 163 22.38604 35.80176 ## NE83498 28.90073 3.405042 163 22.17705 35.62441 ## Redland 28.63517 3.393941 163 21.93341 35.33693 ## NE86503 27.51328 3.451773 163 20.69732 34.32923 ## ## Confidence level used: 0.95 "],["software-implementation-of-lmms.html", "Day 14 Software Implementation of LMMs 14.1 Announcements 14.2 Model review 14.3 Computational tricks 14.4 References", " Day 14 Software Implementation of LMMs 14.1 Announcements Assignment 4 grades will be posted tonight Projects 14.2 Model review In general, we can write LLMs as \\[\\mathbf{y} \\vert \\boldsymbol{b} \\sim N(\\mathbf{X}\\boldsymbol{\\beta} + \\mathbf{Z}\\boldsymbol{b}, \\sigma^2 \\mathbf{W}^{-1})\\] \\(\\boldsymbol{b} \\sim N(\\boldsymbol{0}, \\boldsymbol{\\Sigma})\\), where\\(\\boldsymbol{\\Sigma}\\) must be positive semidefinite and can be expressed also as \\(\\boldsymbol{\\Sigma}_{\\theta} = \\sigma^2\\boldsymbol{\\Lambda}_\\theta \\boldsymbol{\\Lambda}_\\theta^T\\), where \\(\\sigma\\) is the scale factor (same as above) and \\(\\boldsymbol{\\Lambda}_\\theta\\) is a relative covariance factor that depends on the variance component \\(\\theta\\). Check out Bates et al. (2015) 14.3 Computational tricks 14.3.1 Improving computational stability For computational stability and efficiency, reformulate the model such that \\(\\theta\\) appears only in the conditional distribution for the response. For ML, minimize -2 log-likelihood, \\(-2 \\mathcal{L}(\\boldsymbol{\\theta}, \\boldsymbol{\\beta}, \\sigma^2\\vert\\mathbf{y})=\\log \\frac{\\vert\\boldsymbol{L}_\\theta\\vert^2}{\\vert\\boldsymbol{W}\\vert} + n \\log(2\\pi\\sigma^2)+ \\frac{r^2(\\boldsymbol{\\theta})}{\\sigma^2} + \\frac{\\vert\\vert \\mathbf{R}_X(\\boldsymbol{\\beta}-\\hat{\\boldsymbol{\\beta}}_\\theta) \\vert\\vert}{\\sigma^2}\\) 14.3.2 Nonlinear optimizing algorithms Bound Optimization by Quadratic Approximation (bobyqa) Used in lme4 (default) Nelder Mead Alternative in lme4 Visualization Template Model Builder (TMB) nlminb() or optim() under the hood GLMMs Download R script for implementing LMMs 14.4 References Fitting Linear Mixed-Effects Models Using lme4 "],["software-implementation-of-lmms-1.html", "Day 15 Software Implementation of LMMs 15.1 Announcements 15.2 Issues 15.3 References", " Day 15 Software Implementation of LMMs 15.1 Announcements Assignment 4 grades will be posted tonight Projects Download R script for implementing LMMs (from last Monday) 15.2 Issues Optimizing algorithms Convergence problems Singularity and the meaning of zero/negative variance components 15.3 References lme4 troubleshooting I lme4 troubleshooting II Anything by Ben Bolker GLMM FAQ Fitting Linear Mixed-Effects Models Using lme4 "],["accounting-for-spatial-effects.html", "Day 16 Accounting for spatial effects 16.1 Announcements 16.2 Parametric tools 16.3 Non-parametric tools 16.4 B-splines 16.5 Final comments 16.6 Resources", " Day 16 Accounting for spatial effects 16.1 Announcements Comments on Assignment 4 Assignment 5 is due this Friday 16.2 Parametric tools Correlation functions 16.3 Non-parametric tools Minimize/relax assumptions No free lunches! Interpretability bias-variance tradeoff 16.3.1 Splines Splines are special cases of non-parametric tools. Introduced in the sixties (Schoenberg, 1964) They provide a flexible tool to model the variability in the data, where the functional (~ “the shape”) is unknown Polynomials: good for local approximation bad for global approximation 16.4 B-splines We can generally describe smoothing splines as the sum of many basis functions: \\[f(x) = \\sum_{i=1}^k B_i^m(x) \\beta_i \\] Expanded equation: \\[f(x) = \\beta_0 + \\beta_1 x +\\beta_2 x^2 +\\dots + \\beta_p x^p + \\left\\{ b_1(x - \\kappa_1)^p_{+} + b_2(x - \\kappa_2)^p_{+} + \\dots + b_K(x - \\kappa_K)^p_{+} \\right\\},\\] where: \\(\\kappa_1, \\kappa_2, \\dots, \\kappa_K\\) are the knots, \\(b_1, b_2, \\dots, b_K\\) are the spline coefficients. Using matrix notation: \\[f(X_i) = \\beta_0 + X_i \\beta_1 + X^2_i \\beta_2 + \\dots + X_i^p \\beta_p \\\\+ b_1(X_i - \\kappa_1)^p_+ + b_2(X_i - \\kappa_2)^p_+ + \\dots + b_K(X_i - \\kappa_K)^p_+ \\\\ = \\mathbf{X}_i^T \\boldsymbol{\\beta}_X + \\mathbf{B}^T(X_i)\\mathbf{b},\\] minimize: \\[\\sum_{i=1}^n \\left\\{ Y_i - (\\mathbf{X}_i^T \\boldsymbol{\\beta}_X + \\mathbf{B}^T(X_i)\\mathbf{b}) \\right\\}^2 + \\lambda \\mathbf{b}^T\\mathbf{Db},\\] where \\(\\lambda \\mathbf{b}^T\\mathbf{Db}\\) prevents overfitting: \\(\\mathbf{D}\\) positive semidefinite. \\(\\lambda\\) controls penalization and is very important. Then, for a straight line: \\[f(x) = \\beta_0 + \\beta_1 x + \\left\\{ b_1(x - \\kappa_1)_{+} + b_2(x - \\kappa_2)_{+} + \\dots + b_K(x - \\kappa_K)_{+} \\right\\},\\] where: \\(\\kappa_1, \\kappa_2, \\dots, \\kappa_K\\) are the knots, \\(b_1, b_2, \\dots, b_K\\) are the spline coefficients. 16.4.1 Illustrating splines Let’s start with a simple piecewise linear regression. Take the following data example: set.seed(2) df &lt;- data.frame(x = seq(1, 50, by = .3)) %&gt;% mutate(y = 10 + 4*cos(.4*x) + rnorm(n(), 0, 2)) df %&gt;% ggplot(aes(x, y))+ geom_point(alpha= .2)+ theme_pubclean()+ labs(x = &quot;Some predictor&quot;, y = &quot;Some response&quot;) Let’s start with \\[f(x) = \\sum_{i=1}^k B_i^m(x) \\beta_i\\] with \\(m=1\\) and \\(k=7\\). This means that we have a bunch of connected straight lines. More specifically, we expect to have 7 basis functions and 7 knots. library(mgcv) bspline &lt;- gam(y ~ s(x, bs = &quot;bs&quot;, m = 1, k = 7), data = df) df$Bspline &lt;- bspline$fitted.values knots &lt;- data.frame(knots = bspline$smooth[[1]]$knots[-c(1, 9)]) df %&gt;% ggplot(aes(x, y))+ geom_point(alpha= .2)+ geom_vline(aes(xintercept = knots), data = knots, linetype =2)+ geom_line(aes(y = Bspline), size = 1)+ theme_pubclean() Now, the straight lines might not be the best way to represent the relationship between x and y, not even within each region in x. Now, with \\(m=2\\) we will have piecewise polynomic regression. bspline &lt;- gam(y ~ s(x, bs = &quot;bs&quot;, m = 2, k=7), data = df) df$Bspline_m3 &lt;- bspline$fitted.values knots &lt;- data.frame(knots = bspline$smooth[[1]]$knots[-c(1,2,9,10)]) df %&gt;% ggplot(aes(x, y))+ geom_point(alpha= .2)+ geom_vline(aes(xintercept = knots), data = knots, linetype =2)+ geom_line(aes(y = Bspline), size = 1, color = &quot;grey45&quot;)+ geom_line(aes(y = Bspline_m3), size = 1)+ theme_pubclean() Many questions may arise from looking at this: What is the best \\(k\\)? Does a piecewise polynomial really lead to the best results? 16.4.2 Penalized splines low rank smoothers using a B-spline basis \\(\\kappa_1, \\kappa_2, \\dots, \\kappa_K\\) are the knots, \\(b_1, b_2, \\dots, b_K\\) are the spline coefficients. bspline &lt;- gam(y ~ s(x, bs = &quot;bs&quot;, m = 2), data = df) pspline &lt;- gam(y ~ s(x, bs = &quot;ps&quot;, m = 2), data = df) df$Bspline &lt;- bspline$fitted.values df$Pspline &lt;- pspline$fitted.values df &lt;- df %&gt;% pivot_longer(c(Bspline,Pspline), names_to = &quot;spline&quot;, values_to = &quot;fitted&quot;) knots &lt;- data.frame(knots = bspline$smooth[[1]]$knots[-c(1, 2,12, 13)]) df %&gt;% ggplot(aes(x, y))+ geom_point(alpha= .2)+ geom_vline(aes(xintercept = knots), data = knots, linetype =2)+ geom_line(aes(y = fitted, group = spline, color = spline), size = 1.5)+ theme_pubclean()+ labs(color = &quot;Smoothing basis&quot;) 16.4.3 Other types of splines Cyclic splines Thin-plate regression splines supports multiple predictor variables (unlike other basis) avoid the problem of knot placement See Chapter 5 in Wood (2017) 16.5 Final comments Generalized additive models Choosing parametric vs. semi-parametric Choosing types of splines Model selection G-side vs. R-side discussion How to apply this in the spatial model from before: 16.6 Resources Wood, S.N. (2017). Generalized Additive Models. Chapman and Hall/CRC. [link] Ruppert, D. (2004). Nonparametric Regression and Splines. In: Statistics and Finance. Springer Texts in Statistics. Springer, New York, NY. https://doi.org/10.1007/978-1-4419-6876-0_13 "],["smoothing-splines.html", "Day 17 Smoothing Splines 17.1 Announcements 17.2 Non-parametric tools 17.3 Thin-plate regression splines 17.4 Final comments 17.5 Resources", " Day 17 Smoothing Splines 17.1 Announcements Comments on Assignment 4 Assignment 5 is due this Friday 17.2 Non-parametric tools Minimize/relax assumptions No free lunches! Interpretability bias-variance tradeoff 17.2.1 Splines Splines are special cases of non-parametric tools. Introduced in the sixties (Schoenberg, 1964) They provide a flexible tool to model the variability in the data, where the functional (~ “the shape”) is unknown Polynomials: good for local approximation bad for global approximation We can represent the data with the equation \\[y_i = \\beta_0 + g(x_i) + \\varepsilon_i, \\\\ \\varepsilon_i \\sim N(0, \\sigma^2),\\] where \\(g(x_i) = \\sum_{i=1}^k B_i^m(x) \\beta_i\\). B-splines minimize \\(\\sum_{i=1}^n \\left\\{ y_i - (\\mathbf{x}_i&#39;\\boldsymbol{\\beta}_x + \\mathbf{B}^T(\\mathbf{x}_i) \\boldsymbol{b}) \\right\\}^2\\) Penalized splines low rank smoothers using a B-spline basis minimize \\(\\sum_{i=1}^n \\left\\{ y_i - (\\mathbf{x}_i&#39;\\boldsymbol{\\beta}_x + \\mathbf{B}&#39;(\\mathbf{x}_i) \\boldsymbol{b}) \\right\\}^2 + \\lambda \\boldsymbol{b}&#39;\\mathbf{D}\\boldsymbol{b}\\) Cyclic splines 17.3 Thin-plate regression splines Origin of the name “thin-plate” radial basis functions supports multiple predictor variables (unlike other basis) avoid the problem of knot placement not so computationally costly, but may become more relevant for large data (scaling \\(O(k^3)\\)) basis functions and basis funciton dimension See Chapter 5 in Wood (2017) In thin-plate regression, we can represent the data with the equation \\[y_i = \\beta_0 + g(\\mathbf{x}_i) + \\varepsilon_i, \\\\ \\varepsilon_i \\sim N(0, \\sigma^2),\\] where \\(\\mathbf{x}_i\\) is a \\(d-\\)vector with the predictors. Thin-plate smoothers estimate \\(g(\\cdot)\\) by finding the \\(\\hat{f}(\\cdot)\\) that minimizes \\[||\\mathbf{y} - \\mathbf{f}||^2 + \\lambda J_{md}(f),\\] where: \\(\\mathbf{y}\\) is the vector of observations, \\(\\mathbf{f} = [f(\\mathbf{x}_1), f(\\mathbf{x}_2), \\dots, f(\\mathbf{x}_n)]^T\\), \\(J_{md}(f)\\) is penalty functional affecting the ‘wiggliness’ of \\(f\\) \\(\\lambda\\) is a smoothing parameter controlling the tradeoff between data fitting and smoothness of \\(f\\) Figure 17.1: From Wood (2007) x &lt;- seq(1, 50, by = 1) set.seed(42) y &lt;- 15 + 15*sin(sqrt(x*.15 - x*.006))+ rnorm(length(x), 0, 1.5) plot(x, y) splines_df &lt;- data.frame(x, y) m_spline_tp &lt;- gam(y ~ s(x, bs = &quot;tp&quot;, k = 7), method = &quot;REML&quot;, data = splines_df) splines_df$bs_spline &lt;- predict(m_spline_tp, type = &quot;response&quot;) splines_df$bs_spline_se &lt;- predict(m_spline_tp, type = &quot;response&quot;, se.fit = T)$se.fit knots &lt;- m_spline_tp$smooth[[1]]$knots coef(m_spline_tp)[grep(&quot;s\\\\(x\\\\)&quot;, names(coef(m_spline_tp)))] ## s(x).1 s(x).2 s(x).3 s(x).4 s(x).5 s(x).6 ## -7.076170 9.538457 -4.898365 7.277189 18.206627 6.552186 beta_smooth &lt;- coef(m_spline_tp)[grep(&quot;s\\\\(x\\\\)&quot;, names(coef(m_spline_tp)))] Xp &lt;- predict(m_spline_tp, type = &quot;lpmatrix&quot;) # each column = basis function # Remove intercept column for plotting Xp_smooth &lt;- Xp[, grep(&quot;s\\\\(x\\\\)&quot;, colnames(Xp))] # Plot basis functions matplot(x, Xp_smooth, type = &quot;l&quot;, lty = 1, col = rainbow(ncol(Xp_smooth)), main = &quot;Spline Basis Functions&quot;, xlab = &quot;x&quot;, ylab = &quot;Basis value&quot;) # Add contribution from each basis function # multiply basis functions by their contribution y_basis &lt;- sweep(Xp_smooth, 2, beta_smooth, &quot;*&quot;) matplot(x, y_basis, type = &quot;l&quot;, lty = 1, col = rainbow(ncol(Xp_smooth)), main = &quot;Contribution of Each Basis Function&quot;, xlab = &quot;x&quot;, ylab = &quot;Contribution&quot;) df_basis &lt;- as.data.frame(cbind(x, Xp_smooth)) %&gt;% pivot_longer(cols = `s(x).1`:`s(x).6`) splines_df %&gt;% ggplot(aes(x, y))+ geom_line(aes(y = 18 + value*10, group = name, color = name), show.legend = F, data = df_basis, alpha = .6)+ theme_classic()+ theme(panel.border = element_blank(), panel.grid = element_blank())+ geom_vline(aes(xintercept = knots), data = data.frame(knots), linetype = 2)+ geom_line(aes(y = bs_spline+(bs_spline_se*1.96)), linetype = 2)+ geom_line(aes(y = bs_spline-(bs_spline_se*1.96)), linetype = 2)+ geom_line(aes(y = bs_spline))+ coord_cartesian(xlim = c(0, 50))+ labs(x = &quot;Columns (x1)&quot;, y = &quot;y or f(x1)&quot;)+ geom_point() matplot(x, rowSums(y_basis), type = &quot;l&quot;, lty = 1, col = rainbow(ncol(Xp_smooth)), main = &quot;Sum of the contributions of all basis functions&quot;, xlab = &quot;x&quot;, ylab = &quot;f(x)&quot;) 17.4 Final comments Generalized additive models Choosing parametric vs. semi-parametric Choosing types of splines B-splines P-splines Thin-plate splines Model selection G-side vs. R-side discussion 17.5 Resources Wood, S.N. (2017). Generalized Additive Models. Chapman and Hall/CRC. [link] Ruppert, D. (2004). Nonparametric Regression and Splines. In: Statistics and Finance. Springer Texts in Statistics. Springer, New York, NY. https://doi.org/10.1007/978-1-4419-6876-0_13 "],["applications-of-smoothing-splines.html", "Day 18 Applications of Smoothing Splines 18.1 Announcements 18.2 Splines review 18.3 Applications of splines", " Day 18 Applications of Smoothing Splines 18.1 Announcements Assignment 5 is due tonight 18.2 Splines review Piecewise polynomial functions pros of local approximation of polynomials without cons of global approximation of polynomials maximal smoothness PROS: Flexibility No need to specify functional CONS: Less interpretable Computational cost Flexibility must be selected by adjusting the smoothing parameter \\(\\lambda\\) We can represent the data with the equation \\[y_i = \\beta_0 + f(x_i) + \\varepsilon_i, \\\\ \\varepsilon_i \\sim N(0, \\sigma^2),\\] where \\(f(\\cdot)\\) is estimated with the function \\(g(\\cdot)\\) that minimizes \\(||\\mathbf{y}-\\mathbf{g}||^2 + \\lambda J_{md}(g)\\). Reember that \\(\\mathbf{g} = [g(\\mathbf{x}_1), g(\\mathbf{x}_2), \\dots, g(\\mathbf{x}_n)]&#39;\\) \\(J_{md}(g)\\) is a penalty funcitonal that regulates the wiggliness of \\(g(\\cdot)\\) and \\(\\lambda\\) controls the smoothness of \\(g(\\cdot)\\). Thin Plate Regression Splines (Wood, 2003) A Conversation with Grace Wahba Importance of basis dimension Impact on flexibility of the model Checking if basis dimension is too small (residuals vs. neighbours) Comparing effective degrees of freedom with basis dimension Implemented in gam.check k' gives the maximum possible EDF for that smooth compare k' with EDF in a model 18.3 Applications of splines library(tidyverse) # data wrangling library(ggpubr) # data viz library(lme4) # fit LMMs (only supports simple random effects - sompound symmetry) library(mgcv) # fit GAMs (supports both random effects and splines) 18.3.0.1 Spatial effects Traditional approach The data below come from a multi-environment experiment that studies yield of lupin for different genotypes. dat &lt;- drop_na(agridat::verbyla.lupin) dat %&gt;% ggplot(aes(col, row))+ geom_tile(aes(fill = yield))+ facet_grid(year~loc)+ coord_fixed()+ scico::scale_fill_scico()+ theme_pubclean() A (typically) reasonable model could be \\[y_{ijk} \\sim N(\\mu_{ijk}, \\sigma^2),\\\\ \\mu_{ijk} = \\mu_0 + T_i + l_j + b_{k(j)}, \\\\ l_j \\sim N(0, \\sigma^2_l), \\\\ b_{k(j)} \\sim N(0, \\sigma^2_b),\\] where: \\(y_{ijk}\\) is the observed yield and arises from a normal distribution with mean \\(\\mu_{ijk}\\) and variance \\(\\sigma^2\\), \\(\\mu_0\\) is the overall mean yield for all fields, \\(l_j\\) is the (random) effect of the \\(j\\)th field that arises from a normal distribution with variance \\(\\sigma^2_l\\) (note that \\(\\mu_0 + l_j\\) is the overall yield for the \\(j\\)th field), \\(b_{k(j)}\\) is the (random) effect of the \\(k\\)th block in the \\(j\\)th field that arises from a normal distribution with variance \\(\\sigma^2_b\\), \\(T_i\\) is the treatment effect of the \\(i\\)th genotype. tictoc::tic() m1 &lt;- lmer(yield ~ gen + (1|site/rep), data = dat) tictoc::toc() ## 0.05 sec elapsed VarCorr(m1) ## Groups Name Std.Dev. ## rep:site (Intercept) 0.10140 ## site (Intercept) 0.27897 ## Residual 0.30284 dat$resid_1 &lt;- resid(m1) dat %&gt;% ggplot(aes(col, row))+ geom_tile(aes(fill = resid_1))+ facet_grid(~site)+ coord_fixed()+ scico::scale_fill_scico()+ labs(main = &quot;Residuals assuming within-block homogeneity&quot;)+ theme_pubclean() Relaxing an assumption from the traditional approach Alternatively, we could add an element to the linear predictor. Previously, we said \\(\\mu_{ijk} = \\mu_0 + T_i + l_j + b_{k(j)}\\). Potential risks of assuming within-block homogeneity: loss of power Alternative: expand the linear predictor into \\(\\mu_{ijk} = \\mu_0 + T_i + l_j + b_{k(j)} + f(\\mathbf{s}_{ijk})\\), where \\(\\mathbf{s}_{ijk}\\) is a vector of the coordinates of the observation \\(ijk\\), and \\(f(\\cdot)\\) is a smooth function of space. We consider \\(f(\\cdot)\\) can be approximated using smoothing splines. tictoc::tic() m2 &lt;- gam(yield ~ gen + s(rep, site, bs = &quot;re&quot;) + s(row, col, bs = &quot;tp&quot;, by = site), data = dat) tictoc::toc() ## 10.64 sec elapsed summary(m2) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## yield ~ gen + s(rep, site, bs = &quot;re&quot;) + s(row, col, bs = &quot;tp&quot;, ## by = site) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.89485 0.04871 18.372 &lt; 2e-16 *** ## genGungurru -0.02161 0.03232 -0.669 0.503922 ## genIllyarrie -0.12520 0.03255 -3.846 0.000126 *** ## genMerrit -0.01378 0.03300 -0.417 0.676425 ## genMyallie -0.07358 0.03716 -1.980 0.047935 * ## genUnicrop -0.18396 0.03238 -5.681 1.65e-08 *** ## genWarrah -0.17843 0.03243 -5.501 4.54e-08 *** ## genYandee -0.15374 0.03242 -4.742 2.35e-06 *** ## genYorrel -0.09957 0.03254 -3.060 0.002256 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(rep,site) 21.824 27.000 26.549 &lt; 2e-16 *** ## s(row,col):siteS01 2.000 2.000 0.911 0.402579 ## s(row,col):siteS02 17.825 19.763 5.263 &lt; 2e-16 *** ## s(row,col):siteS03 12.999 15.385 2.526 0.000966 *** ## s(row,col):siteS04 2.000 2.000 0.757 0.469094 ## s(row,col):siteS05 3.745 4.893 1.816 0.126446 ## s(row,col):siteS06 2.000 2.000 1.288 0.276202 ## s(row,col):siteS07 9.206 12.532 1.016 0.432038 ## s(row,col):siteS08 13.272 17.680 2.008 0.007607 ** ## s(row,col):siteS09 2.000 2.000 3.518 0.029936 * ## s(row,col):siteS10 2.000 2.000 2.655 0.070672 . ## s(row,col):siteS11 6.062 8.237 0.901 0.519681 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.536 Deviance explained = 57% ## GCV = 0.087403 Scale est. = 0.080877 n = 1392 dat$resid_2 &lt;- resid(m2) dat %&gt;% ggplot(aes(col, row))+ geom_tile(aes(fill = resid_2))+ facet_grid(~site)+ coord_fixed()+ labs(main = &quot;Residuals relaxing the assumption of within-block homogeneity&quot;)+ scico::scale_fill_scico()+ theme_pubclean() An intuitive visualization of splines d_predict &lt;- dat %&gt;% transmute(gen = &quot;Merrit&quot;, row, col, site, rep) d_predict$yhat_1 &lt;- predict(m1, newdata = d_predict) d_predict$yhat_2 &lt;- predict(m2, newdata = d_predict) sp_effects_trad &lt;- d_predict %&gt;% ggplot(aes(col, row))+ geom_tile(aes(fill = yhat_1))+ facet_grid(~site)+ coord_fixed()+ scico::scale_fill_scico(palette = &quot;bamako&quot;)+ labs(fill = expression(Yield~(ton~ha^{-1})))+ theme_pubclean() sp_effects_splines &lt;- d_predict %&gt;% ggplot(aes(col, row))+ geom_tile(aes(fill = yhat_2))+ facet_grid(~site)+ coord_fixed()+ scico::scale_fill_scico(palette = &quot;bamako&quot;)+ labs(fill = expression(Yield~(ton~ha^{-1})))+ theme_pubclean() ggarrange(sp_effects_trad, sp_effects_splines, common.legend = T, ncol = 1) Some thoughts on missing data imputation. Use of multilevel models for missing data imputation Use of GAM models for missing data imputation 18.3.0.2 Modeling time The data below come from an experiment aiming to test whether a diet treatment in cows can reduce the (GH) gas emmission. The experiment consists of 4 groups of cows that were hosted turns in the same facility. Each group contained similar cows and they were randomly divided into 2 subgroups. Those subgroups were assigned to a corral and each corral was assigned with a diet treatment. dat &lt;- read.csv(&quot;../../data_confid/fluxes.csv&quot;) dat %&gt;% ggplot(aes(doy, GHG_flux))+ geom_point(aes(fill = diet), shape= 21)+ scale_fill_manual(values = c(&quot;#FFBC42&quot;, &quot;#218380&quot;))+ labs(fill =&quot;Diet&quot;, y = expression(gas~flux~cow^{-1}))+ theme_pubclean() We can generally describe any point as \\[y_{ijk} \\sim D(\\mu_{ijk}, \\psi), \\\\ g(\\mu_{ijk}) = \\eta_{ijk}\\] where \\(y_{ijk}\\) is the observation of the flux per head for the \\(i\\)th diet, \\(j\\)th group and \\(l\\)th moment. Discussion of predictor \\(\\eta_{ijk}\\) We can, for example, use \\[\\eta_{ijk} = \\mu_0 + D_i + f(t_{ij}) + b_{k} + u_{ik},\\] where \\(\\mu_0\\) is the overall mean, \\(D_i\\) is the effect of the \\(i\\)th diet, \\(f(t_{ij})\\) is the function of time, \\(b_{k}\\) is the effect of the \\(k\\)th group. dat$diet_group &lt;- factor(paste(dat$diet, dat$group)) m1 &lt;- gam(GHG_flux ~ diet + s(doy, bs = &quot;tp&quot;, k =35) + s(group, bs = &quot;re&quot;) + s(diet_group, bs = &quot;re&quot;) , data = dat, family = Gamma(link = &quot;log&quot;)) dat_plot &lt;- expand.grid(doy = seq(min(dat$doy), max(dat$doy), length.out = 1000), diet = unique(dat$diet)) %&gt;% mutate(group = case_when(doy &lt;35 ~ 1, between(doy, 37, 61) ~ 2, between(doy, 61, 71.4) ~ 3, between(doy, 71.4, 117.8) ~ 4, doy&gt;117.8 ~ 5, TRUE ~ 6)) %&gt;% filter(group %in% unique(dat$group)) %&gt;% mutate(across(c( diet), ~as.factor(.)), diet_group = factor(paste(diet, group))) pred_plot &lt;- predict(m1, newdata = dat_plot, type = &quot;link&quot;, se = T) dat_plot$pred_m1 &lt;- pred_plot$fit dat_plot$se_m1 &lt;- pred_plot$se.fit dat_plot %&gt;% ggplot(aes(doy, exp(pred_m1)))+ geom_point(aes(doy, GHG_flux, color = diet), data = dat, alpha = .3)+ geom_ribbon(aes(group = paste(diet, group), fill = diet, ymin = exp(pred_m1 - (se_m1*1.96)) , ymax = exp(pred_m1 +(se_m1*1.96))), alpha = .3)+ geom_line(aes(group = paste(diet, group), color = diet))+ scale_fill_manual(values = c(&quot;#FFBC42&quot;, &quot;#218380&quot;))+ scale_color_manual(values = c(&quot;#FFBC42&quot;, &quot;#218380&quot;))+ coord_cartesian(ylim = c(0, 800))+ theme_pubclean()+ labs(fill =&quot;Diet&quot;, color = &quot;Diet&quot;, y = expression(gas~flux~cow^{-1})) summary(m1) ## ## Family: Gamma ## Link function: log ## ## Formula: ## GHG_flux ~ diet + s(doy, bs = &quot;tp&quot;, k = 35) + s(group, bs = &quot;re&quot;) + ## s(diet_group, bs = &quot;re&quot;) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.43170 0.03345 162.388 &lt; 2e-16 *** ## dietTreatment -0.13537 0.04848 -2.792 0.00534 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(doy) 3.217e+01 33.67 22.911 &lt; 2e-16 *** ## s(group) 1.245e-05 1.00 0.000 0.52837 ## s(diet_group) 2.212e+00 6.00 1.833 0.00177 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.585 Deviance explained = 58% ## GCV = 0.14934 Scale est. = 0.12529 n = 1000 "],["miscellaneous.html", "Day 19 Miscellaneous 19.1 Announcements 19.2 Statistical models 19.3 Illustrating this with a simulation 19.4 Wrap-up &amp; Discussion", " Day 19 Miscellaneous 19.1 Announcements 19.2 Statistical models Figure 19.1: Mindmap Why we need statistical models On Exactitude in Science (J.L. Borges) “What a useful thing a pocket-map is!” I remarked. “That’s another thing we’ve learned from your Nation,” said Mein Herr, “map-making. But we’ve carried it much further than you. What do you consider the largest map that would be really useful?” “About six inches to the mile.” “Only six inches!” exclaimed Mein Herr. “We very soon got to six yards to the mile. Then we tried a hundred yards to the mile. And then came the grandest idea of all! We actually made a map of the country, on the scale of a mile to the mile!” “Have you used it much?” I enquired. “It has never been spread out, yet,” said Mein Herr: “the farmers objected: they said it would cover the whole country, and shut out the sunlight! So we now use the country itself, as its own map, and I assure you it does nearly as well.” from Lewis Carroll, Sylvie and Bruno Concluded, Chapter XI, London, 1893 19.2.1 General linear model \\[\\mathbf{y} \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\Sigma),\\] We know that \\(\\hat{\\boldsymbol{\\beta}}_{MLE} = \\hat{\\boldsymbol{\\beta}}_{LSE} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\) \\(\\hat{\\boldsymbol{\\beta}} \\sim N(\\boldsymbol{\\beta}, \\frac{\\sigma^2}{(n-1)s^2_x})\\) Unbiasedness Estimation uncertainty versus Prediction uncertainty Invariance property of MLEs Bias-variance tradeoff Confidence interval: \\(CI_{95\\%} = \\hat{\\beta} \\pm t_{df} \\cdot s.e.(\\hat{\\beta})\\) Hypothesis tests t-tests: \\(P(t^\\star &gt; t_{dfe, 1 - \\alpha/2})\\), where \\(t^\\star = \\frac{\\hat\\theta}{s.e.(\\hat\\theta)}\\), and (for example) \\(s.e.(\\hat{\\beta}) = \\frac{\\hat{\\sigma}}{s_x \\sqrt{n-1}}\\) (for a CRD) \\(s.e.(\\hat\\mu_2 - \\hat\\mu_1) = \\sqrt{s.e.(\\hat\\mu_2)^2 + s.e.(\\hat\\mu_1)^2 + 2 \\text{cov}(\\hat\\mu_2, \\hat\\mu_1)}\\) Connection between CIs and t-tests F-tests: \\(P(F^\\star &gt; F_{dfn,dfd, 1 - \\alpha})\\), where \\(F^\\star = \\frac{SS_\\theta}{df}\\) Power: \\(Power_D = 1-\\beta\\) A justification to 80% Power [link] Figure 19.2: Types of errors 19.3 Illustrating this with a simulation 19.3.1 What is a simulation? Simulations are helpful tools in statistics to illustrate and study data generating processes in statistics and evaluate methods. Here is how one iteration would go: library(tidyverse) library(emmeans) library(latex2exp) # Set the &#39;true values&#39; mu_0 &lt;- 8 t_i &lt;- c(0, 0, 0.5, 2) sigma &lt;- 1 # generate the fake data set.seed(42) fake_df &lt;- # create the conditions the data will be generated expand.grid(treatment = factor(1:length(t_i)), rep = factor(1:5)) |&gt; # create a column for the expected value mutate(mu_i = mu_0 + t_i[treatment], # create a column for y = E(y) + residual y = mu_i + rnorm(n = n(), 0, sd =sigma)) fake_df %&gt;% ggplot(aes(treatment, y))+ geom_point(aes(fill = treatment), shape =21)+ geom_point(aes(y = mu_i), fill = &quot;gold&quot;, shape =22, size = 4, alpha = .4)+ scico::scale_fill_scico_d()+ theme_pubclean() # fit the model to the data m &lt;- lm(y ~ treatment, data = fake_df) summary(m) ## ## Call: ## lm(formula = y ~ treatment, data = fake_df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.5616 -0.4196 0.2055 0.7400 1.5943 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.4241 0.5617 14.998 7.65e-11 *** ## treatment2 -1.1579 0.7943 -1.458 0.16427 ## treatment3 0.1970 0.7943 0.248 0.80724 ## treatment4 2.5321 0.7943 3.188 0.00572 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.256 on 16 degrees of freedom ## Multiple R-squared: 0.587, Adjusted R-squared: 0.5095 ## F-statistic: 7.58 on 3 and 16 DF, p-value: 0.002247 19.3.2 Simulation with 3 repetitions # Set the &#39;true values&#39; mu_0 &lt;- 8 t_i &lt;- c(0, 0, 0.5, 2) sigma &lt;- 1 # set the conditions to generate fake data iteratively n_sims &lt;- 1000 n_reps &lt;- 3 # create the objects to store the estimates hat_t_2 &lt;- numeric(n_sims) hat_t_4 &lt;- numeric(n_sims) p_value_t2 &lt;- numeric(n_sims) p_value_t4 &lt;- numeric(n_sims) CI_t2 &lt;- matrix(ncol = 2, nrow = n_sims) CI_t4 &lt;- matrix(ncol = 2, nrow = n_sims) # simulation set.seed(2) for (i in 1:n_sims) { # create a fake dataset for those conditions fake_df &lt;- expand.grid(treatment = factor(1:4), rep = factor(1:n_reps)) |&gt; # generate fake data mutate(y = mu_0 + t_i[treatment] + rnorm(n = n(), 0, sd =sigma)) # fit model to fake data m &lt;- lm(y ~ treatment, data = fake_df) # retrieve estimates hat_t_2[i] &lt;- coef(m)[2] hat_t_4[i] &lt;- coef(m)[4] # retrieve p-values p_value_t2[i] &lt;- summary(m)$coefficients[2,&quot;Pr(&gt;|t|)&quot;] p_value_t4[i] &lt;- summary(m)$coefficients[4,&quot;Pr(&gt;|t|)&quot;] # retrieve CI CI_t2[i,] &lt;- confint(m)[2,] CI_t4[i,] &lt;- confint(m)[4,] } data.frame(hat_t_2) %&gt;% ggplot(aes(x = hat_t_2))+ geom_histogram(bins = 20)+ geom_vline(col = &quot;tomato&quot;, xintercept = mean(hat_t_2))+ geom_vline(linetype =2, col = &quot;gold&quot;, xintercept = t_i[2])+ labs(x = TeX(&quot;$\\\\hat{\\\\mu_2}-\\\\hat{\\\\mu_1}$&quot;), title = &quot;Histogram for 1000 simulations&quot;)+ theme_bw()+ theme(aspect.ratio = 1) data.frame(hat_t_4) %&gt;% ggplot(aes(x = hat_t_4))+ geom_histogram(bins = 20)+ geom_vline(col = &quot;tomato&quot;, xintercept = mean(hat_t_4))+ geom_vline(linetype =2, col = &quot;gold&quot;, xintercept = t_i[4])+ labs(x = TeX(&quot;$\\\\hat{\\\\mu_4}-\\\\hat{\\\\mu_1}$&quot;), title = &quot;Histogram for 1000 simulations&quot;)+ theme_bw()+ theme(aspect.ratio = 1) # type I error mean(p_value_t2&lt;0.05) ## [1] 0.057 # Power to detect a difference of 2 mean(p_value_t4&lt;0.05) ## [1] 0.592 data.frame(CI_t2) %&gt;% rowid_to_column(var = &quot;iter&quot;) %&gt;% ggplot(aes(X1, iter))+ geom_errorbarh(aes(xmin = X1, xmax = X2, color = (t_i[2]&gt;= X1 &amp; t_i[2] &lt;= X2)))+ geom_vline(linetype =2, col = &quot;gold&quot;, xintercept = t_i[2], size = 2)+ labs(x = TeX(&quot;$\\\\hat{\\\\mu_2}-\\\\hat{\\\\mu_1}$&quot;), title = &quot;CI for 1000 simulations&quot;)+ geom_text(label =paste(mean(t_i[2]&gt;= CI_t2[,1] &amp; t_i[2] &lt;= CI_t2[,2])*100, &quot;% of the CIs contain the true value&quot;), x = 0, y =900)+ scale_color_manual(values = c(&quot;tomato&quot;, &quot;lightgreen&quot;))+ theme_bw()+ labs(color = &quot;CI contains true value&quot;)+ theme(aspect.ratio = 1) data.frame(CI_t4) %&gt;% rowid_to_column(var = &quot;iter&quot;) %&gt;% ggplot(aes(X1, iter))+ geom_errorbarh(aes(xmin = X1, xmax = X2, color = (t_i[4]&gt;= X1 &amp; t_i[4] &lt;= X2)))+ geom_vline(linetype =2, col = &quot;gold&quot;, xintercept = t_i[4], size = 2)+ labs(x = TeX(&quot;$\\\\hat{\\\\mu_4}-\\\\hat{\\\\mu_1}$&quot;), title = &quot;CI for 1000 simulations&quot;)+ scale_color_manual(values = c(&quot;tomato&quot;, &quot;lightgreen&quot;))+ geom_text(label =paste(mean(t_i[4]&gt;= CI_t4[,1] &amp; t_i[4] &lt;= CI_t4[,2])*100, &quot;% of the CIs contain the true value&quot;), x = 2, y =900)+ theme_bw()+ labs(color = &quot;CI contains true value&quot;)+ theme(aspect.ratio = 1) 19.3.3 Simulation with 5 repetitions Take a look at the range of values that are estimated, and the power. # Set the &#39;true values&#39; mu_0 &lt;- 8 t_i &lt;- c(0, 0, 0.5, 2) sigma &lt;- 1 # set the conditions to generate fake data iteratively n_sims &lt;- 1000 n_reps &lt;- 5 # create the objects to store the estimates hat_t_2 &lt;- numeric(n_sims) hat_t_4 &lt;- numeric(n_sims) p_value_t2 &lt;- numeric(n_sims) p_value_t4 &lt;- numeric(n_sims) # simulation set.seed(83) for (i in 1:n_sims) { fake_df &lt;- expand.grid(treatment = factor(1:4), rep = factor(1:n_reps)) |&gt; mutate(mu_i = mu_0 + t_i[treatment], y = mu_i + rnorm(n = n(), 0, sd =sigma)) m &lt;- lm(y ~ treatment, data = fake_df) hat_t_2[i] &lt;- coef(m)[2] hat_t_4[i] &lt;- coef(m)[4] p_value_t2[i] &lt;- summary(m)$coefficients[2,&quot;Pr(&gt;|t|)&quot;] p_value_t4[i] &lt;- summary(m)$coefficients[4,&quot;Pr(&gt;|t|)&quot;] } data.frame(hat_t_2) %&gt;% ggplot(aes(x = hat_t_2))+ geom_histogram(bins = 20)+ geom_vline(col = &quot;tomato&quot;, xintercept = mean(hat_t_2))+ geom_vline(linetype =2, col = &quot;gold&quot;, xintercept = t_i[2])+ labs(x = TeX(&quot;$\\\\hat{\\\\mu_2}-\\\\hat{\\\\mu_1}$&quot;), title = &quot;Histogram for 1000 simulations&quot;)+ theme_bw()+ theme(aspect.ratio = 1) data.frame(hat_t_4) %&gt;% ggplot(aes(x = hat_t_4))+ geom_histogram(bins = 20)+ geom_vline(col = &quot;tomato&quot;, xintercept = mean(hat_t_4))+ geom_vline(linetype =2, col = &quot;gold&quot;, xintercept = t_i[4])+ labs(x = TeX(&quot;$\\\\hat{\\\\mu_4}-\\\\hat{\\\\mu_1}$&quot;), title = &quot;Histogram for 1000 simulations&quot;)+ theme_bw()+ theme(aspect.ratio = 1) # type I error mean(p_value_t2&lt;0.05) ## [1] 0.058 # Power to detect a difference of 2 mean(p_value_t4&lt;0.05) ## [1] 0.837 19.3.4 Demonstrating the bias-variance tradeoff # Set the &#39;true values&#39; mu_0 &lt;- 8 # treatment effect t_i &lt;- c(0, 0, 0.5, 2) # effect of covariate x1 beta1 &lt;- .1 # effect of covariate x2 beta2 &lt;- .15 sigma &lt;- 1 # set the conditions to generate fake data iteratively n_sims &lt;- 1000 n_reps &lt;- 3 # create the objects to store the estimates hat_t_2_r &lt;- numeric(n_sims) hat_t_2_f &lt;- numeric(n_sims) y_pred_r &lt;- numeric(n_sims) y_pred_f &lt;- numeric(n_sims) # simulation set.seed(33) for (i in 1:n_sims) { fake_df &lt;- expand.grid(treatment = factor(1:4), rep = factor(1:n_reps), x1 = 1:5, x2 = 1:6) |&gt; mutate(mu_i = mu_0 + t_i[treatment] + x1*beta1 + x2*beta2, y = mu_i + rnorm(n = n(), 0, sd =sigma)) m_reduced &lt;- lm(y ~ treatment, data = fake_df) m_full &lt;- lm(y ~ treatment + x1 + x2, data = fake_df) hat_t_2_r[i] &lt;- coef(m_reduced)[2] hat_t_2_f[i] &lt;- coef(m_full)[2] y_pred_r[i] &lt;- predict(m_reduced)[1] y_pred_f[i] &lt;- predict(m_full)[1] } data.frame(y_pred_r) %&gt;% ggplot(aes(x = y_pred_r))+ geom_histogram(bins = 30)+ geom_vline(col = &quot;tomato&quot;, xintercept = mean(y_pred_r))+ geom_vline(linetype =2, col = &quot;gold&quot;, xintercept = fake_df$mu_i[1])+ labs(x = TeX(&quot;$\\\\hat{\\\\mu_1}$&quot;), title = &quot;Histogram for 1000 simulations&quot;)+ theme_bw()+ coord_cartesian(xlim = c(7.75, 9.25))+ theme(aspect.ratio = 1) data.frame(y_pred_f) %&gt;% ggplot(aes(x = y_pred_f))+ geom_histogram(bins = 30)+ geom_vline(col = &quot;tomato&quot;, xintercept = mean(y_pred_f))+ geom_vline(linetype =2, col = &quot;gold&quot;, xintercept = fake_df$mu_i[1])+ labs(x = TeX(&quot;$\\\\hat{\\\\mu_1}$&quot;), title = &quot;Histogram for 1000 simulations&quot;)+ theme_bw()+ coord_cartesian(xlim = c(7.75, 9.25))+ theme(aspect.ratio = 1) 19.4 Wrap-up &amp; Discussion Relevance of statistical models Confidence intervals The role of p-values in Science "],["zero-inflated-models.html", "Day 20 Zero-inflated models 20.1 Announcements 20.2 Zero-inflated models 20.3 Applied example 20.4 Other related distributions", " Day 20 Zero-inflated models 20.1 Announcements Assignment 5 grades are posted Factors associated with: problems of using exploratory multivariable regression to identify causal risk factors (Lewer et al., 2025) 20.2 Zero-inflated models Data contain more zeroes than expected Zero-inflated models can be generally described as \\[P(Y=y) = \\begin{cases} \\pi + (1-\\pi)f(0) &amp; \\text{for } y=0 \\\\(1-\\pi)f(y) &amp; \\text{for } y =1, 2, \\dots \\end{cases},\\] where: \\(y\\) is the data, \\(f(y)\\) is the discrete probability function (e.g., Poisson, NB) \\(\\pi\\) is the probability of the process \\(f(\\cdot)\\) being “off”, \\(1-\\pi\\) is the probability of the process \\(f(\\cdot)\\) being “on”. 20.3 Applied example The data below were generated by a study of gastro-intestinal parasites in Ontario dairy cows based on a Canadian longitudinal study. The outcome of interest is fecal egg count in 5 gram of feces (fec). Independent variables include: tx [treatment; treated (1) or not treated (0)], lact [lactation; multiparous (1) or primiparous (0)], manure [manure spread on pasture; yes (1) or no (0)], library(tidyverse) library(ggpubr) library(glmmTMB) library(emmeans) url &lt;- &quot;https://raw.githubusercontent.com/stat870/fall2025/refs/heads/main/data/fecal_egg_counts.csv&quot; dat &lt;- read.csv(url) |&gt; mutate(across(c(tx, lact, manure, past_lact), ~as.factor(.))) dat |&gt; ggplot(aes(fec))+ geom_histogram(aes(group = tx, fill = tx), alpha = .5, color = &quot;black&quot;, binwidth = 2)+ coord_cartesian(xlim = c(0, 100))+ scico::scale_fill_scico_d(palette = &quot;hawaii&quot;)+ labs(fill = &quot;Treatment&quot;)+ theme_pubclean() ALSO: the data come from a longitudinal study that included multiple cows from different herds and provinces in Canada. That the cows coming from the same herd have more things in common than cows coming from another herd, and same for provinces. dat |&gt; ggplot(aes(factor(province)))+ geom_bar(aes(fill = factor(herd)), show.legend = F)+ labs(x = &quot;Province&quot;)+ scico::scale_fill_scico_d(palette = &quot;lipari&quot;, direction = -1)+ theme_pubclean() 20.3.1 Poisson model So, we know that the groups are herds within province. Then, a statistical model could be \\[y_{ijklmno} | \\boldsymbol{b} \\sim Pois(\\mu_{ijklmno}), \\\\ \\text{log}(\\mu_{ijklmno}) = \\eta_{ijklmno} = \\\\ \\eta_0 + T_i +L_j + MH_k + ML_l + p_m + b_{n(m)}, \\\\ b_{n(m)} \\sim N(0, \\sigma^2_b),\\] where: \\(y_{ijklmno}\\) is the count of eggs in 5 g of fecal matter for the \\(o\\)th cow in the \\(n\\)th herd in the \\(m\\)th province, that had treatment \\(i\\) applied and had parity \\(j\\) and the \\(k\\)th and \\(l\\) manure conditions, \\(\\mu_{ijklmno}\\) is the mean of said observation, \\(\\eta_{ijklmno}\\) is the linear predictor of \\(\\mu_{ijklmno}\\), \\(T_i\\) is the treatment effect on the linear predictor, \\(L_i\\) is the effect of multiparity on the linear predictor, \\(MH_k\\) is the effect of manure on heifer pature on the linear predictor, \\(ML_l\\) is the effect of manure on cow pature on the linear predictor, \\(p_m\\) is the effect of the \\(m\\)th province, \\(b_{n(m)}\\) is the effect of the \\(n\\)th herd in the \\(m\\)th province. m &lt;- glmmTMB(fec ~ tx + lact + manure + province + (1|province:herd), family = poisson(), data = dat) residuals &lt;- DHARMa::simulateResiduals(m, refit = FALSE) DHARMa::testZeroInflation(residuals) ## ## DHARMa zero-inflation test via comparison to expected zeros with simulation under H0 = ## fitted model ## ## data: simulationOutput ## ratioObsSim = 1.6604, p-value &lt; 2.2e-16 ## alternative hypothesis: two.sided plot(residuals) 20.3.2 Zero-inflated Poisson model From the model above, we can keep the linear predictor but add a zero inflation component. \\[P(Y=y_{ijklmno}) = \\begin{cases} \\pi + (1-\\pi)f(0) &amp; \\text{for } y=0 \\\\(1-\\pi)f(y) &amp; \\text{for } y =1, 2, \\dots \\end{cases},\\] where: \\(y_{ijklmno}\\) is the same data as above, \\(f(y)\\) is the discrete probability function elaborated above, \\(\\pi\\) is the probability of the process \\(f(\\cdot)\\) being “off”, \\(1-\\pi\\) is the probability of the process \\(f(\\cdot)\\) being “on”. m &lt;- glmmTMB(fec ~ tx + lact + manure + province + (1|province:herd), family = poisson(), ziformula = ~1, data = dat) residuals &lt;- DHARMa::simulateResiduals(m, refit = FALSE) In the plot below, the grey histogram indicates the amount of zeroes in simulated data generated by the fitted model, and the red line indicates the amount of zeroes in the data. DHARMa::testZeroInflation(residuals) ## ## DHARMa zero-inflation test via comparison to expected zeros with simulation under H0 = ## fitted model ## ## data: simulationOutput ## ratioObsSim = 1.0125, p-value = 0.896 ## alternative hypothesis: two.sided plot(residuals) 20.3.3 Negative Binomial model In the case of overdispersion, negative binomial models come in handy. m &lt;- glmmTMB(fec ~ tx + lact + manure + province + (1|province:herd), family = nbinom1(), data = dat) residuals &lt;- DHARMa::simulateResiduals(m, refit = FALSE) DHARMa::testZeroInflation(residuals) ## ## DHARMa zero-inflation test via comparison to expected zeros with simulation under H0 = ## fitted model ## ## data: simulationOutput ## ratioObsSim = 0.93316, p-value = 0.584 ## alternative hypothesis: two.sided plot(residuals) 20.3.4 Zero-inflated NB model m &lt;- glmmTMB(fec ~ tx + lact + manure + province + (1|province:herd), family = nbinom1(), ziformula = ~ 1 , data = dat) residuals &lt;- DHARMa::simulateResiduals(m, refit = FALSE) DHARMa::testZeroInflation(residuals) ## ## DHARMa zero-inflation test via comparison to expected zeros with simulation under H0 = ## fitted model ## ## data: simulationOutput ## ratioObsSim = 0.92838, p-value = 0.528 ## alternative hypothesis: two.sided plot(residuals) 20.3.5 Marginal means emmeans(m, ~ tx, type = &quot;response&quot;, contr = list(c(1,-1))) ## $emmeans ## tx response SE df asymp.LCL asymp.UCL ## 0 15.86 3.92 Inf 9.78 25.7 ## 1 6.16 2.01 Inf 3.25 11.7 ## ## Results are averaged over the levels of: lact, manure, province ## Confidence level used: 0.95 ## Intervals are back-transformed from the log scale ## ## $contrasts ## contrast ratio SE df null z.ratio p.value ## c(1, -1) 2.58 0.634 Inf 1 3.850 0.0001 ## ## Results are averaged over the levels of: lact, manure, province ## Tests are performed on the log scale 20.4 Other related distributions Hurdle models: Process “on” cannot produce zero \\[P(Y=y) = \\begin{cases} \\pi &amp; \\text{for } y=0 \\\\(1-\\pi)(f(y)/[1-f(0)] &amp; \\text{for } y =1, 2, \\dots \\end{cases},\\] where: \\(y\\) is the data, \\(f(y)\\) is the discrete probability function (e.g., Poisson, NB, Beta, Gamma) \\(\\pi\\) is the probability of the process \\(f(\\cdot)\\) being “off”, \\(1-\\pi\\) is the probability of the process \\(f(\\cdot)\\) being “on”. Hurdle Gamma Hurdle Beta Tweedie distribution: \\(f(y)\\) may be a normal distribution "],["semester-project-1.html", "Day 21 Semester Project 21.1 Learning objectives 21.2 Partial deadlines", " Day 21 Semester Project Semester projects may deal with any topic that interests you [the student], as long as it is approved by the instructor. Broadly, projects are expected to (i) identify a research problem, (ii) develop an appropriate workflow that is appropriate for solving that problem, and (iii) create a report and reproducible tutorial. 21.1 Learning objectives Be able to identify an experiment design that is appropriate for answering a given research question and discuss the strengths and weaknesses of that design for answering the question. Be able to write the materials and methods section of a paper/thesis, including the statistical model that corresponds the experiment design. 21.2 Partial deadlines 21.2.1 Project proposal - Due Friday September 24 at 11:59pm CT Write a page-long project proposal that states your research problem and the objective of your project. An example of an appropriate project proposal can be found here. 21.2.2 Written report for peer review - Due Thursday November 27 at 11:59pm CT for peer review 21.2.3 Oral presentation - Somewhere between December 3 - December 12 Prepare a 15 minute presentation of the core aspects of your project. Presentations should include at least: Motivation, Methods, including a clear and complete description of the statistical model and code, Discussion of strengths and weaknesses. 21.2.4 Written report - Due Friday December 19 at 11:59pm CT for peer review Submit a manuscript including: Introduction (background &amp; justification of the problem) Methods, including: A complete ANOVA table describing the degrees of freedom associated to mean comparisons, A clear and complete description of the statistical model, R code to implement that model, Results Discussion including strengths and weaknesses of the design/analysis, Conclusions. "]]
